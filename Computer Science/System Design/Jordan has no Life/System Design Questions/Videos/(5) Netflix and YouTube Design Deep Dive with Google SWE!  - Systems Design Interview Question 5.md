---
Source:
  - https://www.youtube.com/watch?v=PmryH1_NulU
Reviewed: false
---
- ![[Screenshot 2024-11-20 at 12.34.13 AM.png]]
	- Introduction
		- Hello everybody, I am back again doing a little late night recording in the studio here cooking it up as you know. Um today, I will be doing a systems design of something like Netflix or YouTube. I do have some decently significant differences again with my solution versus "grokking the systems design" solution so we'll go into that in a little bit. But final thing also before we actually go ahead and get into some functional requirements, would like to congratulate my friend Simeon who reached out to me over LinkedIn to let me know that you recently got an Amazon offer so congrats to you Simeon. Glad I could help. Again, really amazing news for anyone watching the channel if this in any way helps you, makes me ecstatic. So with all of this in mind, let's go ahead and get into some systems design and like I said, we'll start out with functional requirements and yeah let's get into it. I don't like wasting time.
	- YouTube and Netflix (Functional Requirements)
		- Description
			- Functional Requirements
				- Upload Videos
				- View Videos
				- Search Vides by Name
				- Number of Likes and Views of Videos
		- All right so as far as the functional requirements go, I can basically come up with like five. You want to be able to upload videos, you want to be able to view a video, so basically to fetch it by an ID. You want to be able to perform searches on video titles, i'll touch upon that in a little bit. You want to be able to perform searches on video titles and although I say perform searches here, I'm not asking us to actually build like a you know an in-house search index here, we can talk about search indexes and stuff later, but i'll have a dedicated video for building a distributed search index. We want to be able to record statistics of videos like likes and comments and views and then i already mentioned comments but yeah just be able to comment on videos generally speaking. 
	- YouTube and Netflix (Capacity Estimations)
		- Description
			- Assumptions
				- 1.5 billion users
				- 800 million DAU
				- 5 User Views per day
				- 1:200 uploads to views
				- 500 hours per minute
				- 50 MB per minute of mp4
			- Calculations
				- 46k Views per second
				- 230 Uploads per second
				- 1500 gb/min
				- `2.2 pb/day`
		- Let's touch upon some capacity estimations. So again, as per usual, I tend to kind of steal the assumptions from "grokking the systems design interview" and then I you know do my own calculations where they're applicable. So according to grokking, they basically estimate a billion and a half total users where 800 million of those are daily active users, each user that is daily active is going to probably be viewing around five videos a day and additionally, more importantly, there is going to be a one to two hundred upload to view ratio which means that for every single upload, we have 200 views per that video or not necessarily for that video but just 200 views of a video for each upload. That ultimately comes out to 46,000 views per second. Why? Because there are 800 million daily active users. We know that they're going to be five video views per user per day and then once you ultimately divide those out per second, we get 46,000 views. Dividing that by 200 results in 230 uploads per second because we know we have our 1 to 200 upload to video ratio. Additionally if we're assuming that about 500 hours worth of video footage is being uploaded per minute where each minute of video footage is about 50 megabytes, this is because you know we're going to be encoding each video in many formats, many resolutions, so it's not just super small. Then we can basically go ahead and do those calculations out and go ahead and say that 500 hours worth per minute with 50 megabytes per minute each is going to result in 1500 gigabytes per minute and basically 2.2 petabytes of data per day so obviously we have a ton in terms of years. We're going to be storing exabytes of storage so this is no joke. We have a lot going on there. We're obviously going to need some sort of partitioning scheme for things like our metadata and we're going to need some sort of super scalable distributed file storage for the actual video files or chunks themselves 
	- YouTube and Netflix (API Design)
		- Description 
			- API Endpoints
				- Upload(userId, videoName, chunks)
				- FetchChunk(chunkId, encoding, resolution)
				- Search(videoName)
		- Alrighty, for the API, they're going to be basically two main endpoints, then we'll talk about three others that we're not going to focus on as much. However obviously if we had a service like this we would still need them. The main one is obviously going to be the upload endpoint which can take in something like a user id, a video name, and an original mp4 file and what that's going to do is just take the file, upload it to our service through you know a variety of different design choices that we'll talk about subsequently but at least in terms of the actual abstraction of just uploading, those are probably the three parameters you have to pass. Additionally, you should be able to fetch a given video by its video id and you know if we're loading the video in a certain way perhaps we might even want a chunk id if that video is split into chunks, the resolution of the chunk that we're trying to fetch, and also the encoding of it because sometimes you want that to be relatively variable depending on the strength of your network connection. Then next we have the actual search functionality like i mentioned there is going to be a search index that is kind of relevant here and so basically a search just would go ahead and take in a video title and it should output a list of video ids or something like that. Then finally, you should also be able to like and comment on videos and liking should just take in a user id and a video id and a comment should go ahead and take any user id, a video id, and some text corresponding to that comment. After this, we can touch upon the tables so that we have a sense of how our data should actually be stored in our service. 
	- YouTube and Netflix (Database Schema)
		- Description
			- Important Tables
				- `Videos [videold, name, timestamp, views]`
				- `VideoChunk [chunkId, videold*, s3Link]`
		- So basically, I'm of the opinion that we should have the following four tables more or less. We should have a users table obviously with a user id and email and a password hash. It's pretty self-explanatory and we've basically had one of these in every single video that we've done so far. We should have a video table which should have some sort of unique video id, a name of the video so we can eventually search based on that. Perhaps a likes counter or if you know we really want to you know make things easy for us, we could do like a video likes table where we keep track of which user actually liked things and this might be better for analysis purposes to show which user liked a video and which hasn't and it would also make things easier to be able to unlike a video or make sure that users don't do it multiple times, but for now sure i'll keep it simple and just do a typical likes counter and then also just a time of upload. Next, I'm going to have a video chunk table. Now if you watched my dropbox or google drive design video, this is a very similar concept there where each video is broken into these small you know say four megabyte chunks or something along those lines and each chunk is maybe going to have a hash to make sure that it isn't being duplicated or something like that. It can have a video id field which should be indexed so we can quickly find all the video chunks corresponding to a given video id and then also some sort of s3 url or url in the file store that allows us to quickly access it from whatever blob or file store that we'll be using. Finally, I also wanted to have a comments table. I'm assuming that comments are going to be single layered here and we're not just going to have all these parent and child comments. If we did, then we would have a different schema, so i basically just had comments with a primary comment ID, a video id, which should be indexed so we can quickly load all of those comments, the text of the comment and also the time of the comment and you know, again, if we want to be able to like comments, we want to be able to reply to comments, this is going to require more functionality within our database but it's all still relatively manageable and i'll let you guys think through that as kind of an exercise for yourselves. It's really not too crazy 
- ![[Screenshot 2024-11-20 at 1.07.46 AM.png]]
	- YouTube and Netflix (Architectural Overview)
		- All right, as per usual, I'm going to do you know a 5-10 minute discussion of some of the choices that we make in the design itself before i actually whip out the entire design and go ahead and show that off. Anyways, basically the first thing to consider is this and this is where i kind of deviate from "grokking the system's design interview" a little bit. Not necessarily in the sense that they disagree with me, but in the sense that they don't really talk about it as much. The first thing is that every single file that we're uploading, obviously we want to do the majority of our work on the write path meaning that because they're going to be so many more video views than there are video uploads, the more work that we can do of pre-processing every single upload, obviously the less work that every single video view is going to have to do in order to basically you know render a certain type of encoding of the video, maybe get its thumbnails, or anything like that. So we have to do all these processing steps of every single video. And since we have all these processing steps, it would probably be easiest if we were to basically split that video file into chunks. So let's choose an arbitrary size, you know, in the few megabyte range again and we're going to basically do this so that we can parallelize all of that computation. Every single one of these chunks is going to have to be encoded in a bunch of different formats and resolutions and as a result, it would be better that as opposed to having our client upload the entire mp4 file at once to the server that it could in parallel upload all these small mp4 chunks to a bunch of different servers and then they could perform the processing. However it's probably not sufficient to have this just be a traditional rest endpoint because that would insinuate that, you know, after say a few seconds, the client would expect to hear back from the server saying, hey, we uploaded everything, you're good to go. Obviously these processes take a while because some of these videos are super long and in addition to that, you know, when you're transferring a lot of data over the internet like you would with an mp4 file, it's a slow process. So probably we're best off actually doing here is using something like a message queue. Now, you might say, okay, well should i be using a log based message queue or an in-memory message queue and i personally would lean towards an in-memory message queue for at least this part of the process. The reasoning for that being, well we don't really care about the order that the file chunks are uploaded, it just needs to be the case that they all eventually get uploaded. So it'd be better to have something like a load balance in memory message queue which basically takes all the chunks of the video and gradually passes them to a bunch of different consumer nodes which then do all of the necessary encoding or possible thumbnail extraction or anything like that. Now obviously kind of the the next step from here is well okay fine we have this original in memory partitioned replicated queue, you know that could be [[RabbitMQ]], it could be amazon SQS, whatever. The point is, let's say now that all these consumers have received the chunks, they've done the processing that needs to be done. How do they go ahead and basically get back to the client and say hey, you know, we have done everything and now this video is entirely uploaded. Well personally, I think that it would be bad if they basically all individually put like a row in a database saying oh yeah my step is done um, and there are a couple of reasons for this. I think that the biggest one is that it's not atomic, right? So it's possible that some of the chunks um might succeed in being processed and some of the chunks might fail in being processed and now these chunks are held in our database as video chunks and the other ones are not and now we have this incomplete picture of a youtube video in our database. So what I personally think would be a cool way of doing this or not necessarily a cool way but a functional way is you create basically another queue and this time it would be log based so that we make sure we have replay ability and durability in the actual messages themselves, and every single one of those processing nodes that processes a chunk and encodes it or does something like that, we'll then put an entry into a log based queue which can be partitioned by video id. So you know that all the entries for a given video are going to be in the same queue and then they can in turn be consumed by the same stateful consumer, and basically so now, we have all this data saying like chunk one processed, chunk two processed, chunk three processed and it's all going through this one log based queue to the same stateful consumer node right? And that could be running software like flink in order to keep track of that state. So now what's going to happen is that flink consumer is going to keep track of this local state and say okay, so for video id, I don't know, 69, we know that you know i got one entry in the queue basically saying um there are going to be i don't know 10 total chunks for this, and it can keep track of all the chunks that it's received. And so once it receives word or messages from that log-based queue of all 10 chunks, it can go ahead and upload all of the data to our metadata table which is basically all the video chunks at once. I think personally it's important that this update is transactional, right? Because like I said, you don't want it to be the case that only part of the chunks are uploaded and the other part are not. So that's why personally i think that for basically all the tables in the scenario, SQL is actually a very acceptable solution and if you're using SQL, it probably means that you're going to be doing something like single leader replication and in terms of partitioning because obviously we have a ton of video data um i think that it's very sufficient to partition the video chunks table by just the video id itself you know just make sure that all the chunks for a given video are on the same machine and then for the video table which is just like a video id and a name, perhaps you partition that by something like a user id of the person who posted it so you could quickly you know go to their wall and find all the videos they've posted in the past. Anyways my point is by having this stateful consumer node which is reading from this log based queue and then ultimately using a transaction to update our metadata, we can ensure that we're never going to have basically a partial view of one of the videos and everything is going to be uploaded at once. In terms of the actual video chunk files themselves, those can obviously go into a flexible storage system like s3 which is very elastic and basically scales up to any level of demand that we could possibly need which makes it very useful. I'm sure some of these websites do have custom storage solutions, but at the end of the day, they're all pretty similar implementations and all end up resembling some sort of file storage or block storage in one way or the other. That being said, while i think it makes sense to actually have the mp4 files in something like s3, for the actual thumbnail files," grokking the system's design" makes a very good point which is that it doesn't necessarily make sense to put them in something like s3 but rather some sort of NoSQL data store where you can get better data locality. They assume that each video has five thumbnails and while i'm not necessarily sure that this is the case on youtube maybe it's more relevant on Netflix, if you're going to be basically storing a bunch of static images for a given video, it's really useful to have a table in something like i don't know Cassandra or they use BigTable for example because BigTable has column oriented storage so then basically we could have this table in BigTable which keep in mind is  the same as HBase if you've watched my HBase video and what you would do is basically have a table called video thumbnails where the partition key is video ID, the sort key is thumbnail number, and then that way we can get this great data locality in storing five thumbnails together so we're not just making like five super random accesses to disk in something like s3 but rather we get this super great data locality as a result of column oriented storage in BigTable or HBase. So that's kind of the logic that they use there which truthfully i agree with. As far as loading videos go, like i said, everything that we're doing, we're optimizing for the read path which is if i am you know creating a video, it's totally fine if it takes a couple hours to upload but if i'm viewing a video, it better get to me as fast as possible and it better render as fast as possible. So the general gist of this is we know our videos are split into chunks. So once we have all these chunk ids that we need to be able to load, what you would then go and do is load the first chunk, you know, let it get a little bit of the way through, then load the second chunk, and load the third and what this allows us to do is further parallelize our computation in the sense that we don't have to sequentially load an entire video file at once, but rather we can wait until one is loaded. then start you know getting a little bit of the other and the point is we can start watching a video before the entire thing is loaded. So that's very very helpful and in addition of course in reality, services like Netflix and YouTube are using tons of cache for the metadata and also tons of cache in the form of CDNs for their video files which is kind of the bread and butter of how they really speed things up. They have massive cache networks and massive CDN networks to make sure that the data is as close to the end user as humanly possible. So with this all being said, let me go ahead and show off my diagram and try to basically wrap up what i just said so that it fits on just one whiteboard 
	- Diagram
		- Well, this is easily the most complex diagram yet that i've drawn up on this channel. Let's get into it because this one's gonna be fun. Okay, firstly we are going to start out with our typical client as one does which hits the load balancer, um i didn't really state it explicitly here but we can run this with some sort of redundant configuration in order to make sure that it's not a single point of failure, e.g. active-active or active-passive configurations. Active actors are when you basically have two load balancers running, but then you run the risk that if one goes down, the other gets overloaded. Active-passives is when the passive one basically knows the state of the active one but just kind of sits there waiting for it to die. Either works, both have their advantages and disadvantages. Anyways, as far as actually uploading a video goes, you're going to reach the upload service which is then basically going to take that file or basically take all the chunks that it's received from the client, and throw them all into RabbitMQ which is a partitioned replicated in memory queue. No it doesn't have to be RabbitMQ but i think for the sake of just listing out technology so that you guys get more familiar with them, this is kind of a nice thing to do to be able to list them like that. I think any in-memory queue here would work just fine and you know just justify your choice. This one's just a popular one. So anyways, once that basically chunk data goes into RabbitMQ, it can be load balanced or basically fanned out into any one of a number of encoders. Perhaps even multiple encoders if a chunk has to reach basically multiple levels of encoding or resolution or anything like that. So it's going to hit the encoders, the encoders are going to do the work that they do which i personally don't know much about but you know they do stuff. Put that data into s3 and then from there if they have any thumbnails, they can also go ahead and throw them into HBase and then finally, what they're going to do is basically then take the result of the successful operation and put that in Kafka queues. Now keep in mind that these Kafka queues are log based and the reasoning for this is you don't want to lose the fact that say any chunk was processed successfully or not successfully. It's very important that we don't lose the data here and you know ordering itself isn't overly important so maybe we actually could use a queue here but the bigger thing for me was i wanted to ensure that um the queue was going to be partitioned basically by video id so, you know, for each message, you can have the actual chunk data, but then also a field for the video id and that way you're basically going ahead and making sure that everything is going in the proper Kafka queue which means that it's going to be handled by the same flink consumer. Now what the flink consumer is going to do is like i mentioned earlier, it keeps track of basically all the chunks that it's seen for a given video id and then once all of those chunks are present and we can determine this by just actually putting the number of chunks for a given video id in each message, maybe there are better ways than doing it than that such as sending a dedicated message itself, but the point is once flink or the flink consumer says, okay, I've seen all the chunks that i need to see, it can go ahead and put that metadata into MySQL thus declaring a new video and also adding the corresponding video chunks. This is very useful to do via a transaction basically because like i said, we don't want it to be the case that any of those video chunk entries for the metadata are not in the MySQL table or else it's going to seem like we don't even have a video or you know perhaps a corrupted one where some of the chunks are missing. Once all of those entries are in the MySQL table, we can do a couple of things. Firstly, we're going to have a metadata cache so that when we actually go ahead and hit our download service to fetch a given video, we can quickly figure out what all the chunk ids are that we need to get from s3 and then furthermore we can stream the change data through another log-based queue like Kafka which makes sure that it's durable and we don't lose any of it and the reasoning for doing this is so that we can actually put it into a distributed search index such as [[Elasticsearch]] so that when we want to be able to search for videos by their name, we can go ahead and hit our Elasticsearch instance or cluster and then go ahead and send those back to the client. Finally, the last piece of the puzzle is the fact that we will have a content delivery network attached to our s3 configuration in a manner such that all of our video content is going to be geographically located in a manner that is close to our client and that way we can further reduce our latency. Yeah, I know it's a lot, maybe there are better ways to do this, maybe more efficient ones, and i'd love to hear it from you guys. Just looking back at it um i guess i technically don't need to use Kafka between the encoders and flink but i think (A) flink wants a log based queue anyway basically so it can you know replay those messages if it needs to ever build back state from a checkpoint and additionally again i just think it's important that we don't lose any of those messages because if we do, then it's going to seem like a video was never fully uploaded or that not all the chunks were uploaded or not all the processing was done when in reality it was. Again another thing is that we have all these video thumbnails in HBase which i guess i didn't really attach the CDN to HBase as well but you can imagine some of those video thumbnails will be stored in the CDN and again the cool thing about HBase is it uses column oriented storage. It's basically just the open source version of BigTable and it's built on top of HDFS or the Hadoop file storage and HBase will just basically allow us to have data locality with all the thumbnails for given video so we're not jumping around a bunch of distributed systems in order to fetch say five different thumbnails but we can rather just go to one location on disk and get all of them which is going to make our life a lot easier. So yeah, I mean i think it's a decently reasonable diagram. I'd like to hear what you guys think, but I just wanted to go into some more detail about generally, you know, kind of the i guess how you might deal with the fact that these files are going to be uploaded in chunks and how to keep track of all those chunks in a way that you know enables the full functionality of stream processing. 
	- So yeah, anyways, guys um i hope this video was enjoyable for all of you like i said. Love hearing the great news that these videos are helping and please subscribe before i get desperate and make some sort of cringy video. There are a lot i could be making. Let's just say that, and they would not make me look good and although I haven't had any friends discover this account, if and when that day comes, I don't want to have a day in the life of a google software engineer on my account because i will get roasted and i will be fighting for my life in the group chat. With that being said, everyone, have a great night