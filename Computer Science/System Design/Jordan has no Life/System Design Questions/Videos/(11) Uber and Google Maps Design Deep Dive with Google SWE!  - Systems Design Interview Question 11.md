---
Source:
  - https://www.youtube.com/watch?v=CI4j-uo58nQ
Reviewed: false
---
- ![[Screenshot 2024-11-25 at 1.29.29 AM.png]]
	- Introduction
		- hey everybody and welcome back for yet another video. Today we are going to be doing an Uber Google Maps combo which even though they're not that similar on a surface level, the kind of part that I'm going to be focusing on here is a little bit similar, so I guess we'll see how that happens as the problem gets about. Also sorry if I'm a little bit tired, I'm going to go to bed right after this and I'm supposed to wake up at 6:30 to lift so obviously I'm going to have to use the cheat code before I pass out combo, and uh yeah, until then, let's try and bang out this video before. So let's get into it. See you guys in a sec 
	- Uber & Google maps (Functional Requirements)
		- Description
			- Show nearby cars to potential riders, and keep their location up to date
			- Once matched, driver and rider should be able to see each others' updated location
			- Optimal route calculation in low latency with both time and distance numbers
		- Okay, so as per usual, we will be discussing this by starting with our functional requirements. Basically, there are going to be three that I'm going to talk about mainly and then I'll kind of discuss how even though I'm mainly going to be focusing on Uber here, how Google Maps kind of comes into play. So the first one is that if you are a potential passenger upon opening up the app, you should be able to see all of the drivers in your vicinity and also all of their locations updating in real time. So keep in mind we're probably going to need some sort of real-time push notification system here which could be something like a long polling or a web hook or a service and event but we'll talk about that in a little bit. In addition, once passengers are matched with a specific driver in order to take them on their route, they should both be able to see the other's location from that point until they're actually in the ride itself and then finally, and this is kind of where the Google Maps aspect of this comes into play because Google Maps is sort of a subset of the Uber app, an estimated time and a distance for any given route should be able to be calculated and we'll talk about efficient ways to do this because this is kind of the main thing I'm going to be focusing on here since as far as basically showing nearby people or nearby drivers, we talked about that a lot in our Yelp video and if you're not so familiar with something like a geospatial index, I'll be touching upon that in this video but I recommend that if that's a new concept to you, you watch the Yelp video on it which is the previous one on my channel as well as the actual geospatial indexing concepts video that I have on my channel from a couple months ago. 
	- Uber & Google Maps (Capacity Estimates)
		- Description
			- Push location changes every 3 seconds
			- 1 mil active daily riders, 500k active daily drivers
			- `~`20 bytes of data to store per rider/driver in geospatial index
			- `1.5mil * 20 bytes = 30Mb`
		- As far as capacity estimates go, I'm again going to steal some of these numbers from "grokking the systems design interview" because as per usual, they're a little bit arbitrary but we're basically going to assume that all location changes are going to be pushed to our back end about every 3 seconds and then additionally, they're going to be about 1 million active daily Riders and about 500,000 active daily drivers. If we're assuming that in kind of this geospatial index that we're eventually going to maintain that we have something like a "4 byte?" geohash a "4 byte?" latitude and longitude and ID and additionally something like a name and also some sort of popularity or relevance score so that we can you know match a rider with a good driver and vice versa, only match riders if you know they have a certain amount of stars or something like that. We can basically say they're around 20ish to maybe 24 bytes and we'll multiply that by 1.5 million because this is all the stuff we're going to be storing in our geospatial index and so we're really only going to have, you know, around 30 something megabytes of data to store which is not a lot at all. 
		- However, the key thing to note here is that we're probably still going to eventually want to Shard this because certain areas are going be very highly populated and as a result of that, if they're getting a lot of queries, it's important that we shard our data in a smart way such that none of our servers are going to be overloaded. So we'll talk about that when it comes to the architectural overview section but I figured I would touch upon that a little bit right now.
	- Uber & Google Maps (API Design)
		- Description
			- Search(lat, `lng`, radius)
			- Match(userId, nearbyDriverIds)
			- GetRoute(startLatLng, endLatLng)
		- In terms of an API design for our service, I can basically think of three main things. The first one is going to be the actual search endpoint and this is very similar to what we did in Yelp where a rider is going to basically request that all of the drivers in their vicinity within a certain radius are going to be displayed on their screen so that search can probably take in an original latitude and a longitude as well as a radius parameter and it can go ahead and return all the drivers in the area and also do so in a manner such that the client then starts receiving realtime updates from all those drivers as to their current locations. Next, we have an actual match API endpoint which is going to basically take in a user ID and Driver ID and more or less say, hey, this is going to be a ride that's going to happen. Then finally, this is the end point that I want to be focusing on the most is the actual get route endpoint where we're going to have a start and end latitude and longitude and basically what that's going to do is it's going to return an estimated ETA based on you know how many minutes from now the ride is going to be as well as something like a distance that uh the ride should be and that get route is going to be kind of similar to what Google Maps might do when providing you with directions and as a result, it deserves some amount of looking into and examining towards how can we make this endpoint particularly efficient. 
	- Uber & Google Maps (Database Schema)
		- Description
			- Users(id, email, `pw_hash`)
			- Drivers(id, email, `pw_hash`, in_ride)
			- Rides/Trips(user_id, driver_id, route, time)
		- Then finally in terms of database schemas that we might have for something like uber, we can basically have ourselves a user table which as we know will have something like an ID, an email, a password hash, pretty typical stuff and then the same goes for a driver table, probably similar information there, then next, we're going to have something like a rides table which is basically going to have user IDs and Driver IDs and some additional information about the ride such as, you know, the route, the cost, things along those lines and then finally, something like some sort of geospatial index which will be keeping basically the location of both users users and drivers in once they're signed into our app in order to help us with the matching process and also the actual visualization of drivers once a user basically starts to request for them.
- ![[Screenshot 2024-11-25 at 1.58.00 AM.png]]
	- Uber & Google Maps (Architectural Overview)
		- Description
			- (2) Compaction Hierarchies
				- Goal: Create smaller version of graph with only "important nodes"
				- If Removing B: Shortest path from A to C = (A, B) + (B, C) so we need to add shortcut edge!
				- We want to remove least "important" edges first!
					- Less important nodes require fewer shortcut edges to be added when removed
					- C and D are "important"! do not remove
			- (3) Graph Segmenting
				- Split Graph into pieces (segments), and determine distance between segments! Use segment boundaries for Dijkstra's algorithm! (Greatly decreases search space)
			- (4) Post-Ride Route Calculation
				- Hidden Markov model, most likely route can be efficiently calculated using Viterbi Algorithm
					- P(driver takes Road A then Road B) = P(driver takes Road A) $*$ P(GPS signal1 and 2 given driver on Road A) $*$ P(driver takes Road B given driver was on Road A) $*$ P(GPS signal 3 given driver on road B)
		- (1) All right so as far as the design goes for this Uber back-end, I really do feel like "grokking the systems design interview" leaves out a lot of key details. So I'm going to obviously as usual try and emphasize those but I also have to talk about the parts of Uber that they also mention which is generally very similar to our Yelp back-end, so let's get started with that. As we know, one of the main functionalities that we have to be able to support is a given user who wants a ride should be able to see all of the other drivers in their vicinity. So if you've watched the Yelp video, the first thing you should be thinking is, all right, we need ourselves a Geo spatial index. Recall that a geospatial index is basically an efficient data structure that allows us to do Geographic queries to find basically some sort of entity within a given radius of a point that doesn't require using some sort of 2D scan of an entire range of points over a 2d coordinate plane that a traditional database might support. So this way, using something like a [[quadtree]] or a [[geohash]], we can really easily find all of the drivers in our vicinity. However in this case that's not enough. Um like I mentioned, one of the things that we want to be able to do is make sure that none of our geospatial indexes are (A) getting overpopulated and (B) that they're remaining up to date because drivers and users are constantly in motion. So we're going to have to make some modifications from the Yelp problem. The first thing as far as I'm concerned is making sure that our geospatial index is up to date and that the driver basically has the right entry for its geohash or something like that. So within that, here's kind of what I'm going to propose. We know that we want these really low latency queries, we know that every 3 seconds, we're going to have to be updating our index and additionally, we know that the actual durability of the data is not as important, right? Like at the end of the day, it's not the end of the world if you know our server went down and some of the locations of our drivers were lost because eventually they would just reconnect to the app and could, you know, connect to say a different partition of our geospatial index or a different replica, but with all these things in mind, it seems like it's a very natural choice to use a memory based solution for our geospatial index and as a result of it being in memory and just also so that we don't overload some servers, what we want to be able to do is basically partition these all in a very complete and smart way. So like I mentioned in the Yelp video, a supernatural way to partition something like a quadtree or a um geohash is to basically partition by the bounding boxes themselves. However, obviously some bounding boxes are going to have more points, more cars, more riders than others and so we want to kind of do this in a dynamic way where say one node in our Redis cluster might have this huge bounding box and another node might have a very small bounding box if that bounding box is corresponding to say downtown Manhattan on a Saturday night when a ton of people are going out and want Ubers back because they're can't take the subway. So with this in mind, a dynamic basically partitioning schema can be really useful in order to split up the data so that none of our servers get overloaded. We can do this using some sort of consistent hashing schema and either a coordination service like zookeeper or using a gossip protocol and it seems like um in real life, I think Uber does actually use a gossip protocol here to keep things very fast and then basically forwards all the writes from node to node in that Redis cluster, but that's not overly important for the moment. So anyways, now we basically have all of these Redis nodes where they are sharded by these geohashes or these bounding boxes that correspond to 2D locations on the map. Um, basically what we can do is say as a driver or a user is moving around every 3 seconds, we're going to push their latitude, longitude location to the right geohash node so basically we're going to have to use this partitioning schema and make sure that the record for a corresponding user or driver is in the right node in that Redis cluster, and also if they were to leave a given geohash area such that they should no longer be held on that node, we have to make sure that that entry is also being removed from the node. It's important as well like I mentioned that a lot of these locations, the most updated ones, are being propagated to users in real time. So what we're going to do is basically once that user requests, "I want 15 drivers close to me right now because I'm about to request a ride", the second they do that, that redis node starts sending them some sort of push notification through something like a WebSocket which can be done in real time or a server sent event or even a long pull so that they can see the actively changing `latlong` of all those drivers. So that's kind of how we would deal with the real-time aspect of this. Additionally, in order to kind of help with matching a given user with a driver, once you've kind of figured out which um Redis cluster or Redis node to pull a driver from, you can basically internally sort those through something like their you know number of stars or their rating score in the past and that way you can pick which driver a user should be matched with. Another point which I should quickly touch upon even though it's not overly relevant which you guys might find cool is that in reality, Uber doesn't actually use [[quadtree|quadtrees]] nor does it use geohashes, instead, they've created their own system known as [[H3]] which basically partitions the world into hexagons because they find that it's a little bit more accurate in terms of doing these radius calculations and you can really nicely split the world into these hexagons while maintaining this hexagonal structure that kind of resembles a circle and as a result, it more accurately reflects basically a radius query when you're just trying to get all the points within one hexagon. Again, that's a huge aside and I'll put an image up of it just so you guys can see, but I thought that was kind of a cool thing to show. Okay, with all of that out of the way, we can basically say, all right, we've covered the part of uber that is very similar to Yelp, right? We've dealt with our geospatial index, we've said to ourselves, here's how we would find all of the nearby drivers which is again very similar to Yelp, we've said here is how we would update all of the locations of both drivers and Riders which you can do just using a WebSocket to the proper Redis node but now we have to kind of address the new unique part of the problem and this is something that both Uber and Google Maps share in common which is once we know basically the source and the destination of a given ride, how is it that we're actually going to figure out the route to take between them. So before we get into this, let's talk about some sort of data structures and algorithms concepts that you guys may remember or you may not remember as much. If you have a graph of a bunch of nodes and edges and in this case the nodes would be representing basically a bunch of intersections on you know our world and our road system and every edge would be representing a road and we want to find the shortest path, there are a few algorithms you can use to do this. The first one is Dijkstra's algorithm where Dijkstra's is basically a breath first search where you gradually expand out from your starting point into a set known as the frontier set and then using basically some sort of dynamic programming, every single node in the frontier set, you gradually calculate the shortest path to the node in the frontier set and then you can create a new frontier set from the old frontier set where you get the shortest path for each of the nodes in the new frontier set. So that's Dijkstra's. An optimization on this is known as [[A*]] which I personally don't even know too well but I do know that it's certainly a faster algorithm and you guys can feel free to look into that. It's not as relevant for this problem but the issue with both Dijkstra's and A* is that if we're trying to find the shortest paths between two nodes in an actual graph of, you know, our Earth with all the roads and the possibilities and the interstates that there are, algorithmically this can take a really long time and it's not really good if that computation takes a while because obviously the end user expects it to be very fast. They want to basically say, hey get me the directions and return them very quickly and give me a time estimate of how long this route is going to take. So what can we actually do in order to kind of speed up this process? Well, the answer as with most things when you want to speed up is some form of caching and the algorithm that I'm going to be talking about in particular is known as [[contraction hierarchies]] and it's relatively new but it seems to already be in use for a lot of basically GPS systems both in Google and other places and the general premise is as follows
		- (2) Like I said, running Dijkstra on the actual world map is way too expensive. There's too much to consider and you're basically running this huge breath first algorithm over a gigantic search space and there's really no time for that. However, what we could do instead is basically split our graph up like so. We know that generally speaking the majority of nodes in our graph are pretty much useless to us, right? Like the majority of nodes are going to be on back roads that we don't care about because the truth of the matter is, when you're driving, you generally want to be taking highways from place to place until you get off those highways. So what can we actually do? Well, we can basically take our original graph of the entire world which you know I guess Google has or someone else might but the whole point is that you're going to start removing nodes from that graph and this is known as a [[node contraction]]. So basically any node that isn't very relevant and we know it's not very relevant because basically we've deemed it unimportant and I'll discuss a little bit more about what that means, we're going to remove it from the graph. So basically we're going to do the following. Let's imagine we want to remove a node V from the graph. We can do so but we also can't take away any of the roads that go through it because, well, let's say I need to go through V to get from point U to point W. I need to make sure that there is still a route from U to W and we know how long it takes in order to get there. So what we can do is the following, if we're removing a node, let's say it's V, and we have two nodes U and W and the shortest path from U to W is through V, when we remove V from our graph, we can actually just create a new edge from U to W which reflects the actual distance from U to W as if we had gone through Point V but now the node for Point V is no longer there and I'll make sure to do visual representations of all of this. So basically, we've now created the shortcut edge and what these shortcut edges allow us to do are to remove a bunch of nodes from our graph itself such that we can pre-compute the distances between points that we deem to be more relevant. Now how can we basically decide what nodes they are that we should be removing first? Well one good heuristic to use here is to basically remove all the nodes that require adding the fewest shortcut edges meaning, you know, say I were to remove a point on a major highway. Then I would have to add so many shortcut edges to our graph because there are so many fast routes that go through that highway. However if I were to remove a node in some Farmland in Kansas, it's not really going to add a lot of shortcut edges because it just happens to be the case that that was a back road and it didn't really get us to a lot of places efficiently so with that in mind we basically want to be removing all of these relatively irrelevant nodes such as these back roads and Alleyways and things like that that don't really get us places quickly and then what's going to be left over after a few iterations of this are these pretty much compacted graphs where there are many fewer nodes and basically the majority of those nodes remaining are on more popular roads and highways and things like that.
		- (3) and so now what we've done effectively is we've segmented our graph in a way that we can really quickly calculate the distance between very popular points and once we have those precomputed distances between popular points, you know, we can get a popular point that's close to both our source node and our destination node and then from there, we run a very limited Dijkstra's search between the source to the first popular point that we care about and the destination and the second popular point that we care about. And so by doing so, we're able to basically pre-compute all of this direction and ETA information and as a result of that, we can make this query basically 15 times faster than what it was and they're actual numbers to support that. Basically a second and a half to 100 milliseconds is the number that I keep seeing and this is basically the technique that all these companies use because by you know pre-computing all these numbers, we can effectively cut down on our search space by a ton and even though it is the case that we won't necessarily have the fastest route because to some extent we still have to do some approximations in this whole pre-computation method with um compaction hierarchy, we're going to have a very generally good route which is going to make it very quick in terms of the latency of our queries. 
		- (4) The final thing that I would like to note is um and this is you know back to kind of being unique to Uber is there is a ton of background processing that has to go on both during the ride and more importantly after the ride. That tends to be things like collecting payments, um collecting data about the actual location of the driver during the ride. So for example you're going to get a ton of GPS signals from that car during the ride itself so that you can basically show the rider the route that the driver took after the fact and that right there probably involves some sort of stream processing where all of those GPS signals are ingested into a Kafka stream which are eventually hitting some sort of processor node which knows once it's received all of the GPS signals and then it has to actually go and create a route along a GPS map and that's actually not as easy as you might think because the GPS signals themselves are a bit imprecise, right? Like they might be you know 10 ft or 20 ft off the road so one way that I've seen that Uber will will actually account for something like this is by using something known as a [[hidden Markov model]] where basically it's saying, okay considering all of our GPS signals and considering how likely we are for a given road to basically get a GPS signal that we've gotten and considering the possibility that you're going to go from Road "A" to Road "B" which is known as a [[transition probability]] and then there's also the [[emission probabilities]] of saying uh, what's the likelihood that we get you know the GPS signal that we see here from Road "A". You can basically map out this hidden Markov model and then run a dynamic programming algorithm known as the [[Viterbi algorithm]] in order to basically quickly compute the solution of what is the most likely root the driver took from the um source to the destination of the rider. I know that was kind of a very high level overview of what's actually going on and the reason I don't go into it further is because truthfully, I just don't really think it's that necessary for the systems design interviews. Something like Markov models and the Viterbi algorithm is more relevant for something like machine learning which I personally think is super cool, but I also want to generally keep these videos to around the half hour mark and I don't think it'll make anyone happy if I spent all day talking, but I am going to link a bunch of articles below so that for those of you who are interested or just want to learn more about how this stuff works, um you can.
- ![[Screenshot 2024-11-25 at 2.13.21 AM.png]]
	- All right, apologies as I passed out before I could do the last part of the video but as per usual, we will be talking about the diagram .This back-end particular is quite large. So anyways, let's get into it and diagnose this thing and, you know, go ahead and go through it. So first of all, we have our client which is going to hit a load balancer as per usual. This is going to be a single point of failure and as such we can run a second load balancer in either active-active or active-passive configuration. As we said, one of our functional requirements is that at first, the client is going to basically query the surrounding area to see all possible drivers that could potentially pick them up as well as getting real-time updates as to their latitude and longitude as it changes. As a result, they're going to first hit this Geo-service which is going to be basically horizontally um scaled out and afterwards, hit our Redis cluster which we described earlier which is basically our cluster of in-memory databases that are sharded by geohashes so you know we can dynamically Shard them based on the size of the geohash But ultimately the point is a bunch of cars within a given you know box or geohash bound are going to be on the same Redis node and as a result we can quickly return the results back to the client as well as all of their latitude and longitudes and we can do this via a WebSocket because as we said, we want to be updating all of these positions in real time which insinuates that we want some sort of real-time push uh technology like a WebSocket as opposed to typical polling. Next after that, once the client has basically seen all of the nearby drivers, it's going to actually request a ride and this is done via the dispatch service. The dispatch service is yet again going to hit this Redis cluster and in particular, it's going going to hit the node that the client is a part of or perhaps an even bigger node or multiple of those Redis nodes if we have to look at multiple bounding boxes because there aren't too many drivers available and then once it does so, the dispatch service is basically going to look at all of the possible drivers nearby in the Redis cluster based on that geospatial index and select one or possibly even more than one based on the actual popularity of the driver and we can do that using, you know, say something like the number of stars. Afterwards, um one of the drivers on their own Client app is basically going to say, I accept this ride and then both the client and the driver are alerted of the fact that they have now been paired up. After the actual pairing itself, the client now needs to see the route that they'll be expecting to take and this is important because the route service is not only going to basically fetch the directions for the driver, but additionally, and more importantly it is going to give an estimate of the pricing because it'll give our distance. So the route service will first check a route cache um to basically see if the root is in there and if not, it can go ahead and actually consult the database. Now if you'll see here as opposed to using something like a relational database or you know in particular a NoSQL database like a wide store or a key Value Store I've chosen to go with a graph database and there are a couple of reasons for this. The first is that as opposed to just keeping all of our graph data in memory and using a bunch of pointers, we know that you know the graph of the world is really large and there's going to be a ton of data in there so it seems pretty natural to use something like a distributed graph database in order to basically hold all of that data and then obviously for the popular routes, we want those to be stored in the cache and when I say popular routes you know not every single route is going to be in the cache but perhaps we could store kind of the common parts of a route. So for example like I mentioned because we're using these contraction hierarchies, what we're basically going to do for a given Source node of a ride and a given Target node of a ride is assuming that the source and target themselves aren't in that contracted um graph of only the popular nodes, we can basically run a breath first search from both the source and target node until we find one of the nodes in that contracted graph which is going to have only the popular nodes in our database and then basically we can cache only the popular routes. So basically the routes from one node in the contracted graph to the other node in the contracted graph and as a result of that, we can basically cache these really um popular components of a lot of routes but not necessarily the entire route and that's going to help us a lot. Another reason that using a graph database here is actually going to help a lot with Neo4j in particular is that not only are things like breath first search, depth first search and dijkstra's algorithm built into graph databases but more importantly, graph traversals are actually faster. The reason for this is that graph databases will actually have pointers to the location of connected nodes on disk. This is really useful because as opposed to basically non-native graph databases which are basically just an abstraction over a typical database index, what is good about a graph database that is native is that you're using disk pointers to go from node to node. If you didn't do that, you would basically have to say, okay, node "A" has an edge to node B with say ID6. Now I have to go through my index and actually find the node with ID6 and so every single time you traverse from node to node, you have to go through an index to find the node on the other end of the edge and what that means is that because an index basically is holding all of the vertices in the graph, the amount of time it's going to take to traverse from node to node is going to scale proportionally or rather proportionally but logarithmically to the size of the index because we can binary search the index, so basically, if you're using a non-native graph database, every single time you add a node, it's going to increase your graph traversal times by a factor of log n where n is the number of nodes in the graph. So a native graph database on the other hand is very useful and you can see me talk more about that in my actual graph database concepts video. But like I said, having a graph database here is going to be super useful because we have a ton of graph data and it's going to allow us to perform all sorts of cool searches and traversals over our data. Then finally we have our ride data service. The ride data service is basically going to be ingesting a bunch of points and data points that are going to come up while the ride is actually occurring or the trip is occurring so because like I mentioned a lot of this is going to be GPS signals from the car or the driver itself, we can plop those into a Kafka stream which I'm using Kafka here in particular because it's log based and that means those points are going to be durable and replayable and that is usable very well with our stream processing framework Flink. And the reason Flink is good here is because Flink allows us to basically keep state when doing stream processing so we can ingest all of these GPS data points for a specific ride and we can basically just partition these Kafka streams and Flink processors such that for a given ride ID, all of the GPS points are going to the same Flink node and then we can send one more signal when the ride is done through our Kafka stream and basically once the Flink processor uh receives that done signal, it can go ahead and cache all the GPS signals that it's received and after receiving the done, it can perform the computation that is necessary in order to kind of calculate what route was actually taken and we can do that using that Hidden Markov model and Viterbi algorithm that I mentioned previously. And then once that calculation is complete once we've basically guessed which route the driver has actually taken, we can basically get the distance that corresponds to that and a price and we can put all of that information into our trips Cassandra database. Now the reason I'm using a Cassandra database here is because it allows for really quick inserts, um, we can basically store data at great scale and we don't really care about things like eventual consistency here because they're all just going to be going on in the background so Cassandra acts as a really easy way to just store all of our trip data and additionally the fact that there's no schema makes it easy to kind of add additional data over time to our database and then the way that I personally would probably go ahead and Shard out our Cassandra database is by using the user ID as the primary key for the trip (aka the rider ID) and then similarly, um the the secondary key or the sort key to be the timestamp and what this will allow us to do is for a given user, we can really easily find all of their rides and also sort them by you know most recent to least recent. Um yeah I mean obviously there are some details that I left out such as payment information and things like sending emails to a rider once their ride is done and same probably goes to the driver but overall, I think this acts as a pretty good overview of things that can be done. Um, you can always do some more like stream processing as things get added to the trip's database and then connect that to something like a notification service or an email service, but overall I feel like this is a pretty comprehensive diagram of how something like uber might actually work. If we're dealing with Google Maps, obviously the majority of that computation is going to be the route service component where we're basically taking this huge graph database, caching a bunch of kind of the popular components of many roots and then using something like contraction hierarchies in order to simplify our calculation a lot and then combining that with graph traversal algorithms such as dijkstra's or A* in order to get our results really quickly. 
	- Okay, I hope this video was useful guys. I'm trying to uh crank them out as best I can, but in the meantime, I hope you guys are all learning a lot. I'm still waiting on this collaboration video that I made to be posted so um yeah subscribers are looking moderately low but hey you know what I'm not really doing this for the subs and if I was I would probably just be making day in the Life videos and here's the resume that got me into Google. But uh yeah, not doing that. At least yet. Soon I'll get desperate and then I will, but for now I won't.