---
Source:
  - https://www.youtube.com/watch?v=1moO3rn42uk
---
- ![[Screenshot 2024-11-20 at 1.32.25 AM.png]]
	- Introduction
		- Hello boys and girls. We're back with yet another systems design interview prep video. Uh before i get started let me go ahead and address the elephant in the room which are these cloud goggles that i currently have equipped. The reasoning for this is that my body is letting me down and it seems that over the last couple of days, I have contracted a bad case of pink eye. So rather than letting you guys see my face, what i'm instead going to do is do my best Guy Fieri impression with these glasses here and do the entire interview wearing them. Obviously there are coders that dress worse than this so i don't really think that it should be a huge matter of concern for any of you, and I hope that we can all ignore it and be adults about this. Furthermore, I should let everyone know that my voice is letting me down because maybe i have covid again, who knows and additionally I don't really even like this problem that much which is the building a [[Typeahead]] suggestion system. That being said, it gets asked frequently and as a result of that, I want to cover it. So, let's get into it. Um, I'm going to try and do a little unique view today where i actually get the camera over this white board while i write on it and if that all goes to plan, maybe we can scrap ourselves together a video here. Have a good one guys and let's get into this.
	- Typeahead Suggestions (Functional Requirements)
		- Description
			- Functional Requirements
				- Find popular suggested terms starting with prefix that user provided as input
		- So as per usual before i get into the actual design of what it is that we're going to be building today, it would be better to first talk about some functional requirements, capacity estimates, and api design and also a bit of a database schema assuming it's relevant for this problem which may not be. So in terms of the actual functional requirements themselves, basically all that's going to be happening is as a user is typing into maybe their browser or maybe the keyboard on their phone, all we're going to be doing is taking the prefix of what they typed and then suggesting the top 10 terms with that prefix and basically just you know giving them suggestions for what they may be typing. Obviously this has to be happening in super low latency and it's pretty much the single requirement of this entire system which means that it is going to be the sole focus and we want to get our latency as low as possible. 
	- Typeahead Suggestions (Capacity Estimations)
		- Capacity Estimates
			- 5 billion searches per day
			- 60k searches per second
			- 100 million unique terms
			- 3 words per term, 5 characters per word
			- 3Gb to store all terms
		- In terms of capacity estimates, again i'm ripping a lot of these numbers from "grokking the systems design interview" because i think you know they just have reasonable estimations for the most part. If we're going to assume that like 5 billion things are being searched for per day, that means they're about 60,000 searches per second and additionally if there are going to be 100 million unique terms that are being placed in this index or whatever data structure eventually that we're going to be using in order to generate these suggestions, if every single query is three words and five characters on average per word, that means there's going to be 15 characters per query and if we're using our normal two byte per character estimation, 100 million basically unique terms times 30 bytes per term is about 3 gigabytes. So obviously we don't have a ton of data to store here. We don't necessarily have to partition it but if we're able to partition our data in a way that can decrease the latency of our system, then partitioning is still definitely something that is worth thinking about. 
	- Typeahead Suggestions (API Design)
		- Description
			- API Design
				- Fetch(userId, prefix)
		- If we're going to be discussing an api design, this is something that's going to be pretty simple because like i said, we really only have one functional requirement, right? Which means that we really only need one endpoint. That is going to be the fetch endpoint where it's basically just going to be a user id and then the prefix itself. The reason I'm including a user id here is just so that we can go ahead and potentially load in some suggestions specifically tailored for the user but technically that's out of scope for this problem and you know basically just acknowledging the fact that we may end up doing that in the future should be sufficient here. We mostly just care about getting relevant search terms for a given prefix. Obviously that's just going to be some get rest endpoint and then whatever our servers are, we'll be returning the top 10 suggestions 
	- Typeahead Suggestions (Database Schema)
		- Okay, let's talk quickly about some sort of database design because the question really shouldn't be necessarily which database do we use in this problem but more so should we actually be using a database because everything here needs to happen with such low latency because users are typing so quickly and want those suggestions fast, a database probably isn't the right choice for a solution here but rather what we are using to kind of keep track of all these suggestions should be able to fit in memory. We already mentioned that it's only about three gigabytes which means that whatever data structure we end up storing can go on just one machine in memory and as a result of that, it's probably best to avoid using a database here. Obviously we're going to need some sort of disk eventually in order to kind of persist the data structure that we have in memory, but it's not necessarily the case that we need something like a database because we don't necessarily need all of these guarantees and relationships and all these other features that databases offer as opposed to perhaps just one specific data structure that might help us a lot in order to kind of do this prefix searching, and we'll go into what that data structure is in a second but let's just say that using memory is probably the way to go here
- ![[Screenshot 2024-11-20 at 1.47.00 AM.png]]
	- Typeahead Suggestions (Architectural Overview)
		- All right, so basically as a result of me not having long enough computer cables, I am unable to do an overhead image setup which is pretty pathetic but i'll try and fix that soon. So what i'll go ahead and do is basically describe what it is that i would have been writing out and then separately, I'll just go ahead and have some images on the whiteboard of what i ended up actually drawing out. So in terms of basically what is the design that we want to use here, well keep in mind that basically we're typing out a prefix and then from that prefix we want to find all of the suggestions which are easily basically fetchable from that prefix. So in the past when we've been using hard drives in order to solve this problem, what we would typically do is rely on something like a binary search with an index on disk where all of those terms are sorted and then we would go ahead and run this binary search in a manner such that we could quickly find all the terms that have a given prefix and i covered that in my search indexes videos. However, because of the fact that we're working with memory here, we can actually achieve a better solution than just running a binary search and get a better performance than just having a bunch of words that are falling within a given range of terms. So what we actually go and do here is we use something known as a [[trie]]. Now i'm going to go ahead and draw a trie on the whiteboard, but the general point is this. It's a tree that keeps track of all of the words that we've seen using a bunch of nodes where each node represents a character and then what this allows us to do is basically for every single letter of a word, we can then go through a node by node in the trie and then that way we can see if that word is in the trie. So what we do is for a given prefix, we'll iterate through the trie and then any of the nodes that fall under the end of that prefix are going to be search terms that may have stemmed from that prefix. Now the thing is here though, we don't want to return all of the possible search terms. We only want to be returning the top 10. So this is where we have to think of some clever solutions because if we wanted to return all of the possible search terms, that could take us a super long time 
		- And additionally, if we just want to return the top 10 most frequent ones, imagine we have basically the counts of every single search term at the leaf of the trie, even if we were to have those precomputed we would still have to iterate through all possible branches from the trie after the end of the prefix in order to figure out which the most popular terms were. So what we would be better off doing is actually go ahead and pre-computing what the most 10 popular terms are from that prefix node and the way that we can do this is as opposed to just putting the entire term itself in the node because that takes a lot of space, what you can do is put an in memory reference to the actual leaf node that corresponds to the end of that search term. That way, you can then just go to the leaf term directly because it's random access memory, you can just go to that memory address, and then iterate through parent pointers from the leaf node all the way back to the root node. By doing so, we can quickly basically memoize or cache all of the top 10 results from a given prefix node and so basically the trie structure here is going to allow us to really efficiently solve this problem and basically use memory in order to do it. However, even still this trie isn't perfect and there's definitely other things that we have to talk about.
- ![[Screenshot 2024-11-20 at 1.57.07 AM.png]]
	- Typeahead Suggestions (Architectural Overview) Part 2
		- So the first thing to note is that obviously as we have new search terms coming in, we're going to have to be able to update our trie. However, we don't want to be updating our trie every single time a search term comes in because of the fact that if we were to do this, we would be putting a ton of load on all of these servers and kind of what we care more about is actually just being able to get the maximum read performance possible and we don't want to bog down all these trie, you know, data structures with a bunch of writes so we should instead be doing is basically taking all of our new queries that we're getting, logging those to some basically disk maybe s3 or something like that, and then offline, maybe once per day, running a new batch job either by MapReduce or something like Spark where we aggregate the counts of all of those search terms and then actually go ahead and update our trie. The thing is when we update our trie, we have to keep in mind the fact that it's possible that now some of the search terms may have been more popular than before and we're going to have to update our pre-computed caches of the 10 most popular terms from any given node in the trie which means that we basically have to go ahead and do our typical bottom-up depth-first search frequency calculations at every single node in the trie. So again it's important that this is a batch job because we're potentially updating the entire trie at least from the leaf to the root every single time the leaf frequency of a given search term is actually going to be changed. Additionally another consideration that we need is the fact that this trie is actually going to have to be persisted probably to disk. So one thing to note is we can't really just store the trie as is in disk because the concept of pointers don't really work so well on disk. 
		- So we have to kind of figure out a way of serializing our try such that we can put all of this text on disk. So the way that we might actually do this is by basically go ahead and taking the root node and then saying how many children it has, followed by the first child and how many children it has, and then the child of that first child and how many children it has, and this way we're able to effectively store like a depth first search list of the traversal and that way we can rebuild our trie. But one thing to note is that our pre-computed frequencies of every single term are now no longer going to be able to be stored. Keep in mind that the fact that we're actually referencing a node in all of those pre-computations because we're referencing the leaf node of a given search term, is no longer going to be valid on disk because the pointer was something that was being referred to in memory. So what we're going to have to do is basically just store the nodes of the trie and then upon basically taking that trie from disk and reinstating it in memory of the given server, we're going to have to do that entire recalculation or pre-computation of finding the most frequent terms from every single node in the trie. Obviously that's a significant computation, however the point is we're only going to be doing this once we're restarting a trie server and as a result of that, it's not really a huge deal if this takes a little bit longer
		- And then finally as far as replication and partitioning go, let's think about both of these. Obviously we want replication, the main reason being that one server won't be overloaded by a ton of queries and also our data will be fault tolerant if one of them were to go down. So sure replication makes a lot of sense and also having replicas means that we can potentially update our trie in more interesting ways for example if there's a master and a slave trie, you could potentially update the slave trie after a given batch job, turn that into the master and then update the old master node. So you know there's obviously going to be some potential utility of replication beyond just fault tolerance. As for partitioning, there are a few different approaches that we could possibly use here. We obviously only want to be partitioning our data if it's going to make our queries faster because keep in mind I said there's only going to be a few gigabytes of data in the trie and that's something that can fit on one server. So if it's faster to not partition, then we shouldn't be. However, there are some interesting partitioning schemas here that could reduce the load in a way that might make our queries faster. So one possible way is we basically take ranges of the trie or ranges of search queries and distribute those from machine to machine. So keep in mind that every single time you're going down a given path of the trie and you reach a node, all children of that node are within a given range of strings. However the point is here that it's hard to basically go ahead and pre-allocate these partitions, right? If we were to statically partition all of these ranges, what might happen is that a given query or a given basically range of queries may become hot and then a certain server may get overloaded. So if we were to do something like a range based partitioning, perhaps it would be better if this was done dynamically and we would probably need some system to go ahead and do that for us which could get a little complex. Another thing that we could possibly do is hash based partitioning where we basically just hash every single search query term and put it on a machine that corresponds to that. The issue here now is that for a given prefix, we have to check every single partition to see all of the terms that could come after it and considering we want such low latency, I personally don't think this is the greatest idea because now we're going to have to basically wait on the slowest possible partition in order to aggregate our results. Even though these calls are being made in parallel, the fact that we have to wait for this aggregation is going to take a while. So ultimately i feel like if anything you would do range based queries and possibly in a dynamic configuration to hopefully reduce hot spots and if there were to be hot spots, the big thing here is that we would want to introduce a caching layer where as opposed to having basically these linear time scans where you go through the trie every single time a prefix comes in, we would just say for a given prefix, is this result already in the cache and if so let's go ahead and pull the results out from that and that would be a constant time scan as opposed to linear time which would be really nice and the cache like i said would be super useful there. 
- ![[Screenshot 2024-11-20 at 2.03.10 AM.png]]
	- Diagram
		- Oh god, I'm actually dying. This is so brutal. Uh okay, let's get into this diagram. So I have my Typeahead suggestion diagram written out below and basically the premise is this. So for starters, we have our client, right? And just as a little bit of a front-end optimization that i wanted to mention, we should probably make it the case that the client does two things. First of all, the second the browser opens or whatever client it is that is going to be ultimately connecting to our service, they should probably instantly establish a tcp connection to our back end. The reasoning for this being that a TCP connection, since it's a multiple way handshake, actually is going to require a couple of round trips and as a result of that, the earlier we get this done before we really need these super low latency queries, the better off that we are. Additionally, we should probably make it the case that we're only going to perform searches for the type of head once we've actually let the client wait a couple seconds so that we don't basically perform a search every single time they're going to type a character because that would be hammering our servers too much. Okay with that out of the way, the client is going to hit our load balancer which is going to choose one of our fetch service servers and then once that happens, hit a second load balancer which basically holds our partitioning configuration for our redis instances which are going to actually hold the data structure for our trie. That is going to basically contain all of the lookup information which i would hope would be dynamically sharded based on the actual range of strings but again, we don't even necessarily need to use sharding here. I just included it for the sake of complicating the diagram a little bit. Additionally, we have our add query service. The reasoning for this being that every single time a client actually ends up choosing what they're going to be searching, we want to take that query and put it in HDFS. The reasoning for this being so that at that interval that we specify, perhaps once a day, maybe twice a day, we're going to run a huge batch job through something like MapReduce or Spark and we're going to then update all of our redis instances where we can basically create a copy of this new trie and then switch over on all the redis instances to using that trie. Note that the redis instances are replicated so that in the event that one of those partitions goes down, the other one can take over and whatever ends up taking over for that redis service that went down, can go ahead and basically re-establish from disk the trie and you know, they can pull that from HDFS. Finally, the last thing is we have a cache layer so that the fetch service can go ahead and quickly pull from that cache if the search term was recently used by either that user or someone else. Um something like an LRU policy works very nicely for this cache, and you might say to yourself here, oh well actually isn't the cache going to be equally as linear time as the try, because you know the string by virtue of using a hash map on that cache table, we're going to have to first in the cache actually convert the string to some sort of hash which involves linearly going through the string and making that conversion and that's the same complexity as going through the try and while i might agree with you what you could actually have is um before the client even submits a request, they actually on the client side convert the string to a hash and that way we can make it such that we genuinely just have constant time operations in the cache and we're not overloading our cache. We just quickly receive the result. 
- Conclusion
	- Yeah, so pretty simple diagram for the most part but still figured it would be nice to go ahead and write that out because "grokking the system's design" doesn't actually include that whole picture, but yeah i mean i hope this video was useful for you guys. Um i'm sorry i couldn't do a better job here in terms of differentiating myself a little bit more just because like i said, it's mostly an algorithm question and it's dealing with just kind of making that trie and storing it in a way that it's easily reconvertible and there are definitely a lot of trade-offs to be made here in terms of you know storing the trie to disc and pulling it back from disk sooner and most of them kind of involve how much storage we're actually going to need. So i think grokking's solution is actually really good for something like this here, but yeah overall hope this video helped, maybe i won't look like you know a monster the next time i actually make a video. I can take these glasses off but in the meantime, good luck with all the studying guys and I look forward to making the next one