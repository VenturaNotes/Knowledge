---
Source:
  - https://www.youtube.com/watch?v=_KoiMoZZ3C8
---
- ![[Screenshot 2024-11-25 at 8.19.06 PM.png]]
	- Introduction
		- hello everybody and welcome back to the channel. But actually let's go ahead and get into this video because i don't want to waste anyone's time here on this lovely Saturday. Today we're going to be talking about metrics and logging and personally i don't care about metrics and logging because as a sigma male myself, I don't care about what anyone else does whether it's on my website or my company's website but that being said, apparently all these beta companies do care about metrics and logging and as a result, i'm probably gonna have to cover it. So anyways let's get into it, we'll get into the architecture, i'll show you the diagram as per usual and then we can be on our merry way.
	- Metrics and Logging (Functional Requirements)
		- Description
			- Allow rapid publishing of logs and metrics from both clients and internal servers to eventually be viewed by employees.
			- Persist data for more exploration down the line
		- So in typical fashion, we'll be starting off with our functional requirements. So the first one is basically going to be that basically clients or even possibly devices if we're talking about metrics are able to generate both logs and metrics that other you know internal people working in the company will eventually be able to check out and hopefully gain some insights from. Additionally, they should also be available in a way that is persistent for further analytical exploration down the line. So you know that means that you want them in some sort of big data storage so that people can run batch jobs on them run SQL queries on them, things like that. So let's move on to our capacity estimates.
	- Metrics and Logging (Capacity Estimates)
		- Description
			- 1 billion messages per day (I misspoke sorry)
			- 100 bytes on average per log/metric
			- 100CB of data per day
		- Um, so i'm going to just use some kind of coarse numbers here because the truth of the matter is every company is operating at different scale, but let's assume about 100 million messages whether that's logs, metrics, user generated, anything like that or generated per day because you know we're going to have a lot of users, each of them every single time they basically do anything on our site, it's going to generate some sort of log. So i feel like a billion is a pretty solid estimate. Let's say on average each message is 100 bytes. Now it's possible that some of these messages are going to be these kind of huge json strings and they could be more than 100 bytes, but let's assume 100 especially if we're going to be using some sort of kind of like compression framework and i'll talk about that a little bit later. So if that's the case, then that means we have about 100 gigabytes of data per day to process and you know you can extrapolate that over a year or multiple years. This is obviously going to become a huge data set that we're going to have to deal with  
	- Metrics and Logging (API Design)
		- Description
			- sendMetric(source, payload, timestamp)
			- fetchDataset(seriesId, timeRange)
		- Okay, I'll mention some api endpoints here, but truthfully the majority of this is probably going to be done over something like an RPC call and i'll discuss the reason for that later. It kind of has to do more with all that compression stuff, but you know, if we're just talking about ways that our client might interact with our application servers, there's probably going to basically be one to actually go ahead and send the message so that's going to have like a payload, a timestamp, maybe a topic if you wanted to go to a certain place or another and then also a second endpoint could just be to fetch a given data set, you know, if i'm an internal user or rather if i'm an employee for the company and i want to basically go ahead and load a data set, i'm going to have to tell some sort of service, hey fetch this for me and then it'll return some sort of visualization or the data points within that set, but again we'll talk about that all more later.
	- Metrics and Logging (Architectural Overview)
		- All right, now let's get straight into design. The reason i'm skipping over the database schema is that it's kind of open-ended for a problem like this. Right you know the database schema itself, it's not really clear what data exactly it is that we're storing, so i'm more going to talk about kind of the the types of databases that we would use and the technologies as opposed to the actual specific data that's going in there, so let's start from kind of an outside view looking into our system and then we can break things down further and further as we go. So we know that we have these clients, we know that we have all these you know different servers throughout our system and they're going to be sending a ton of messages. So in theory it would be easy to say, oh well every single time a message is generated, let's just put it right into a database. But the truth of the matter is, it's not so simple, right? Because every single message, it's not just that we want to put it into a database, the second that message is generated, there's a ton of processing that we want to do with it, and that's going to be both in real time so stream processing and also possibly batch processing later down the line too. So let's talk about where we can actually put our message that enables us to kind of do all of these analytics on it. Generally speaking when you're going to be generating all these messages from all these different sources and you want them contained in one place to be handled in the same way, we should probably be using a message broker. So the first question that should come into your head is well would we rather be using an in-memory message broker or a log-based message broker. So keep in mind that the advantages of using an in-memory based message broker are that when you kind of just want to take all these messages and send them to consumers that are stateless because you just want them to run some job on it and then you know once the job is done, all is good, that's kind of when an in-memory broker is really good. However for our case it seems like a log based message broker may be a little bit better and there are a couple of reasons for this. One of which is the durability of messages because everything is persisted to disk we can be sure that none of our metrics or number of logs that we have are getting lost and our data can be as accurate as possible. In addition to that because of the fact that we're going to have all these consumer nodes which are doing different things with our messages and these consumer nodes are often going to be maintaining some sort of internal state based on the messages that we've seen, having a message broker that allows us to replay messages that were in the past will allow us to (A) add new types of consumers that can do new types of processing on the data and (B) basically um you know get their state back if they were to crash because then they could keep track of you know the last state checkpoint that they have, which was probably you know going to be an S3 or something and then they can replay the messages after that point. So it seems that even though you know the throughput in theory is a bit less, the log based message broker is going to be a little bit better for our design here and that's something that i see you know actual companies doing. For example, i watched a like a Pinterest video to see how they did their distributed logging in order to help make this video and they're very much using Kafka as opposed to something like RabbitMQ. Okay, so I keep mentioning that we're going to have all these consumer nodes and that they're holding some sort of state, but what's some examples of state that they may be holding and why might they have it? Well the first thing is stream enrichment, a lot of these messages are going to have things like a user id, but they're probably not going to include a bunch of relevant data with that user which we may actually want in our logs so we don't have to query a SQL table every time we see a given log. So in terms of stream enrichment, something like you know either a flink consumer or a spark streaming consumer could be keeping a local copy of a given table on it in order to basically join with every single one of these messages so that it could eventually be placed into a database with more information. Additionally, as opposed to just doing the stream enrichments, we could even be doing stream-stream joins where we have a given consumer pulling from two different Kafka queues, so basically two queues of different topics and then they're pulling from one partition of each of those topics and then they're joining that information together and again that can help us make these really insightful logs and i've mentioned kind of cases of this in my stream processing video Another thing that we can do is time-based aggregations. Oftentimes it's not enough to just take the metric from you know every single you know maybe one second that we're getting them and say we want five minute aggregations, ten minute aggregations, things like that. So there are three types of windows that i'm gonna address here and here's how the stream processing nodes can do it. There are tumbling windows which are basically fixed length and fixed start time non-overlapping windows so it's basically saying one minute windows starting at the beginning of every single minute and it's really easy to do those. You basically just take the messages as they come in and you know cut off the part that you don't care about and say, oh it's going into this window, and then there's also hopping windows which is a question of basically, I have you know five minute windows but i can have one that starts at 12 o'clock and it goes to 12:05 and then i can have one that starts at 12:01 and it goes to 12:06. So you know hopping windows can overlap with one another, so the way you would create those is by having these tumbling windows which i just mentioned and actually aggregating you know say five one minute tumbling windows to create all of these hopping windows, and then the third type of possible window is a sliding window where even though it has a fixed length, there's no necessarily constraints on the start time or the end time. So you know even like starting within a half a second or something i can have this sliding window and then it just lasts five minutes and the way you would do that is by basically maintaining something like a linked list in memory and using one process to add elements to the end of the list and another process to remove elements from the beginning of the list once they've been expired from that sliding window. So those are some examples of stream processing that we can actually be doing and for more examples of those, I recommend watching my stream processing video, but let's actually talk about which technology we should use as our stream consumers. One example would be flink where the main advantage here is that even though both flink and spark streaming which are the two examples i'm going to address can do stream processing and maintain state and you know have reliability by being able to get their state back if for some reason it gets lost, Flink is handling messages in real time whereas spark is handling messages in mini batches. So kind of why is one case good for certain scenarios and mini batching might be better for different scenarios. Well in terms of handling messages in real time, it means that you know if we're putting those in a database, I'm going to be able to see that message in the database more quickly. On the other hand if we're handling messages in mini batches so say you know as a consumer node i get 10 messages at a time, it means that i can also upload those messages to the database in a mini batch and as a result there's less network overhead in sending all of them. I think it's probably not really a huge difference which gets used in these cases, but that is kind of a difference to know between the whole real-time stream processing versus semi-real time using mini batches, um then furthermore, the next kind of piece of the puzzle is where we are actually going to send the data after it goes through these consumer nodes. Well there are a few options. The first is going to be a time series database, so yeah obviously we could just throw it in you know Cassandra or something or Hbase and call it a day, however the really useful thing about a time series database is that it's specifically created in order to handle this timestamp data like metrics and like logs so that it can be both written and read really quickly. So how do they actually do that? Well what a lot of time series databases will do is as opposed to having one huge index over the entire table to both read and write into, they'll actually split out the table which i've seen called a hyper table into a bunch of these mini indexes where each index represents one source of logs, so say like sensor one and then the time range for those logs. Sensor one January first and then we would have sensor one January second and that would be a different index. So why is this really helpful? Well the main kind of thing is that all of these writes for you know a given day are probably only going to be going to a couple of these mini indexes and as a result we can cache the entire index in memory and get really really good write performance and read performance by doing that. Whereas you know say we had like a b-tree based index and um you know split out all over this tree, it would be hard to kind of cache only the relevant portions for us. So it's really useful to split out this hyper table into these smaller indexes. Another reason that's super useful is a very common functionality or use case in time series databases is to actually delete certain data once it reaches a certain age. The reason that's bad in say LSM-tree databases for example is that there are no actually deletes on keys in LSM databases. All you can really do is put a tombstone over a key where you just mark it as deleted and as a result, you have to wait for that SSTable compaction to go down before the key is actually removed but because we have these indexes and they're all much smaller, once an index representing a certain sensor and certain time gets old enough and we want to get rid of it, we can just actually go ahead and delete that index on disk as opposed to having to go through this whole expensive process of creating a tombstone and then eventually compacting it. So that's why time series databases are really useful, but you know for more detail on that because i kind of brushed through it, I recommend watching my time series database video. Okay, so i mentioned that some of these logs and some of these metrics either aggregated or unaggregated can go into a time series database and that's going to be really good for both reads and writes and things like data visualization if you want to use something like tableau. However, a lot of this stuff is going to be less formatted or kind of in a jumbled mess and if we do have things like that, we want to be able to process it later, perhaps the better place to put it would be something like S3 and we would use S3 as something known as a data lake where we're basically just throwing in a bunch of jumbled unformatted data and then we know that we're going to be doing some processing on it later. So by putting it in S3, there are no requirements on you know the actual formatting of the logs and that can be really useful sometimes because logs are coming from a ton of different sources and as are metrics and it can be a pain to have to format them and it might just be better to get them in the database and deal with that later. So what might we do once that information is all in S3? Well typically what we would do is then basically export our data from S3 to something like a Hadoop cluster because you can't do data processing directly from S3, it doesn't have good data locality unfortunately, but it's just cheap so that's why companies use it. So you would use something like a Hadoop cluster and run spark on that data once it's loaded in the cluster and spark is basically just a batch processing service that you know is super widely used. It's better than MapReduce for many reasons which i've spoken about in the past and basically you can use spark to do any complicated processing that you may have to do and potentially even format your data in a way so that you can throw it in a data warehouse. So what's really useful about having data in both a time series database and a data warehouse. Well oftentimes it's actually stored in a column-oriented format and what that means is if you're trying to basically rip all the results from one single column, say there's one metric that you're really interested in, then being able to store all of your data in a column-oriented format, means that all of the basically entries in a single column are going to be in the same file and you're going to get much better data locality when trying to make that reads, and it's going to be a much faster read. Additionally, a nice thing about column oriented storage is that you could use something like Parquet which is just an open source framework to further basically compress your column-oriented storage because a lot of these metrics are going to be very similar to one another and as a result you can use strategies like dictionary encoding or bitmap encoding to really reduce the size of your data and that's really important obviously. Another further thing to actually be able to do is you want to um if you can if you know the format of all these logs as they're coming in or you can kind of specify them beforehand, you can encode a lot of these messages using data encoding frameworks like AVRO protocol buffers or thrift and that's going to be super useful because not only does it reduce the amount of data that has to be sent over the network, but more importantly, it is going to reduce the amount of data that's actually being held on disk so it's going to take you know the fact that you're storing a ton of data and ideally greatly reduce the size that you're actually having to store and hopefully save a bunch of costs for the company. At least i can say with certainty in google we pretty much use protocol buffers for everything and the reasoning for that is you know you get huge network gains and you also get huge disk gains. It's going to save a lot of time and money. 
		- And then finally eventually you know we have a ton of data, what can we do to basically i don't know say not necessarily get rid of some but how can we cut costs because a lot of this data is going to become more and more irrelevant over time. Well there are a few ways, for starters, you could do things like sub sampling where you only take a certain portion of the data and keep that in your databases or another thing you can do is as data gets older and older you can move it to kind of an archiving solution so for amazon specifically you have s3 for you know the data that you really care about, and then as it gets old and it's less important to you, you can put it in a slower storage solution something like AWS glacier. So I hope this makes general sense for the most part, you know logging metrics are obviously super important for any company especially once they want to be running things like machine learning models on that because it's the way that they basically get better as a product and you know it's kind of just the best way to improve your service so this is obviously something that every company cares about. With that being said, I've kind of described everything that you might want in a logging service, let's go ahead and take a look at a diagram. 
- ![[Screenshot 2024-11-25 at 8.22.39 PM.png]]
	- Diagram
		- Alrighty, let's do this diagram. Sorry about my pen. Anyways let's get into it. Metrics and logging. So first we have our client or you know this might even be just another internal server that's sending metrics, it might be an IOT device, basically any source of a metric or a log that, you know, we want to be keeping track of. The metric that it's ultimately going to be producing is getting sent to a load balancer because everything is going to be horizontally scaled here because we want to be doing it on consumer hardware and the load balancer can be in either a active-passive or active-active type of configuration. So anyways, once we hit our logging service, we can go ahead and start throwing those messages into our Kafka queue where they're going to reach our flink consumers. Ultimately i chose flink just because i wanted some real-time processing that is going to get things into our time series db quickly because sometimes you want to be able to you know get at least close to real-time updates on the time series data and you know if um you're looking at server logs, you don't want like a couple second lag before you see those messages but obviously you're going to get some lag so, it is what it is. The time series database can just use something like single leader replication in order to you know ensure fault tolerance and then another possible sink for data to go to from our flink consumers is actually going to be s3. This is where it can be in something like, you know, an unformatted setting and as a result of being an s3, we can either put it in glacier if it's really old or better yet we can go ahead and transport that data into a Hadoop cluster where we can start running all sorts of spark jobs to put it into a more formatted setting such as a data warehouse where we're going to be storing all of our data in a column oriented fashion running compression on that and that way we can get something like SQL insights for our business analysts. So yeah, I mean a pretty quick overview of things. Obviously in practice, they're going to be a lot more micro services that are going to be reading from the data warehouse. They're going to be reading from the time series database, they're going to be doing visualizations, there's going to be a ton of caches on top of all of these because you know when there's a ton of data, obviously we want to be caching a bunch of that. Especially if it's going to be recent and a lot of people are going to be looking at it. So this is kind of the initial building block for what it is that we're actually working with. However, at the end of the day, this is very much just a starting point and metrics and logs are only as useful as what you're actually going to do with them. So this is how we might populate them but at the end of the day you know you can make a whole other video devoted to just you know the types of insights that you might make once all of those logs are in a warehouse or in a time series database.
		- Okay guys, I hope this video was helpful for all of you, um i've still been enjoying making them and i think i have like probably another like 10 good video ideas in me for now and um yeah, when i finish all of those, then it's kind of a question of what this channel becomes. I mean i i really don't like making redundant videos and i don't think that's useful for either you or me because i don't want to waste your time. I feel like a lot of i don't know influencers will just like to state the same thing over, and over, and over again and it's really annoying for both me and you know anyone else who just watches them and gets their time wasted. Um, there's like i don't know a 50% chance that i just become a shill and you know start making garbage videos about days in my life and solving leetcode problems but hopefully it doesn't come to that and i can keep some good content coming out. Obviously if you guys have any ideas, feel free to let me know but at least for the time being, you know next few months, I have stuff to talk about and stuff I want to talk about and hopefully everyone's benefiting and learning from all this stuff. I appreciate all the comments, you guys are even teaching me sometimes and making some great suggestions and it goes to show to me that a lot of you were learning. So yeah let's keep after it.