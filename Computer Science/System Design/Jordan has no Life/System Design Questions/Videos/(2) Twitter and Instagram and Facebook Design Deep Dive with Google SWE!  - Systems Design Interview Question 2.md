---
Source:
  - https://www.youtube.com/watch?v=bOhQLr7nbhQ
---
- ![[Screenshot 2024-11-17 at 1.44.53 AM.png]]
	- Introduction
		- So today we're going to be doing [[facebook]], [[instagram]], and [[twitter]]. What do they have in common? Well actually it's that they're all ruining the world, but no, really it's more so that they have a news feed and that's kind of hard to come up with a good solution for because whatever, we'll talk about in a bit. But, the point is, this should be a useful video because after Elon fires everyone at twitter, they're going to need a bunch of new engineers and that could be all of you guys because now you're going to know how to build their entire website. So, with that being said, let's get started on this video. Uh, feel free to subscribe if you enjoyed it, and let me know how i can improve. See you guys in a bit 
	- Twitter, Instagram, Facebook (Functional Requirements)
		- Description
			- Functional Requirements
				- Make posts with text, images, or video
				- Follow other users
				- News Feed that quickly loads new posts from followers
			- Extended Requirements
				- Liking & Replying to posts
		- Okay, so let's talk about some of the functional requirements of our service so we can kind of hammer down what exactly is that we need to implement and as far as just looking at the requirements, I'm going to try and take kind of the common denominators of Instagram, Twitter, and Facebook, so i'm not going to be looking at any you know super specific services to each of those individual ones, you know, if you're looking at like messaging specifically, i'll do that in a separate video but for now, let's address the functional requirements as the following. Basically, users need to be able to make posts of some type that can contain either text or video or images as well, so basically, you know, just text that can fit in a database or some static content that you would probably end up putting in an object store. In addition to that, they should be able to follow other users and what that means is that if i'm going to follow someone else when i eventually go ahead and access my news feed, I'm going to see any new posts that they've made since the last time i've checked my news feed. So that's important and then the third one is that i am able to basically go ahead and generate this news feed of people that i follow with relatively low latency. If you're trying to be snarky and say, oh well on facebook, you have friends, let's just consider friends a dual directional follow relationship where i follow my friend and my friend also follows me back. We're both seeing each other on our news feed. Some extended requirements that i'm probably not going to go into as much in this video, but you guys can think about how you might implement them yourself because they're not overly complicated are things like liking a post and replying to a post and think about that more so in terms of the database schema and the type of database that may be best suited for that type of application. But I'll maybe touch upon that a little bit towards the end of the video. Okay, let's move on to some capacity estimations 
	- Twitter, Instagram, Facebook (Capacity Estimations)
		- Description
			- Data
				- 200 million daily active users
				- Each user follows 200 
				- 100 million new tweets per day
				- 300 bytes per tweet
			- Calculations
				- 30gb tweet storage per day
				- `55tb` of disk over 5 years
				- 180gb of cache per month
				- `6tb` of cache per day if each user's home feed is cached on a separate server
		- Alrighty, so as far as capacity estimates go, I'm going to use some of the numbers that i ripped from "grokking the systems design interview" and then i'll modify them slightly as i kind of see fit. So just pulling these straight from them, they basically assume 200 million daily active users for our social media service, they also assume that each user follows on average around 200 people. Keep in mind, that's the average case. It might be the case that certain users have millions of followers and you know that's obviously going to complicate things. Additionally, they assume that there are about 100 million new tweets per day, so on average half of the people of our service tweet per day, um, additionally, if a tweet is around 140 characters, remember that each character is two bytes in a database so we can assume that from the 280 bytes there plus around 30 metadata bytes of you know things like a time stamp or user id helping us to identify a tweet or if there's you know some sort of image or video content perhaps, something like an s3 url, we can probably cap that individual post size at around 300 bytes per post and as a result of that, that means that there are going to be 30 gigabytes per day of storage because that is 100 million new tweets per day times 300 bytes, that means over five years, we're storing 55 terabytes of data thus hinting that we should probably be partitioning our data. This probably isn't going to all be able to be stored over just one table, and you know, that's going to help us a lot plus it'll reduce the load on certain servers anyway. And then additionally, if we're going to cache 20% of our tweets because you know 20% of the tweets are responsible for 80% of the traffic, that's going to be 6 gigabytes per day of cache which means 180 gigabytes per month of cache. Again, technically you could fit it on one server which can have up to basically 256 gigabytes of ram, but it's probably a better idea to not do that and just partition it a little bit over multiple servers. Perhaps even by region or something like that. 
		- So we can talk about partitioning and replication and all of those things later in the video, but these are just some basic estimates to start thinking about to kind of get a sense of the scale that we're working with and as a result, because of the fact that we're operating at such big scale, we do definitely have to think about our performance here, especially considering the high magnitude of reads and news feed reads compared to the actual writes themselves. we're basically going to be assuming that reads are probably happening like 100 times as often as tweet writes are going down and as a result of that, we have to be optimizing for basically pulling all of those tweets from our news feed at once as opposed to going and loading those lazily. All right, let's start talking about some APIs
	- Twitter, Instagram, Facebook (API design)
		- Description
			- Iâ€™m dumb and forgot to mention POST follow(user1_id, user2_id, unfollow=false)
		- Okay, starting to actually think about our API design and kind of in terms of the end points that we might have for our social media service, we should probably have two main ones and then the extended requirements warrant a couple more, but basically, the main one is obviously going to be to create a post, so that might include something like the user id, perhaps some sort of authorization token in order to make sure that you're actually the user you claim you are, some metadata about the tweet, the content of the tweet, so those are all things that you're going to want when you're actually making one of these posts. Additionally, then we're going to be having a second endpoint. This one's just going to be a get, instead of a post endpoint, where we're actually going to be retrieving this news feed. In theory, this doesn't necessarily have to be an endpoint, it could be the case that, you know, the server is going to gradually be pushing new posts um to a client device and so, you know, then you wouldn't have an endpoint. You'd be using something like a WebSocket, but for the sake of simplicity, I really don't think we need real-time updates of the news feed at the moment, so i'm just going to list that as a get endpoint where it's just like, fetch news feed, and for that again, we would need user id, something like um, you know, an authorization token of some sort like a bearer token and then that's more or less it. And that's going to go ahead and basically return all the posts on a user's news feed, and then if we're starting to think about our extended requirements a little bit. Two extra endpoints, we might be adding our ones to like a post or you know unlike a post or just like toggle to like and then additionally also to reply to a post. So yeah, basically four endpoints in total but we're mainly just going to be focusing on those first two because those are kind of the more complex ones that we have to really think about the logic for. Okay, let's start talking about database schema
	- Twitter, Instagram, Facebook (Database Schema)
		- Description
			- Posts Table
				- UserId: int
				- content: string
				- metadata: json
			- Users Table
				- UserId: int
				- email: string
				- `pw_hash: string`
			- UserFollows Table
				- UserId1: int
				- UserId2: int
		- Okay, let's quickly talk about the database schema so that we can kind of decide on how we want to go ahead and format our data and the multiple tables that we're going to need in order to do so. So the first pretty obvious table that we're obviously going to have to have is like a posts table or like a tweets table and basically all it's going to have is some information like a user ID, the content of the tweet, so whether that's the actual text of it or you know, the s3 link corresponding to that. In addition, some metadata like the location of the person who made the post such as the time stamp that it was posted, things along those lines. So pretty simple for the most part. That's kind of the row that we're estimating is around 300 bytes. In addition to that, we need a users table where that's going to have something like a unique user ID. In addition to that, it's going to have things like their name, their email, hash of the password, etc, etc, creation date of the account. Another table, this is kind of going to be more relational, so this is kind of assuming that we're going to be using a relational database but maybe we'll go back on this once we actually think about how relational databases work or don't work for this problem is a user follow table where it basically just has a user one id and a user two id and all that happens is that user one is basically following user two and we have their ids for both and what we can go ahead and do is place indexes on both of those columns and that will allow us to really quickly query for basically, you know, who's following a given user or who follows a given user. So that's useful there and then finally, if we are kind of dealing with that extended scope of the problem, we could have tables for basically likes on a post and also replies on a post or um, you know, depending on how clever we want to be, we can even treat replies as like their own separate posts and give them like a parent id or something but that's again extended scope, we're not really going to be talking about that as much. I just kind of wanted to off-handedly mention it, and now, finally, we can get into the actual architecture of this problem which is probably going to take me a while to talk about, so we'll talk about things like replication and sharding first, and i'll hopefully be able to show off a diagram
- ![[Screenshot 2024-11-17 at 2.05.55 AM.png]]
	- Twitter, Instagram, Facebook (Architecture)
		- Okay, now for everyone's favorite part of the video, the actual architecture of the service. So before i go ahead and show you guys a diagram and explain exactly what technologies i used and kind of the entire layout of the service i'm planning on creating, let's first talk about kind of the actual central problem of building something like a social media platform. So the biggest thing here is that like i've mentioned previously, we have a lot more reads than writes, and so as a result, we really want to be optimizing for the scenario where a user is going to be pulling in their home feed. We don't really care as much about high latencies when a user makes a post because the truth of the matter is, it's only a small percentage of the time where they're going to be making a new tweet or a new instagram post or a new facebook post and as a result of that, if they have to experience slightly slower response times when they actually complete those actions, it's not really a huge deal. So with that being said, let's talk about first a naive solution for approaching this problem, and then why it might not be so great. So basically, the naive solution that we might have is to implement all of those tables that i mentioned previously with just some huge SQL relational database and we can just go ahead and vertically scale that, so that it's able to basically scale to the point where it can service all of our users. Every single time we post a tweet, it goes and just plops itself into the database as it should with you know some sort of unique tweet id and every single time that we load a feed and basically say okay let's see what new tweets i have to see, it goes ahead and does a huge join operation on that SQL table where the join operation is basically saying for user Jordan go ahead and find all of the people that i'm following and you know i've got some great followers. Might be Barrack Obama, might be you know Tom Holland etc, etc. Go do a huge join operation, find all of those follower IDs and then from there, find all of their tweets from these followers and limit that to say the 100 most recent and we can do that by just indexing on timestamp basically. So that's easy enough, right? But the issue is if every single person, every single time they want to get their news feed is calling this operation, that join operation is super expensive on the database. I mean, we're scanning over the entire table of tweets and in addition to that, you know, if everyone is going to be calling this, we're going to completely overload our sql database and it's going to run into an infeasible system and be a performance bottleneck really quickly. So instead, we have to be pretty smart about doing this. Unfortunately because i'm not that smart, the engineers at twitter have actually released a blog post basically on how they personally have managed to make this more of a solvable problem. So basically, what the naive solution does is it does all of the intense computations on the read path which means that as opposed to having a bunch of you know work being done every single time i write a tweet, every single time i request my newsfeed and look to read a bunch of stuff from the database, there's a ton of work being done. So instead, why don't we do more work on the write path which is something that is going to be happening less frequently because less tweets are being posted and that way we can go ahead and basically pre-compute our news feeds for users. So basically, instead of every tweet just going to the database, what's now going to happen is that every single time i write a tweet, it's going to go ahead and get sent to all of these different caches where each cache is basically representing the news feed of every single one of my followers and so the thing is here, this is all great and it's very doable using stream processing and i'll show that off in a little bit, but it comes with the caveat that say I'm Cristiano Ronaldo. I certainly have his looks but unfortunately not his soccer skills. Basically, if i have millions or billions of followers and i make a new post, it's infeasible for basically the back end of our application to go ahead and deliver that post to millions or billions of followers. So instead what we need is this kind of hybrid approach where for the typical average user with only a few hundred followers, we can go ahead and deliver that tweet to everyone's cache. However, if you have, you know, if you're a verified user and you have millions or billions of followers, what we need is for the work to be done in the read path so when we actually request that feed, we take some of the tweets from the cache, we request some of the tweets from the database, and then we basically combine those together based on timestamps. Okay, so now you might be saying, okay, we've gone through the general algorithm, but what are probably the best database choices to do this? Because yes, actually there is still going to be some relational stuff going on here like i mentioned at least for say the verified users, we're still going to be loading their tweets in a relational manner where we're going ahead and still performing that join operation. We're just not doing it for every single tweet possible, we're caching the majority of them. So as a result of that, considering the fact that these joins are going to be in the minority of the cases, what we're ultimately probably better off doing is using a NoSQL database like Cassandra and kind of central to whenever you say i'm going to use Cassandra is basically, how are you going to go ahead and partition this data. And so basically, we have now a few Cassandra tables, so let's go ahead and talk about all of them because i mentioned the database tables before, but now we have to really start thinking about how this is going to work as far as Cassandra goes. So because there's no relational model anymore, there's no such thing as a user followers table. Sure, we can have our users table. Sure we can have our posts table, but now we have to explicitly keep track of a list of people that each user is following and that each user is followed by. And so as a result of that, we basically have these two separate lists. So every single following operation that you know is going to our API is going to go ahead and basically update both of these two lists and so that's going to require some sort of cross-partition transaction which is pretty unfortunate but fortunately basically following operations don't occur that often, so we don't really have to worry about this too much, so we're basically maintaining all these huge lists of followers and followings and then additionally now as far as actually going ahead and sharding the post table because this is where kind of the main sharding logic comes into play, we have a couple of options. We can just basically randomly shard posts around by giving them some sort of arbitrarily assigned post id and then effectively hash those or we can try and do it in a little bit of a more clever method and actually keep the posts from the same users together. Now this is actually where i disagree with "grokking the systems design interview" because that book goes ahead and basically says, yeah, just randomly distribute all these posts everywhere all over our shards and i personally think that's stupid. The reason for this being that it is very common functionality for you to say, go to your instagram profile, and want to be able to see all of your existing posts and the way to do that is by grouping them all together by users. Is it true that one user in particular might have an absolute crap ton of posts that might overwhelm a single shard, yes absolutely, however there's still ways that you can mitigate this problem by basically go ahead and adding some you know random number to every single user id for particularly, um, you know, high posting users and then that way you can further distribute all of their posts over the shards and then when you're loading their posts, you kind of have to do the same logic and go ahead and load from all the shards that you further distributed to which is annoying, but in my opinion, this is a lot more feasible than just randomly distributing posts everywhere because when you do that and you know you're eventually going to have to run these operations where you want to fetch all of the posts from a given user, that's going to be really annoying if you're doing this and all of the posts for you know say my own user are going to be on completely different shards. So i personally think you should be sharding by user ID and then using timestamp as a sort key. However, I guess you can make the argument for otherwise. The point is you just have to justify yourself, right? So as far as replication goes, if i'm choosing Cassandra, that basically means that we're using leaderless replication which further implies that we have this eventual consistency and also the potential for some right conflicts. However, because we have this kind of timestamp parameter for every single post, there really shouldn't be any write conflicts going on. Um I, can definitely think of edge cases that might occur as far as some of the following and follower stuff goes, but at the end of the day, I really don't think it's too huge of a deal and because it's social media, you know, we're not dealing with financial data, I think that eventual consistency is fine here and if there is the occasional write conflict and one write gets clobbered by last write wins, I really don't think it's the end of the world. As far as caching goes because this is kind of important to talk about, basically like i mentioned, it's very possible that a certain user or a certain post in particular is going to be very hot and as a result, everyone kind of wants to read from that database partition. For example, like i said, if Ronaldo goes ahead and makes a post because he's a verified user, everyone on their newsfeed is going to actually be hitting that shard. However, what you can go ahead and do is just go ahead and cache those super popular posts and that's kind of just like our individual hot post cache but in addition to that, like i mentioned before, we have feed caches for every single user which can and should be replicated. So you can basically just throw the entire feed of a given user on one of these redis instances which is then replicated to a couple of other redis instances so that, you know, we have high availability when trying to go ahead and access a user's feed. Furthermore, as far as just the images and video component of our social media service goes, we should pretty much always be storing static media in an object store like amazon s3. The reason for this mainly being that it is elastic in nature if we were to run HDFS or Hadoop, then we wouldn't be able to go ahead and scale this out super easily. We would have to actually add more physical servers to our cluster. On the other hand, s3 scales out really easily for you and it's pretty performant especially when you add a CDN to that which we certainly should for popular images and popular tweets. And then the final thing, I just quickly want to touch upon is as far as like load balancing and scaling out our servers goes, it's important that because our actual like write servers or rather our write service where we're actually posting a new tweet and reading from our newsfeed are at kind of completely different levels of scale, right? One is being accessed more than multiple orders of magnitude than the other, basically we should probably be operating those as separate micro services and that way we can have separate load balancers for each and scale them independently. Okay, so those are kind of the major considerations to think about as far as replication goes, partitioning, caching, load balancing, but now i'll actually go ahead and dig out a diagram that i'll make myself which is going to be i guess a little bit complicated for this one and we can go through every single chunk and i'll describe the certain technologies that would probably be best used to go ahead and implement something like this. 
	- Diagram
		- This thing got complicated. I'm gonna need a bigger whiteboard. Okay, but well, let's go through it. So, basically we start with our client and obviously we're gonna have some sort of load balancer if we want more redundancy in order to make sure this isn't a single point of failure putting this in an active-active or an active-passive configuration would work just fine. Additionally, I actually forgot to previously mention a follow service because this is kind of fundamental to how we make sure that when we process posts, our followers are actually going to be up to date. So basically, here's what happens. If i as a client want to go ahead and make a follow, i'm going to make a request to our follow microservice which is then going to do basically two writes and we're going to hopefully make sure that both of them go through either via two-phase commit or some other less valid protocol that isn't guaranteed to make both work but something along those lines. So basically we're gonna update the followers and the followings whenever we make one of these follows, put that in our user follows Cassandra table and then what we're going to do is basically go ahead and stream those changes from the Cassandra table which is known as change data to a log based message broker. The reason we want a log based one is so that we can replay the changes if it were to go down because it's very important that we're able to basically keep the local copy of this user follows Cassandra table (also because order matters in updates!!) up to date in our stream consumer for the post. So basically in order to go ahead and consume this stream, we're using a technology called flank which is super useful for basically keeping this state local to the actual cluster of nodes which are going to be consuming from that other post stream which i'll be talking about in a little bit and then by doing so, we can make sure that the posts are basically going to be directed to the proper caches for user feeds. I know i just went through a lot there but basically the gist is this. Once we make a post, the post is going to go to an in-memory message broker. The reason we use in-memory this time instead of a log-based message broker is because we care about faster processing and we don't really care about the order of the processing. Also even though i didn't really write it out here, we can basically go ahead and assume that by virtue of the technologies that we're using whether it's Kafka or RabbitMQ for the memory based message broker, we are going to be partitioning and replicating these queues to ensure high availability so they're not single points of failure and basically all of these posts are going to go into this in-memory message broker whether that's RabbitMQ or something like amazon SQS where after that they're going into our flink consumer nodes where basically the flink consumer nodes are going to go ahead and load all of the um followers of that user who posted the tweet and then it's going to place that tweet into all of their respected feed caches. The feed caches are then basically just a Redis instance or a memcache instance which can be replicated for further availability and then in addition to not only putting them in the feed cache, we also want to be putting them into a separate Cassandra table which actually contain all of the posts if we ever just want to, you know, say load the post for one user. It's important to kind of have a durable actual representation of these posts and not just only put them in the cache. So then once we have those posts in the Cassandra database, you know, we can actually start using a cache on top of that because like i said certain posts in the database are going to be accessed a lot if they're from someone like a celebrity and as a result we have this second post cache here which can just go ahead and pull basically instances of a post out of that cache whenever they're needed. We can just use an LRU policy for all of the caches involved here because that tends to be sufficient. The last component of the service that i have yet to mention because i forgot to is the fact that whenever a post is made, if it's going to have any static content such as an image or a video, that's going to first go into s3 so that we can get the url for that, and then eventually from s3 if it's super popular, maybe pulled into our CDN which will hopefully be located at least geographically close to our client. 
- Conclusion
	- oh man, like i said it was gonna be a lot but i mean it should make sense. This is more or less what they actually do, so yeah, as you can see this turned out to be a stream processing problem because the truth is SQL just isn't sufficient at scale a lot of the time. Okay, in conclusion i hope this video is useful. As always, feel free to leave any questions in the comment section. You guys seem to like it when you get to see me nice and up close and you know very detailed so i'm okay with it. I kind of like just banging out power points but whatever let's do this. And uh to those of you who are new to the channel because i think there may be some depending on when this video gets posted, Have fun. Stick around. Welcome to the community