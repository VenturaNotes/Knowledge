---
Source:
  - https://www.youtube.com/watch?v=XbkjEX-jgj0
Reviewed: false
---
- ![[Screenshot 2024-11-27 at 2.08.28 AM.png]]
	- Introduction
		- Today we are going to be doing a problem that i will dub the leaderboard problem, but you may see elsewhere called the top k problem. I'd like to thank one of my viewers Franklin Yao for some of the inspiration on this one since uh kind of suggested it and i was able to merge it in with some problem that i was basically going to be doing anyway. So this one's kind of convoluted in the sense that there are a bunch of basically different possible requirements and i'm going to go through hopefully all of the possible solutions for the problem, a lot of this video is going to be heavily plagiarized from other channels, so uh yeah you know don't throw me in the fire on this one, but uh anyways other than that, yeah just quick announcement, thanks for watching everyone. Tons of new subscribers which is really awesome to see. Made some youtube shorts which i like doing. Uh yeah, without further ado, let's get into it. Make another systems design video so i can go to bed.
	- Leaderboard Problem (Functional Requirements)
		- Description
			- Measure the top k events of any type
			- Be able to get results for any window of time
		- All right as per usual, I'll be starting this one off by talking about functional requirements. So in this case, basically i mentioned that we want to be building some sort of leaderboard and the question now is what actually goes in this leaderboard. Well truthfully, it could be anything. The general premise is we just want to be able to measure basically the most occurring of some sort of event that occurs on our platform, that could be views across all youtube videos, it could be searches with a given keyword or a query term, it could even literally be the score of a video game. The point is, it's just going to be of some event that has a very commonly occurring type of thing and basically as a result, the throughput is going to be too high to just compile it on one server, otherwise that would be a very simple solution to do this and it would make our life nice and easy. Additionally another thing that's going to make this problem a little bit harder and probably would vary from interviewer to interviewer is the fact that the leaderboard is not just over the all-time window but it can be on any time interval you might request. So as a result of that, you know you have to start thinking about the types of aggregations that we can do in order to speed this process up and you know perhaps whether they might even be exactly correct or just approximations. So without further ado, let's do some capacity estimates
	- Leaderboard Problem (Capacity Estimates)
		- Description
			- k < 1000
			- 1 billion relevant event updates per day
		- Um talking about capacity estimates here, it's hard for me to really give any concrete numbers because again, this isn't really a concrete problem, but i will just give some amount of detail to give us a sense of the scale of this problem and, you know, why it's not necessarily so easy. So i mentioned that we kind of want the top k count or just the leaderboard where let's say k is less than a thousand because if k is more than a thousand, we're gonna have some really really intense computation and there's probably zero chance that we can do any of it on the fly which we're gonna try to do at some extent um but additionally, uh let's imagine that we have say a billion updates to the items that we care about per day that could potentially be comprising our leaderboard and so again it's just too much to be handled in a single server. We're going to have to figure out ways to either you know kind of manage our memory efficiently or do this in a distributed way
	- Leaderboard Problem (API spec)
		- Description
			- fetchLeaderboard(k, startTime, endTime)
		- As far as an api spec for our service, obviously this is going to be specific for whatever platform it is that we're trying to get the top k events for, however, one endpoint that we are certainly going to have is just to actually get that leaderboard and that's basically just going to take in a start point and an endpoint and i suppose the parameter k as well where like i said k is probably going to be less than a thousand.
- ![[Screenshot 2024-11-27 at 2.24.07 AM.png]]
	- Leaderboard Problem (Database Schema)
		- As far as a database design, I'm sure the platform itself will have plenty. In terms of what we might want, uh perhaps we want a database just to store some historical calls to you know the leaderboard function if we want to kind of cache those but generally speaking, I wouldn't really concern ourselves with any specific tables. I will mention tables later in this video but they're not necessary and it kind of depends on the implementation that we ultimately decide to go with. So for now, I'll kind of leave this one open and we'll talk about the algorithm so i start to make a little bit more sense but uh yeah, like i said, this is a super open-ended problem and that's kind of what makes it fun to talk about.
	- Leaderboard Problem (Architectural Overview)
		- All right, let's start getting into the details of what we can do to actually go ahead and calculate these top k suggestions or events or whatever they may be. Basically, the first step that we have to realize is like a bunch of other systems design problems that we've done in the past, there's going to be a ton of data that we're dealing with and as a result of that, we can't calculate it on the fly. We're going to have to do a ton of more work on the write path in order to basically pre-compute our results for the read path. This is very similar to you know a pattern that we saw with our social media service like with our news feeds. So again the point is we just want to be doing as much work as we can in advance. Do all the pre-computing that we possibly can but still keep things relatively low latency when it's possible so we can at least return you know somewhat up-to-date data. Okay, so basically i'm going to start with the following premise. If we just needed one leaderboard for all time, it would be pretty easy. Why? Because basically every single event that happened, we could put it into some log-based message broker. We could go ahead and pull that using a stream processing framework and basically partition it on the ID of the event, and then on every single individual node, we could go ahead and calculate the counts of all the events that that node is seeing, so now we have all of these basically top k lists for every single individual node and then what we could do is add batch intervals. We could periodically merge those together. 
		- (1) Now just for completion sake, even though this isn't like an algorithms based video, in terms of the way that we would keep track of the top k items on a given node, you could use something like a min heap of size k and what a min heap does is basically give you really quick accesses to the lowest element and as a result of that, you can really quickly swap out an element if it's no longer in the top k and put in the the relevant element that's in the top case so i went ahead and made a diagram for you guys to show that off and that's got pretty good time complexity compared to just something like i don't know sorting and then taking the first few elements. 
		- Okay, so now once we have all of these lists on each of these stream processing nodes, we could go ahead and merge those together and we've seen merging sorted lists all the time in these kind of systems design videos. It's pretty much very central to the concept of something like SSTables, so you should be familiar with that by now and if you wanted to improve that even more, you could technically use a heap to do that too where you basically have a max heap pointing to the first element of every single list that needs to be merged together and then as you pull an element from a list to merge in, you move the pointer of that heap but again, you could kind of look into that a little bit more if you want to. That's more of a leetcode style problem then anyway. And this merging of list could be done, you know, say every hour using something like a batch job. So there are a couple of issues with this implementation. The first is that we want to be able to select any time window and what i just described, this kind of process of keeping running tallies of all the lists on every single stream processing node and then eventually merging them together is not something that allows you to kind of select any time window that you want, but rather it can only get you a leaderboard for all time. The second issue with this implementation is that we're running it in batch, and oftentimes you know, we don't want uh data at the granularity of an hour. Maybe we want to know the most popular searches of say the last 30 seconds or the most popular videos of the last 10 minutes, and so as a result by using this batch computation, we lose a lot of flexibility in our design. So let's first talk about the kind of time window flexibility, what can we actually do to do that? Well here's one possible naive solution but that being said, even though it's a naive solution, it will at least be accurate and the first thing we could do is from our message broker, instead of sending all of these messages to the stream processors, what we could do is just use something like a time series database as a sync and that way at least from a time series database because of the column oriented storage because of the fact that they kind of do smart things with indexes (TSDB will probably have to be sharded on event type and then aggregated too, use consistent hashing to avoid hotspots) in order to improve the caching there and reading and writing to and from that caching, we should be able to get very fast reads and as a result, perform some aggregations on the fly. It's not an ideal method in terms of performance, but if we need exact numbers and we need to be able to do it over any window, a time series database is probably our best bet. So that's going to be option one, but like I mentioned, even the time series database is pretty slow. Effectively we would have to do all the work in the read path and that's something that we basically said we want to be rolling out. So what if we relax one of our requirements and that requirement is basically that we need to have exactly correct results. We can eventually get exactly correct results if we were to say put stuff in a time series database and recalculate it down the line, but what if at least in shorter intervals it's okay if we were to be approximately correct. Well then one option we have is a very similar process to our original one where we're basically pulling things from our stream processors but let's say now that instead of keeping a fixed count of basically the frequency of all elements of all time, what we would do is say after every 10 minutes, we would reset the counts on all of our stream processing nodes and then that way when we're running our batch job every single 10 minute span, we can go ahead and get the leaderboard in 10 minute intervals and then if we needed say the leaderboard over an hour, we could merge together those 10 minute interval leaderboards kind of like what we've discussed in the past with hopping and tumbling windows. So that is a viable solution, however, like I said, it's not exact. The reasoning being that say you know in one of those 10-minute intervals, the element with the 11th highest count actually had a pretty significant count and by leaving that out when calculating our hour interval, now that element isn't relevant anymore even though it would have been in reality. So as you can see, it's clearly an approximation to do these kind of micro windows and then merge them together is not perfect but it will be a lot faster and also gives us a lot more abilities in terms of flexibility with our timing window compared to a time series database, but what if we wanted to do things really fast because right here, we're still using a batch job at the end of the day, and what that means is that basically we're only able to get results of the granularity of every few minutes because you know there's a ton of network I/O in having to get all of the data on all of these different stream processing nodes and then having to merge them together. So we can't do it that fast. The only way we could really do a super fast computation is if we kept all of the counts data on a single node which in theory to do this perfectly and you know get accurate results is pretty impossible because like i said, there's a ton of events coming in and as a result of that, we can't keep them all in memory and you know have them in a list and iterate through them and do this whole heap algorithm. It would literally be too much memory for a node in our system, but what if we came up with an approximation in order to get the counts of the top k elements while also minimizing our memory footprint. This is where something known as count min sketch comes in and i'll explain this with a diagram. 
		- (2) So basically what count min sketch is, is effectively a 2d array and each row of the array represents a possible output of a hash function and then you can have multiple hash functions. So now we have this fixed size 2d array and so for every element that comes along, what you do is you keep track of it and then you'll put tallies for every single cell where the hash function lands, you know say there are three hash functions, like there are in my diagram and say the letter "A" comes up and "A" first marks these first three cells, I'm going to put a 1 next to those tally, and every single time that same element is going to come up, you're going to increase the tally in those cells. Now you might say to yourself, oh wait, well it's actually possible that you know due to the fact that these hash functions are kind of random and you know they're not infinitely sized, if we have a ton of elements, there's going to be some overlap in the cells.
		- (2.4) And you know a cell that might correspond to the letter "A" might also receive tallies from the letter "B" coming up. Well that's true but probabilistically, we can get a pretty good estimate of the count of each element if out of all the cells corresponding to the letter "A", basically all the hash results from hashing "A" with each of the functions, if we take the minimum tally from all of those cells, we have a pretty good approximation of the count of that element "A" and the same goes for "B", "C" and "D" and by doing this, we can basically keep the computation for determining the counts of all of these elements into a fixed amount of memory without having to kind of expand in a linear matter with respect to the number of elements that we're encountering 
		- And so as a result of this, we can keep taking in elements from something like a stream, you know, putting them into our count min sketch and then throwing them out after the fact and by doing so, we can really efficiently kind of approximate the counts of all of these elements and what this even helps us do further, is say you know after 15 seconds, we no longer care about these elements, we can throw out our count mint sketch, initialize a new one, and then we can start aggregating these small approximate windows of all the counts within that time and what that's really useful for is as opposed to something like a spark streaming solution where we have to have all this node coordination over the network to merge these lists, we've effectively done all of our computation on just one node and so it's probably not as accurate but it's going to be super fast and it allows us to get these leaderboards at very small granularities. Now you may say to yourself, okay fair enough, but now we have this one node and how is it that we can kind of replicate this functionality to make sure it's at least fault tolerant, and if you've watched my stock exchange video, what you may be thinking to yourself is actually well let's use state machine replication. We know that if we're using a log based message broker, I could literally set up a second node that does identical calculations to this first node and as a result of that, we can have them running the same exact operations and getting the same exact counts because the log based message broker at the end of the day is not going to drop messages, so we know we can pull all of those in at least once eventually and so this is a really great thing in the sense that now we have this replicated service where we're able to get approximations of the top k accounts for very small time intervals and so now we have all this flexibility depending on the size of the interval that we're being asked for and perhaps whether we care about whether it's an exact leaderboard or it's an approximate leaderboard, we've basically delineated four possible solutions for how we can go ahead and get those results. So i'm now going to turn over to the diagram where i can kind of draw out these solutions for you and hopefully you know for those of you who are visual learners like myself, make this make a little bit more sense.
- ![[Screenshot 2024-11-27 at 2.27.54 AM.png]]
	- Diagram
		- All right, so the following diagram that i'm going to be implementing here is what's actually known colloquially as something called the [[lambda architecture]] in the sense that it's got both batch and stream processing present and the point of the stream processing, is you know to get some instant results and the point of the batch processing is to get accurate results later down the line, but anyway, as per usual, we're starting with a client who talks to a load balancer who in this case will be talking to a leaderboard server which we can just go ahead and horizontally shard for fault tolerance or rather horizontally scale and then from there, we can push all of these messages into our Kafka broker which is going to be log based in order to keep those messages durable. We can also shard these messages over the brokers in kind of clever ways like i said if we're going to a spark cluster in order to do some aggregation here, what we want to be doing is sharding by some sort of event ID such that you know all the events that we want to be counting that are the same are going to be kept on the same processing node and that way we're able to kind of merge those lists together at the end of things. So let's start with the bottom solution first because that was kind of my baseline solution which is, if we want an exact leaderboard over all time. So all that's going to be happening here is again, all of these events are being sharded by event ID. Every single node in that Hadoop cluster which is going to be running spark streaming is going to keep a localized count of all the elements that it's seen, and then occasionally you can run a spark job to go ahead and merge those lists into one list of size k and then finally just place that into some cache, maybe a database or wherever, but the point is that we now have easy access to this computation. Another possibility is say we want this exact solution over any time window but it's at the cost of being a little bit slower, so this is the second possible way of doing things from the bottom. What's going to happen here now, is that instead of ingesting things into a stream processing consumer, we're just going to go and ingest them into a time series database and if we ever need to make aggregations down the line, we can just run a batch job to do so or perhaps if you know it's by request of a given user, maybe you can just go ahead and have a server call those and pull that from the database. Okay, the third solution is now going to be an approximate solution but with a slightly larger window and like i mentioned, we can only do a little bit larger windows with this because at the end of the day, network latency is going to limit us a lot from doing solutions probably in the span of like seconds and so this is where you again have some sort of spark streaming processors and they're basically aggregating the data but what they'll do instead, is they'll create um you know hour-long windows of the data, throw out all the rest, upload that to a database like MySQL because in theory uh a MySQL database is probably going to be a little bit better for reading due to the B-tree architecture as opposed to an LSM-tree and then after putting it in the database, they can go ahead and reset all of their local counts and then start aggregating data for that next hour. Okay, and then the final possible solution is going to be the one using count min sketch where you can see i've got a couple of count min sketch servers basically both receiving messages from Kafka doing that state machine replication, ultimately setting up their 2d matrices over very small windows of time and then once they go ahead and compute that, either they can offload it to something like a cache, they can plop it in a database or maybe they can even just keep it around if you want to make a bigger window eventually. But yeah, those are kind of the four possible tenets of this solution. I'm curious to hear if you guys think there are any others that i'm leaving out. Obviously there are going to be very naive ones where you just, you know, throw things into a database and lock them and count them but that's going to be super slow and infeasible, so i think these are probably the four most legitimate ways, but uh yeah, I hope uh that makes sense for the most part 
		- All right guys well i hope this was a useful video. I know it's a little bit abstract but uh yeah i just think it kind of combines a lot of the topics that we've been trying to study on here and talk about together in a very interesting way and even though there's no perfect solution which is a little bit frustrating for me personally because i like when things kind of just you know come together and there's a nice solution, the truth of the matter is that a lot of times in systems design, there is no perfect solution and uh you just gotta work it out and you know if things get hacky, things get hacky.