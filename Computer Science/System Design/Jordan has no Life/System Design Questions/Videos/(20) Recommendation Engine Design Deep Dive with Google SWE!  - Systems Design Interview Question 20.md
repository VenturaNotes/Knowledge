---
Source:
  - https://www.youtube.com/watch?v=ZAIBFf8KpgI
---
- ![[Screenshot 2024-11-27 at 2.48.44 AM.png]]
	- Introduction
		- hello everybody and welcome back to the channel. I'm glad we get to make another one so soon. A couple of announcements uh not really sure what my weekend upload schedule is going to be because I'm actually moving into a new place which is going to be sweet and then bigger announcement, I've actually got some dry erase markers back and that means we don't have to do this paper anymore which is great so now I can actually show you guys clear images of what I'm writing, but anyways, we're going to be doing a recommendation service today which is sweet because it means that now that I figured this, out I can finally build my billion dollar recommendation service idea which is going to be very useful for me personally and I'm sure a lot of other happy individuals around the world are going to love it, but anyways let's just jump into this, uh, it's a cool problem and if you don't know too much about machine learning, you're going to learn a little bit, but I'm not going to dive too deep into things but uh yeah it's just an interesting problem so let's go ahead and get into things.
	- Recommendation Engine (Functional Requirements)
		- Description
			- Recommendation engine for items on site
		- All right so this problem is purposefully kind of vague and that's so I can talk about a lot for it but uh I'll still do my best to go through my typical loop of you know functional requirements and so on. So starting out with those functional requirements, more or less, there's only one and it's going to be build a recommendation engine for our service. I mean you're all watching me on YouTube which means you all know what a recommendation engine is which means that you all understand that when you log onto the site, you're getting recommended a bunch of videos or I'll just call them items throughout the video in order to basically represent your past interests and what you might be interested in clicking on again. So now that we've covered that, let's get into some capacity estimates. 
	- Recommendation Engine (Capacity Estimates)
		- Description
			- 1 billion total users
			- 100 million new items added per day
		- Okay, so in terms of estimating capacity, let's just assume there are a billion users so we're operating at huge scale here, we're going to have to do a bunch of distributed computation and additionally that they're going to be say 100 million new items added per day. I don't think that's that unreasonable because it just means 10% of your user base is you know doing something on your platform adding a new item. You know let's imagine we're talking about YouTube or TikTok here, I think this is pretty reasonable. So anyways, um yeah obviously we're going to have a ton of recommendations that we need to provide. It's going to take a lot of storage to store all of the links to our videos or whatever it is that we're recommending, and more importantly, it may even be the case that we want to kind of adapt our architecture a little bit to account for the fact that there are so many new items constantly being added to our service and I'll talk about that a little bit later but the the route that you may think we're going to be going down, it might actually be a little bit different.
	- Recommendation Engine (API Design)
		- Description
			- recommendations(userId)
		- Alrighty, in terms of our API design, it's not going to be anything particularly complicated because we're not getting overly specific into who exactly is running our service here, let's just go ahead and assume that we really only have one endpoint that we care about and that's to get recommendations for a given user ID 
	- Recommendation Engine (Database Schema)
		- And hopefully my camera chills out because it's focusing more than a kid on Adderall before his midterms, but anyways the last piece of the puzzle here before we get into an actual design is going to be the database schema for which you know again nothing really too specific, there's going to be one for the items, whatever those may be, you know, it could be videos, could be clothes, could be you know Amazon products or anything like that, anything we're providing recommendations for and then additionally, just possibly uh the actual recommendations themselves if we're going to be caching them or something along those lines, but we'll discuss that more so in the next section 
- ![[Screenshot 2024-11-27 at 3.04.23 AM.png]]
	- Recommendation Engine (Architectural Overview)
		- All right everybody prepare yourselves for a nice ramble that I'm about to go on. So anyway, here's what we do to go ahead and design a recommendation engine, Basically if you're thinking about this, um the first thing that should probably pop into your mind is kind of this idea of batch computation and if you're familiar with machine learning at all, basically the general premise is you have all this training data which is basically saying like here's who your user is, here's what they've looked at in the past, here's some details about what the items we have are and then we're trying to make predictions and saying for a given user and a given item ultimately, how likely is it that this user is going to be interested in this item. How likely is it that the user is going to buy this piece of clothing. How likely is it that they're going to watch this video and so that's actually a pretty complex computation. If you know anything about machine learning, what this effectively means is that we're going to be creating some huge basically set of matrices or possibly even functions connecting them and as a result of that, it takes a lot of time and space to go ahead and compute those. So typically this is something done in a batch computation or done offline.
		- (1) And that's really nice because what it means is that, you know, if we're going and pulling in a bunch of things, we can basically go and ingest all the new items and every single day we take in our user data, we take in our item data, and we take in our existing model and then we can run a once per day spark job that basically goes ahead and populates an existing cache with all of the recommendations that we want and this is great and it means that you know every single time a user is going to click one of our suggestions or perhaps even not click one of our suggestions, we can throw that into a Kafka queue and ingest it back into our Hadoop cluster so the following day, we can account for that information, perhaps update our model, and of course make new predictions. This is really nice because it doesn't really give many requirements in terms of the availability of our service. Since it's just a batch job, all it really means is that we don't have to have a bunch of on-call Engineers or anything like that, and we're just running it once per day. It doesn't have to be a real-time thing and it's very easy to manage. 
		- However, even though this is kind of an obvious solution to the problem, a lot of companies are actually kind of moving a little bit away from this type of batch computation for machine learning. So why is this actually the case. Well there are a couple of reasons. One is that in that batch job, we're going to be computing recommendations for every single user. This is something that is going to waste a lot of compute because let's say, you know, only 50% of users are actually going to come to your site daily. Now you've used double the compute for no reason. Additionally, what if we have a site where there's new content that's constantly being added. Take TikTok for example. People are posting five, ten times a day on their accounts and if we're only running a batch job once per day to create recommendations, people aren't going to see these videos until after a day and that's kind of problematic. So in a lot of cases, people are actually kind of moving towards this real-time workflow. So even though I've gone ahead and written out this batch diagram, since this is kind of a relatively simple thing to do, I'm actually going to be talking about the real-time recommendation a little bit more because again, this is kind of where the technology is becoming a little bit more cutting edge. A lot of companies are moving towards this real-time machine learning for a decent amount of applications because like I said, it incorporates more data, it can provide more up-to-date suggestions, and if you're a new user and you know you're visiting a site for the first time, you don't want to have to wait a day for your recommendations to come in, or else you may be less likely to come back. So as a result of that, this real-time recommendation concept is also coming in which is funny because it kind of goes against everything that we've been saying in our systems design learning up to this point. We've basically been saying, hey you know like look at Twitter feeds for example. We want to be doing all the work on the write path. Let's pre-populate these caches so the common operation of reading is super fast whereas in this real-time recommendation engine, we're basically throwing that all out and saying whatever, let's just make the best possible suggestions and you know perhaps sacrifice a little bit of latency. So obviously I went over the batch case for the purpose of completion's sake but I'm also going to go into this real-time recommendation engine because it's pretty interesting in terms of the machine learning steps that are required to do it and also the architecture that you might use. So basically think about it under the following premise. The real-time prediction is done in two major steps and this is basically assuming that I'm a user, I'm loading YouTube, I'm hitting an API and the API is returning suggestions to me that were not actually pre-computed. So the two steps of real-time predictions are retrieval and ranking. And so retrieval is basically saying the following: once I go on to the site, our back-end is going to load, say, a few thousand suggestions that aren't necessarily super accurate for me personally, but this is a pretty quick operation so we can load a bunch of possible candidates which is what they're called and then once we have these few thousand candidates, we can then narrow them down in the ranking phase by saying okay, here are the top 10 to 15 candidates and those are going to be the ones that I end up showing to the end user. So obviously ranking is going to be a lot slower, you're doing a lot more computation tailored to the actual user itself whereas with retrieval, we'll discuss it a little bit more, but you're basically just you know finding similar content to what they've looked at or liked in the past. So how does retrieval actually work? 
		- (2) Well retrieval is heavily based off this concept of embeddings, so if you don't know what these are, I'm going to explain it now. Basically embeddings are saying, take anything that you know, you might be trying to provide recommendations on, that could be words, it could be images, it could be product data and turn it into a vector, a multi-dimensional vector. So as you can see on the screen, what I've done is I've taken a few words and I've turned them into vectors and this is a common thing in natural language processing. Now I've only done it with two Dimensions here but in reality, these might be 32 or 64 dimensional vectors and you know obviously it's going to be very hard to visualize those, but as you can see, these three animals, you know, cat, dog, and puppy are going to have pretty close vectors but that last word over there which is not necessarily similar to an animal is going to have a vector that is not near them on the coordinate plane.
		- So in the same way, basically the first step of the retrieval process is understanding that all possible products or items or whatever it is that you're trying to provide recommendations on, can be turned into vectors and the way that you actually turn these things into vectors is you're actually using a machine learning model to do this, and in this particular case, that's called an embedding model and you can train that embedding model offline, so that's not going to be part of our actual read path. We just have an already existing embedding model and it's going to basically take in some of the things that the user has interacted with in the past and start creating candidates based on the most similar embeddings to the things that the candidate has interacted with in the past or rather the user has interacted with the past. So to make this more concrete, let's say I go to YouTube, I want to watch a new video and it's going to say okay, let's look at your five most recently watched videos and find a thousand embeddings that are closest to each of those five. So there are a few things now that are going to happen. The first thing is that (A) getting the candidates for you know the thousand for each of those five videos, one should be distributed over multiple servers, you can just put the embedding model on multiple servers and two, what you can also do is create an index over the actual near embeddings because now what we have, or we have all these vectors and we want to calculate basically a few thousand of the closest ones. So this may pop something into your head, and that's actually that we could use something like a geospatial index here. Even though in the past I've talked about geospatial indexes in two Dimensions, theoretically there's no reason we can't create those bounding boxes like we've done in the past but done them in even more dimensions and then we just break the entire space down into smaller and smaller dimensions and so people will actually do that. However, since we have this kind of offline component of our service, we can actually do something that's a little bit more efficient than just having um kind of this geospatial index. What we can actually do is just say for each embedding, calculate the thousand (1000 nearest neighbor vectors) most offline in a batch process and save them in some index and then that way, we can really quickly just directly fetch the thousand for each embedding instead of having to do a sort of binary search like we would with a geospatial index. So instead of being a logarithmic complexity, it goes to a constant time complexity at the cost of having to use some more space. So now basically we have two components that we've discussed so far. We have this embedding model and then we also have an index saying for each embedding, here is its nearest neighbors and that's going to be the retrieval step and the last thing that we have to be able to do in the retrieval step is say okay, well we have a bunch of possible candidates but what if there are some candidates that we know are never going to be a solution. What if we want to Blacklist some candidates because say I don't know say you've already seen a video or we're on a kids video account and one of these has curse words in it or you know we've already bought this product. So we don't want to be even considering those as candidates anymore. Well one optimization that we can actually run in order to do this kind of filtering out really quickly is Bloom filters and this is actually a really interesting application of them. So I have a dedicated video on Bloom filters and you're welcome to go watch that but basically the point of Bloom filters is that for a bunch of items in a set and in this case the set are going to be candidates, what you can do is you hash them all to a few different hash functions and then that way, you can very quickly tell if a given item is not in the set. So for example, if I'm going to say yeah make sure nothing in the set of horror movies is in our set of candidates, we can check our Bloom filter, hash all of the horror movies and see if any of the horror movies may be in our set and if they're not, we can skip over the whole going through our set and pulling out the horror movies. So again watch the bloom filter video for more information, it'll be more concrete. But that is an optimization that you can do and it seems like it is used in practice. Okay, so now we've gone completely through the retrieval phase. We've gathered all our candidates, we filtered out any potentially bad candidates and now we basically have all of these things. So what can we do from here? Well this is the stage where we're actually going to be doing the rankings. Now similarly to the retrieval phase, for the rankings again, we should (A) be distributing things because we're going to have literally thousands of candidates that should be sent to multiple different nodes in order to be processed because the model is going to be the same on each of them and all we care about is the output, and then more importantly, once we get the output of all these different candidates, we know we have to sort them and this is actually pretty fun because like literally the last video I made on this channel, I talked about how you can go ahead and merge a bunch of lists of size K and this is kind of the same thing. You have all of these sorted lists on each node of the highest scores and then all you have to do is you merge them right together and that should be a relatively quick operation that you can then pass back to the user to get the few videos with the highest possible score. The final thing to note is that again, like retrieval, the actual models themselves for the ranking should probably be calculated offline. It is the case even that it doesn't necessarily have to be an entire neural network, they could be very simple models. Something like logistic regression which I'm not really going to get too much into because again, that starts becoming more machine learning but the point is, we can use a daily spark job to calculate these models and then place them on our ranking servers, and then finally, of course in order to kind of keep our machine learning model up to date and giving it the ability to make relevant suggestions as time goes by, every single time a user clicks one of the recommendations or doesn't, we should very much be putting all of that data into a Kafka queue so we can put it back into our Hadoop cluster to do further processing on it. Anyways, let's go into a diagram and I will go ahead and elaborate on all of this.
- ![[Screenshot 2024-11-27 at 3.04.44 AM.png]]
	- Diagram
		- Alrighty, so let's take a look at our diagram. So as per usual, we're going to be starting out with a client who wants some recommendations. They're going to hit a load balancer that can hit any of our horizontally scaled out recommendation service nodes. From the recommendation service, we're just going to pull a little bit of information about the user because obviously we want to be kind of starting our search based on some of the users previous history, so in this case, the user DB might hold like say the five most recently watched videos. From there, we can then go ahead and pass those videos to the embedding model and neighboring index where those videos are all going to be converted into different embeddings and then furthermore, we're going to calculate all the possible candidates from there. Once we have these candidates calculated, we can distribute the work of actually going ahead and calculating the scores of each of the candidates on the ranking servers so we could use a load balancer to go ahead and distribute those out. Once we have all the data filled up in the ranking servers, we can merge them together back on the recommendation service which will also act as an aggregator and pass them back to the client. Then eventually when the client does whatever they do, we can go ahead and send those user logs through a queue back into our Hadoop cluster for further processing where we can basically use spark jobs to gradually update the models that are being held on any of our servers. So yeah, not an overly complicated design but there are a lot of considerations to think about when making a system like this and obviously a lot of them are going to be machine learning specific, but I think that at least the details that I provided for this video weren't you know overly necessary to just being a machine learning engineer and more so were just generally useful. 
		- So anyways guys, I hope this video helped. I'm going to do my best to keep coming up with like interesting and unique Systems design videos to talk about. Um but when I run out, just don't be surprised if I show you the resume that got me into Google. You know keep subscribing or else this channel is going to get pretty cringe is all I'll say. But uh yeah have a good one guys. I enjoy making these videos a lot and I hope you guys enjoy watching them. Have a great day.