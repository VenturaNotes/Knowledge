---
Source:
  - https://www.youtube.com/watch?v=b4awhnG4R3M
---
- ![[Screenshot 2024-11-27 at 8.48.53 AM.png]]
	- Introduction
		- Anyways, today let's talk about [[Shazam]]. This is one of those services or problems that if an interviewer asks you this, it's surprising. It's genuinely an interesting distributed systems problem and i feel like there's still things we can talk about in addition to the core algorithm that are actually exemplary of stuff that you could be quizzed on and so yeah, I think it's still useful overall but um you know if you're just here to learn a little bit, that's what i did as well and it's pretty interesting so let's get into it.
	- Shazam (Functional Requirements)
		- Description
			- Return most likely song being played based on audio input from the cellphone
		- Okay, so the functional requirements of our service are pretty simple. Basically all we're going to be doing is building some sort of music recognition service which means that we're going to allow a user to take in an audio clip into something like their phone microphone and then you know they'll send that to our servers and afterwards we're going to return them a suggestion of what we think the song that they just heard is.
	- Shazam (Capacity Estimates)
		- Description
			- 100 million songs in our database
			- 3k fingerprints per song, 64 bits each
			- 2.4 TB total index size
		- As far as capacity estimates go, this is going to be a little bit backwards. The reasoning for that being that i'm kind of going to mention a key term here that we'll cover in a bit but it'll make sense a little bit more later. Basically i'm going to assume that there are 100 million songs that we're you know having under consideration and then basically per song, let's imagine that there are 3,000 indexable keys and i'll touch upon this later what i really mean is something known as fingerprints but again we'll speak about that later and if each of those indexable keys is 64 bits, then we can basically say that we're going to have to create an index that is 2.4 terabytes. So obviously that's going to be pretty big, we can't fit it all on one machine or i guess we could with a hard drive but if we plan on doing any sort of processing in memory, we're going to have to be doing sharding and in addition, 2.4 terabytes is a lot anyway, you would probably want to do some sharding regardless because that obviously has an ability to grow.
	- Shazam (API Design)
		- Description
			- GetSuggestedSong(songRecording)
		- The api for this service is pretty quick. It's just gonna be that we can go ahead and find a song and the one parameter that would take in is the recording of a song so let's go ahead and get on into the database design. 
	- Shazam (Database Schema)
		- Description
			- SongSearchIndex(songDataChunk, songId)
			- SongDB(songID, songDataChunks)
		- Well, if you think about this problem as kind of a search problem for databases, then there's really just going to be two things that you would need. The first one is going to be some sort of index that allows us to take in our client-side clip of audio and spits out some potential ideas of audio that you know we could be playing and then the second thing is probably going to be some overarching database of all the audio files or something pertaining to the audio that we can go ahead and search afterwards and return back to our user.
- ![[Screenshot 2024-11-27 at 9.03.07 AM.png]]
	- Shazam (Architectural Overview)
		- Okay so let's talk about an algorithm for music recognition service like Shazam. Like i said earlier, it's you know completely absurd for any sort of interviewer to ever expect you to know something like this because i mean it's quite literally a multi-100 million dollar idea that they would be expecting you to just come up with on the spot but i think it would be somewhat reasonable if they kind of gave you a bunch of hints about what was going on and then said well now that we have this algorithm, how would we actually kind of support this in distributed environment. So anyways let's go ahead and get into it. We know that our entire goal is basically to take a sample of audio data and convert it to a format where we can just go ahead and match that with you know existing audio data that we have in our databases and of course this needs to be done with the service that is both highly available and also has very low latency. That's going to be a requirement in any sort of systems design interview. So anyways, before we can really start getting into the algorithm, we have to talk about just how audio files work in general. 
		- (1) Audio can be represented by something known as a spectrogram which is a 3d graph and i'll put one of these up on screen so you guys can follow along. A spectrogram basically has three parameters. It has the time that you know you heard a specific part of the audio, it has the frequency of the wavelength of the audio and then additionally it has the amplitude which kind of represents something like the volume (amplitude shown by the color of the lines here). 
		- (2) So Shazam does kind of a clever optimization and this is something that they've actually written in their paper. I'm just not making this up on the spot, but basically they take all of the peaks at certain amplitudes within the graph in this 3d graph and they reduce this 3d graph down to a 2d graph and create something they call a constellation map where basically now we have a 2d scatter plot of all these points that were at really high amplitude and basically now they're just you know little scatters on um basically a plot of both the time and frequency. So this allows us to simplify our search space pretty considerably and it also allows us to account for variations between things like the actual audio clip that we might have in our database and also the audio clip that a user might play for us. 
		- Obviously there's going to be a lot of variation that can happen between the two of those, between things like you know device compression, external noise, and just a bunch of other factors and as a result of that, kind of reducing this complicated 3d graph down to this more simple 2d one allows us to have a better chance to be able to match these songs together. So now that we have these 2d graphs, what can we actually do in order to kind of start dealing with creating a song index and making it such that we can search songs quickly. Well this is where something known as a combinatorial hash comes in and Shazam goes ahead and does something like the following: one option would be that every single point on that constellation graph, we could go through all of those frequencies at a given time and we could say, well what songs have all of these you know frequency points at a given time but there's a couple issues with that. The first is that well frequencies are basically representing like the note of the song you might hear. So if you're just looking at one frequency and trying to you know limit down your fields of songs based on which songs have that frequency, most songs are probably going to have it. It's like saying you know what songs don't have like the note c when you're looking at all of music. They probably all do, So what Shazam does instead to limit down the space is they actually look at pairs of notes and this has a bunch of really useful properties. 
		- (3) So basically if you take two frequencies so two of those points on the graph and you take the first one which comes before the second one in time and you say the first one is what we'll call the anchor point, basically create some sort of tuple where we have you know the anchor frequency, the second frequency and then the last thing is going to be the amount of time and offset between them. Now this is really useful because you know one issue with having to create an index like this is we don't know where the user's clip is going to be played in terms of at what point in the song it starts, right? You know my song could be three minutes long but I might be playing an audio clip from my phone where it starts a minute in, so we have to be able to align all of these clips. 
		- (4) So actually using pairs of these frequencies with the time delta between them is super useful because if we can find basically you know for our user audio (To limit the number of possible pairs of points in the graph (or else there would be $n^2$ combinations), Shazam says for each "anchor point", to only create pairs with points) if we split it into a bunch of different you know pairs of points where we have the anchor point, then the second point, then the time between them, let's say we have like i estimated earlier 3,000 indexable keys per song where the key is going to be one of those tuples, then all we have to do is basically figure out the song in the database where we have the most alignments between the user clip and that song and we can get you know genuinely a lot of alignment because like i said, we kind of reduce this complicated 3d graph with a lot of potential for noise into a more simple 2d graph which you know gets rid of a lot of the variation that can occur from having to use client-side audio through a microphone. 
		- So basically now here's what's going to happen. For our let's say 30 second user clip of audio, we're going to create a bunch of all of these you know anchor points, secondary point, you know time delta tuples between them, you know. It could be up to something like a thousand and in theory, we can really get a lot of these hashes and the hash is basically going to be comprised of that tuple. Let's say, you know, we want a 32-bit hash, we can basically use something like 10 bits for the first frequency, 10 bits for the second frequency, and then 12 bits for the time delta between them, and so now for our given user clip, we have all of these different hashes that we can look up in the actual index and we're going to do that. So if we have an inverted index where basically the inverted index also is taking all of these hashes or these fingerprints of all of the existing songs that we know and says you know, for this hash right here, here are all the song ids that correspond to it or have that exact hash, then we can look up all of the hashes in our user clip, find all of the potential songs that we have to you know look at to compare them and then say, okay now we have all these potential songs where one hash is matching, what is the best match we can get when we look at all of the hashes in our user clip? So basically it's a two-step process. We use all of the hashes individually in this kind of index search where we get a bunch of possible song IDs and then we limit down all of the potential song IDs to just one by comparing all of our hashes in our user clip of audio and seeing how well they line up with some basically sequence of hashes within all the candidate songs. And so that's basically the algorithm right there. It's not overly complicated, but the reason that i kind of like this problem is because now once we have this idea of okay first we're hitting this you know big inverted index and then after that we have to pull a bunch of things from a database and do a bunch of processing in order to kind of you know figure out which song matches best, there's a lot of CPU work to do there and it's not easy to speed up. So how can we actually go ahead and do that? Well for starters, as i mentioned earlier in our capacity estimates section, our index is probably going to be in the order of magnitude of terabytes. In fact i estimated two terabytes and so if we really wanted to basically have super fast you know matching in an ideal world, we'd be able to use something like memory with a hash map where the hashmap is the key which is going to be that tuple that 32-bit thing i mentioned earlier (Inevitably we'll have to parallelize our search over many different index nodes since we are searching for many fingerprints) and then the value is going to be a list of song ids that are held in the database. But in commodity servers these days, it's rare that you can have more than 256 gigabytes of ram and as a result of that, we're going to have to be able to shard our index in order to really get everything in memory. We could put things on disk but then we're limiting ourselves to pretty slow performance because we're basically going to have to perform a binary search for all of the you know corresponding hashes. We've spoken about this in the past. You're really basically limited to logarithmic time complexity when using a disk based solution whereas with memory, not only are you getting faster accesses just because it's in memory, but you also have a hash table and that allows you to get constant time accesses. So that's probably the ideal way of storing this index. That being said, we have to come up with a good way of actually sharding out our index. One nice property of the actual hashes that have been created is that 32-bit key is going to start with you know some encoding of the anchor point (All fingerprints with the same fingerprint frequency (will be many per song) can be stored on the same index node with consistent hashing) and so since we're going to have a bunch of hashes with the same anchor point, it's going to allow us to send basically a bunch of requests for you know one anchor point but then a bunch of different secondary points all to the same search index node which is really useful because consistent hashing will basically make sure that all of those keys with the same anchor point are probably going to be on the same node, and then in addition to that, we're probably going to have to parallelize the rest of our requests because they may have a different anchor point in that tuple and i'll make sure to kind of draw this out so it makes sense for you know how we're actually looking to get all of those hashes. So either way, one way or another, we're probably going to have to be making a bunch of different calls to different shards of that index and in an ideal world because they're parallelized, it'll all be decently fast and returned back to our server quickly enough. But even once we do that, even if we're able to achieve low latency just in that index alone, we still now have to do this entire secondary phase of processing where we check out all the possible songs and calculate which one is the actual best match. So now again we could basically come up with another problem where you know that has to be fast but basically the only way i can think about how you would do it is (A) either you keep all of those songs in memory which would be really useful because in theory keeping all of the fingerprints for every single song should be the same exact size as the inverted index, it's just stored in different formats. You could shard it as well and still use memory again or perhaps and this might be a little bit more practical is for the popular songs you know at a given time, a lot of people are going to be wanting to figure out what one particular song is, you can actually just go ahead and cache the sequence of fingerprints for that song and as a result, it should greatly speed up a lot of those operations and requests to the database but either way, no matter what happens, because you're going to have multiple terabytes of data to store in terms of songs, you're going to have to parallelize these computations and in fact you should because all of those songs are separate entities and as a result, you can perform the computation of similarity between the user audio clip and a given song audio clip separately. So it should be totally fine if you know you're making a bunch of parallel different requests, two different shards of the database and then on some sort of aggregation server, you would go ahead and basically say okay, which one of these songs matched the closest to our user uploaded clip. Then the final piece of the puzzle is probably just going to be that every once in a while, there are going to be new songs that are created and uploaded and recorded and as a result of that, we're going to need some reoccurring batch job that's going to take in those new songs, get the fingerprints for them according to the algorithm that i mentioned earlier, place them in our index and also place them in our database itself. 
- ![[Screenshot 2024-11-28 at 2.51.34 AM.png]]
	- Diagram
		- Okay, so as always let's formalize all of that with a diagram. So basically imagine we have a client who wants to figure out what it is that they're currently listening to over the radio and the first thing they're going to do after recording that clip on their phone is hit a load balancer and upload that clip to the recognition service. The recognition service is then basically going to go ahead and say, okay, I know now that i basically have all of these fingerprints for the user uploaded clip, let's look them all up at the same time in parallel in our fingerprint index which in an ideal world, we can keep in you know a memory based database solution such as Redis. Once it basically gets all those lists of potential song IDs, the next thing it has to do is reach out to the matching service with that aggregated list and then the matching service can basically say okay, I have all these song ids which i need to get the fingerprints of. If they're in the fingerprint cache, then great, that's going to speed up some of the requests. If not, I have to fetch them from the database. You'll see that i used a MongoDB database for the songs here and there are a couple reasons for that. The first thing is that (A) it still uses a b-tree even though it's NoSQL which is nice because b-trees in theory at least should be faster for reads than an LSM-tree. Additionally, another nice thing about the MongoDB is that because it is a document store, you should have really nice data locality which should allow you to fetch basically the huge document that is going to be you know one song ID and all of the corresponding fingerprints for that so those documents can get pretty big but you know as a result of that data locality, hopefully the reads will be a little bit faster and then the final component of the puzzle is basically just having some sort of Hadoop cluster which as new songs are created and published, we'll go ahead and use a daily spark job to go ahead and update both the index and the database to account for all of the new songs.
		- It's an interesting problem. I wouldn't expect that anyone would ever ask you this but i don't think it's unfair if someone gave you like i don't know a 10 minute primer on how those keys are created for a specific song and additionally you know kind of the process of how you would figure out once you have the fingerprints of the user clip and also the fingerprints of you know the potential candidate songs, um and then basically say how would you distribute this, that's decently fair but I just can't see it happening. I've just heard of this problem as a systems design problem and you know i like to cover it.