---
Source:
  - https://www.youtube.com/watch?v=B9DBB7-Z9gg
---
- ![[Screenshot 2024-11-17 at 3.43.50 PM.png]]
	- Introduction
		- Hello everybody. I am back for episode number three and today we are going to be doing [[Dropbox]] or i guess [[google drive]]. These services are great for someone like me who has hours of video footage that they would like to be accessing amongst computers and much of it is stuff that i don't want to keep on my hard drive because some of it is verging on illegal. So with this in mind, let's go ahead and start getting into the video and we can go ahead and cover this. This problem was inspired by one that i saw in "Grokking the systems design interview" and frankly i don't really love the detail with which they went into things like conflicts, so i'm going to put an extensive amount of focus on that topic in particular because i want to cover that because obviously if you guys really just wanted to read about this problem, you could probably find it in 10 other places on the internet. Also I'm 21 and probably know nothing that anyone else doesn't. A further thing is that the actual way the dropbox made their services scalable and faster was by building their own block storage. We are not doing that. We're doing a much higher level design, so keep that in mind. Feel free to read their blog posts which i'm happy to link in this video for how they went ahead and started to go ahead and do that. They have a great tech blog, um but for the sake of this video, uh basically more or less we are going to be doing something a lot more zoomed out and try and take a general look at how a product like dropbox might work on an end to end scale.
	- Dropbox and Google Drive (Functional Requirements)
		- Description
			- Upload and Download Files to Cloud
			- Share Files with Other Users
			- Realtime Push to Other Users
			- File Consistency
			- Keep Track of Older File Versions
			- Offline Editing
		- So as far as functional requirements go, let's just list more or less the following. There aren't going to be too many extended ones in this video because dropbox is a pretty cut and dry service for the most part as long as you're not including things like their real-time editing and text collaboration which we can talk about in another video. So as far as the functional requirements go, obviously you have to be able to upload and download files to and from your computer to the cloud. Additionally, you need to be able to share those files with a bunch of other users. If i have a document or a video or something and i want to give other people the permission to look at it and edit it, I should be able to do so. Additionally, all of the changes to files need to be propagated on other devices and that's probably going to be via a push style of change. It's not where they have to manually load it, but rather it's automatically going to be loaded onto their device assuming they have an internet connection and then additionally, no file corruption. So it shouldn't be possible that, you know, our files are getting jumbled up or additionally you know like we have a bunch of chunks of one file that get interleaved with a bunch of chunks from a different version of the file and then the final two things are that we want to be able to save pretty much all the versions of the file, so we can reconstruct it to look at kind of the edit history of that file, and then finally, users should be able to edit their files offline meaning that if I don't have an internet connection, I should be able to go ahead and change my file and then plop it back into the cloud eventually when I do get my internet connection again and obviously this is going to leave some room to have write conflicts and we'll talk about how we might deal with those in a little bit.
	- Dropbox and Google Drive (capacity estimates)
		- Description
			- Assumptions
				- Files up to 1gb in size
				- Equal amounts of reads and writes
				- Split files into 4mb chunks for fast upload and differencing
				- 500 million total users, 100 million per day
				- 200 files per user, 100kb on average per file
				- 100k $*$ 500 million users $*$ 200 files = 10Pb
		- For these numbers, I'm basically just going to go ahead and steal them from grokking again because, i don't know, they're pretty much arbitrary, and then we're obviously going to put more focus into the design section. So thinking about these capacity estimates, for starters, we're only going to consider file sizes up to a gigabyte. Obviously on dropbox, you can store hundreds, maybe even thousands or millions of gigabytes of storage because it's supposed to be limitless, but um, you know, by considering our scope to only files of a gigabyte, you'll see how that kind of affects the way that our logic works and especially breaking our files into smaller chunks. How we don't need any crazy data structures in order to keep track of all those chunks but rather we can kind of just do linear scans. So we'll talk about that a bit later. Additionally because this is such a write based system, we're obviously going to be propagating all the files that we have locally on our computer to the cloud all the time, we have a read write ratio of one to one. It seems decently reasonable, every single time you're going to read a file, you're probably going to edit it at some point. Additionally, we can probably go ahead and split our files into chunks of four megabytes in order to go ahead and store those in our block store. This is a pretty arbitrary number, but the scale of megabytes is definitely something that gets used in other distributed file systems like Hadoop, so, even though maybe they use 32 or 64 megabytes, sure whatever, four is fine. Um, assuming 500 million total users and 100 million daily active users, if each user has 200 files, they're going to be 100 billion files and if we assume the average size of a file is going to be 100 kilobytes, we can go ahead and do the math there and say 100 kilobytes times 100 billion files is going to be 10 petabytes of storage. So that's obviously a ton and this kind of speaks to the fact that we're probably going to have to use someone else's solution like amazon because they can basically give us infinite storage through the fact that they are themselves such a large company. Obviously, in reality, dropbox is storing exabytes of storage and like i mentioned, they ended up building their own storage solution even though they originally in the earlier stages of their company used amazon s3 and then furthermore, as far as actually pulling for changes um in real time, we can basically assume there are a million active connections per minute. So you know, if you want to basically use WebSockets or something like that, that's something to think about there.
	- Dropbox and Google Drive (API Design)
		- Description
			- API Endpoints
				- upload(userid, field, chunks)
				- download(field, localChunks)
					- returns chunk URLS not yet on local machine
				- addUser(userId, field, permissions)
		- All right, as we start to kind of get into the api endpoints of our service, I can basically think of three. So the first is going to be some sort of upload endpoint where we go ahead and incorporate a user id, a file name or a file id, a timestamp and also basically the differences between the older version of the file that we had locally and the newer version of the file to go ahead and actually upload them. This may actually in that diffs parameter contain the actual chunks of the file that are differentiated for upload and so as a result, we may be sending a ton of data here and it may require us to create a pretty complex solution in order to kind of deal with that. It may involve some queuing. As far as download goes, we have a user id, a file name or a file id, either or, and then also, I originally had merkle tree here and i don't necessarily think that it's necessary, but you know, if our files were potentially limitlessly large in terms of the size that they could, be then i think that using something like a merkle tree in order to track the differences between huge potentially stores of data because you know a file could be hundreds of gigabytes or even more, if it's a huge log or something like that, could be very useful for kind of transmitting the differences between two files, but for now, I actually think that um just kind of a list of hashes of chunks might be sufficient here and/or basically just for the download, the chunks themselves and we can go ahead and fetch kind of what it is that we need to download in order to um download the minimum amount over the network possible, but i'll touch upon that more later because i'm starting to give things away a little bit. And then finally, pretty basic, endpoint is just the `addUser` endpoint where for a given file, we go ahead and add a user id with a certain set of permissions. 
	- Dropbox and Google Drive (Database Schema)
		- Description
			- Tables
				- Users(userId, email, passwordHash)
				- Files (fileID, path)
				- FileUsers(userId, fileID)
				- FileVersionChunks(fileID, version, chunkHash)
				- Chunks (chunkHash, s3URL, field)
		- All right, let's start thinking about our databases here. So there are two main considerations to think of. We know that our databases main kind of purpose here is to go ahead and store the metadata of each file, right? I've mentioned that since we can have these huge one gigabyte files, it probably makes the most sense to go ahead and split them into chunks. The reasoning for this being that splitting files into chunks means that if you make a small change to a file on a local machine, you don't have to re-upload the entire file but basically only the modified chunks (also more parallelism when uploading to S3), and you can do this by using hashes of chunks to differentiate you know if a chunk is already in the system or whether it's not. So basically what this insinuates is we're going to need some sort of database that obviously has a users table, obviously has some table with files and their ID and their name, but also something that is going to associate for a given version of the file which chunks are listed with that version, so even though normally i would kind of save this discussion for a little bit later, I think by virtue of this problem and the fact that we kind of need that acid compliance, i want to start talking about what type of database we should be using now and also additionally what type of replication and partitioning. So basically since we want to be storing this file metadata together, it's very important that we have transactions. This is because, imagine i were to go ahead and make a change to certain chunks and basically in the middle of that, someone else were to make a change to different chunks, and you know we were both changing three chunks at once, if it happened to be the case that my changes were interleaved with theirs, what would happen is that the next time someone tried to subsequently download the file, basically all of the chunks that they download wouldn't make sense together. There would be this corrupted data and so as a result, it's very important that as far as storing our metadata goes, we have transactions in play so that we can make sure that if I on a computer make a bunch of changes to multiple different chunks, they either all go through together or they all fail together and additionally that they never get interleaved with another transaction of multiple different changes to chunks. So that's very important to note. As a result, obviously this means that we're going to be using a relational SQL database. Um, I think there are probably parts of this that can be put in a NoSQL database like Cassandra but at the end of the day, it's very important to not have any write conflicts as well because you don't want any anyone's changes from their computer to get clobbered and so as a result of that, Cassandra, which uses a leaderless architecture for its replication strategy, also personally to me it doesn't really seem to make sense here because it uses last write wins, if i were to make a write and then another person were to make a similar write at the same time on their computer and we both were writing to different nodes in kind of the database cluster, eventually what might happen when the chips fall and the anti-entropy process is performed is that one of our changes or our set of changes just gets completely thrown out and that would be very very bad for a file system like this. So i think that a single leader replication SQL configuration is probably the best one to use here. And so let's start talking about tables and then I'll quickly talk about how I'll partition them because obviously we have a ton of data. Like i mentioned, 10 petabytes of probably file storage but obviously that's all going to come with a ton of metadata that's going to have to be sharded. 
		- So we have our users table with a user id which is an int, an email, and a password hash. We have probably a files table which has a file ID and a string for the name, and then additionally, a file users table which is basically just a one-to-many relationship between a file and all of its users that have permissions on it. A file version chunks table, so this is kind of the bread and butter here where basically we have a mapping from a given file id and a version of that file to all of the chunk hashes that it's going to match up to. So it's a file id version and a chunk hash and then by using indexes on first the file id and then the version, you can basically go ahead and quickly find all of the chunk hashes that you need to get in order to go ahead and download for that version of the file. Additionally, finally, we're going to need a chunks table which is basically going to have that hash of that chunk as the primary key and additionally an s3 url so we can go ahead and fetch that chunk from block storage. For this chunks table also, i'd like to include a third field which is the file id field which should just be an integer and the reason for having this is so we can actually partition our chunks metadata table on that file id field and we can just take a hash of the file ID field, and then use consistent hashing or something like that, but the reason for doing this is that we want more data locality. If we're going to be fetching one file, we want it to be the case that all the metadata for the chunks in that file are going to be on the same machine so we don't have to run a bunch of cross partition joins. This is going to be a pretty useful way of kind of partitioning our metadata which is basically everything that belongs to the same file, should probably be on the same shard. 
- ![[Screenshot 2024-11-17 at 4.07.44 PM.png]]
	- Dropbox and Google Drive (Architecture)
		- Alrighty, so i've already started to touch on how we're going to replicate and how we're going to partition our data and keep in mind that kind of the central idea is all replication is going to be single leader because we want to maintain consistency. All partitioning is basically going to be partitioned by files so we get a bunch of data locality, we don't basically have to do cross partition joins and, you know, if we want for a given file to find all the chunks of it, we can just look at one partition. So keep that in mind because that's going to kind of be a central theme here. So before again as i normally do, we go into my actual design decisions, I want to kind of preface this by saying that it seems that the way that people on youtube have done this problem before me and the way that grokking the systems design has done it which i imagine has probably strongly influenced their design choices, it seems like there are a lot of unnecessary components to me at times and i'll touch upon those and you guys can feel free to kind of disagree with me and you know feel free to let me know in the comments but i think that um this can actually be simplified a little bit more than it needs to be and the "grokking the systems design interview" isn't great. But that being said, i think it does have some really good components, so let's start talking about kind of the general algorithm about how we might go about doing these things. So as i mentioned before, all files should more or less be split into really small chunks. This is really important so that if we can only determine the kind of differences between a file and its subsequent revisions in terms of you know say a few chunks instead of all 250 chunks, the reason i say 250 is because a chunk is four megabytes, a file size can be up to one gigabyte we assumed so that means up to 250 potential chunks. So say instead of 250 chunks to upload, we only have to upload three. That's going to seriously save not only the amount of new data that we have to store, but the amount of network bandwidth So now the whole point is, we want to be basically reducing the load to a minimum on all of our servers and having our clients which are effectively just a local cache of the files or documents in our dropbox or google drive doing the maximum amount of logic possible. So with this in mind, what i think should be done is more or less the following. The client can basically look at its old revision of a document and then look at the new changes that have been made and then basically say to itself, all right here are the basically hashes of all the chunks that we last downloaded from the database. Here are the new hashes of all the chunks, we can see that we only have to say, upload these few chunks. Once it determines that, the problem still isn't overly simple. Like i mentioned, one area that "grokking the systems design" doesn't really put much thought into in my opinion is conflict resolution. So, how can we actually determine if a client is updating the most up-to-date copy of that file? Well, the only way to really do so is to actually reach out to our metadata server and ensure that there's no newer versions of the files. So basically unless we're going to be using something like a distributed transaction and two-phase commit, there's no really good way for us to go ahead and say, okay, go and upload all these chunks to s3 and at the same time increment the version counter but don't increment the version counter if there's a newer version and if there is a newer version, then don't upload those files to s3. We could use a distributed transaction here, but that's going to add a ton of latency. So what i think simplifies the system is to just from the client, directly upload those differing chunks to s3 and then go ahead and reach out to the actual metadata server and basically say, if i have the newest version of the file, then go ahead and commit these changes. Of course, it's possible that the client doesn't have the newest version of the file because someone else made a change while they were offline, in which case, what's probably going to have to happen is the client is going to have to download the newest version of the file, merge those changes somehow, and then try this all again. You may say to yourself, oh well this is going to result in some redundant uploads to s3 and while that's technically true, I don't think this is a case that's going to be happening frequently enough that it's going to result in a ton of unused chunks in s3 but if it really got to that point and you know, there was just tons of unused data in s3 and we were really bloating our data storage and causing ourselves a ton of money, in theory, we could write some sort of separate cleanup or background process that basically occasionally pulls the database, tries to check whether a given hash is ever in use in a document and if not, purchase it from s3. But, you know, that's kind of a discussion for a different time. Basically my point is, I think you should first upload the files to s3, then basically reach out to your metadata database and say, okay, if we're good to go here because we were updating the newest version, let's go ahead and commit these changes (You can use a transaction here to avoid any read-modify-write race conditions). So that's covering that. Um, another thing that i think that kind of the "grokking the systems design interview" kind of gets wrong is that they basically go ahead and say, every single time a client wants to change a document, they should go ahead and make the request to change it and plop it into this global request queue, and frankly I can't really understand why this is happening because at least in my system, the only thing that you're really doing is making a single database update to your metadata table and it's not that like complex or you know heavy of an operation that it needs to go into a queue first. It doesn't have to be processed in the background or anything. This is a pretty simple operation, so i personally would just keep this as a traditional http post request. Furthermore, another thing that they do which i'm not really a fan of is, they say, that every single time that there is a change to a document, Obviously we want all the clients to be alerted of it or at least all the clients that have permissions for that document. What i don't like about the way they handle this is they say that, every single time a document is changed, each client should have a response queue corresponding to it and the server that changed the document is responsible for potentially sending messages to every single response queue where there's a response queue for every single client. Obviously, I don't like this because it means that, you know, say there's one particular document that's maybe you know company-wide so if it's document for amazon, there's going to be a hundred thousand people that have access to this document. You're telling me that if we were to update that document, now it has to be pushed to a hundred thousand people. It's kind of like that twitter issue where you know if a document has tons of followers, you don't want to be pushing it to all these different response queues. So i've kind of taken a different approach to that where i believe that instead of pushing every single change to response queues partitioned by user where every single user has their own response queue and we have to do all these pushes, I think what we should do instead is go ahead and basically partition our response queues by file id and the reasoning for this is that, we know that based on the assumptions of our problem, pretty much all the users aren't going to have any more than 200 files and so as a result of that, each user potentially only has to keep about you know 200-ish WebSockets open with those response queues or servers and events or whatever it may be. But they only have to persist 200 connections which is very doable whereas, you know, having our internal servers potentially have to push to hundreds of thousands or even maybe millions of queues seems completely infeasible to me. So yeah, that's the general design discussion that i want to have as far as conflict resolution goes, as you can see, I've kind of opted to basically go ahead and just store only one version of a given file and if there's a potential conflict where i make a write while i'm offline, I first have to download the newer version of the file and basically merge them together before i actually upload again. What you could in theory do is as opposed to just making everyone merge their conflicts the second you detect one, you could actually store both those values in the database as siblings and this is something that we've seen before, in you know, types of databases where you can actually have write conflicts and you know things like Riak for example will use something like a version vector in order to determine um basically whether two writes were concurrent and then they'll store them both. So that's a possibility, but i think it adds a ton of additional complexity to our system and it's not like we're dealing with git here, we're just kind of dealing with dropbox and as a result of that i think that it's kind of unnecessary to actually kind of store both versions of the document but rather i think we should just have one kind of linked list of versions that clearly show the history of the document and if a client is to ever make a conflicting write, you basically say, hey, your writes about to conflict, go ahead and redownload the newer version of the document, merge them in, and then you should be good to go, but we'll obviously make sure to check it again so we don't have any race conditions or anything like that. Okay, yeah, I think that more or less explains everything so with that being said let me go ahead and show my diagram off and then i'll explain it.
	- Diagram
		- So, as far as actually going ahead and making any modification to a file goes, I'm assuming that a client is basically holding the files that they have on hand, and in addition to holding those files they have on hand, they're holding this kind of limited internal version of a metadata table where they're keeping track of all the chunks and hashes of the chunks that had previously made up the version of the file that they've since edited. So anyways, basically what i'm assuming now is that when a client wants to go ahead and make a change, what it's going to do is first consult that kind of mini metadata table and say, okay, compared to this, what are the chunks of the way the file currently looks and what are the chunks now that i need to upload to s3. It then goes ahead and uploads these files to s3, which as you can see, I have attached to a CDN if certain files are very popular. A CDN can really speed up the access times of those chunks and so anyways, after uploading kind of those chunks to our file storage, what we're going to go ahead and do is hit our load balancer which we can you know have in a kind of redundant configuration to ensure it's not a single point of failure, and the load balancer can basically either send you to a user service if you need to make, you know, changes to user information or the permissions of a file in a user or more importantly, if we're actually updating a file to the metadata service, where the metadata service is basically going to say the following: it's going to take a look at the files table and the chunks table and say, if you're trying to update this file and you are updating what was previously known as the most up-to-date version, then you're good. Let's go ahead and make a change to the chunks table to basically say, we've constituted a new version of the file. Otherwise, we're going to basically respond back to the client saying hey, you actually need to go ahead and redownload the newer version of this document and merge in those conflicts because you don't actually know what the newest version was and as a result we don't want to basically let write conflicts occur. So those are the two possibilities there but assuming that the write was successful, you know, the chunks are uploaded into the metadata table which keep in mind should certainly be a SQL transaction, that's the reason all these tables are in SQL is that not only does changing all these chunks for a version of the file have to be atomic, it's very important that it is serializable in the sense that we can't have two people trying to interleave the chunks for a given version of a file that would corrupt the file and be very bad. So once those changes are in the chunks table, we're going to basically have a metadata cache to speed up reads for very commonly accessed files and then additionally to that, keep in mind, that the chunks tables like i mentioned earlier are going to be partitioned on file id, so what that means is that when we pull this change data from every single chunk table, what it's effectively going to be telling us are all the changes being made to a given set of files, and we can, you know, because we're partitioning on file id or rather probably a hash of file id, what we can go ahead and do from the beginning is have our client since it's only probably has around 200 files in it, go ahead and subscribe to all of those message queues that are corresponding to the files that the client cares about. And so by subscribing to basically probably up to 200 message queues, we can either use something like long polling or server side events or web hooks it doesn't really matter here, to go ahead and pull from these log based message queues, and that way we can ensure that a client connected to the internet is constantly receiving new updates for every single file in the system. So as you can see here, this is kind of one of the big differences that i have between my design and the design of others is that they don't really specify the exact type of response queue that they're going to use but at least as far as "grokking the systems design interview" goes, that response queue is partitioned by user id which i think is super super dumb because for popular documents, you're going to have to potentially be sending that document to literally millions of queues at once which is a massive bottleneck. So instead, i think it's better to just keep the change data from every single one of these partitions of the chunks tables in its own separate queue and the reason i would have them as a log based message queue is because so many clients can potentially be pulling from each queue that we want the messages to be persistent. If one client were to get disconnected for some reason, you want it to be able to kind of pick up where it left off and start reading from those log base message queues again. Obviously, these cues can be replicated as well to ensure fault tolerance but, yeah, overall i think this is a pretty feasible system. Like i've mentioned in the beginning of my video kind of the point here is that this is one way to actually kind of wire things together but when you really want super high performance at scale like this, you have to start coming up with custom solutions and i think the way that dropbox ultimately did that was kind of coming up with their own file store to replace s3 but that's a little beyond our scope. We can talk about something like making file stores in a different video but for now, this is the general high level design that i hope to achieve and i think it's actually pretty feasible. It's a little different than the grokking one in the sense that, you know, there's no request queue and the response queues are partitioned quite differently but ultimately, they share a lot of aspects in common such as the chunking of files for i guess like a quicker upload to our file storage 
	- Conclusion
		- Uh yeah, so anyways guys, i hope that you enjoyed this video. I tried to actually differentiate myself a little bit here from the typical um i don't know systems design interviews and i made sure to watch a few before making this one of dropbox and google drive so that i could try and you know cover what they didn't for the most part. It seems that most of those videos are generally just clones of "grokking the system's design interview" and so i purposely tend to try and go out of my way to differentiate myself and actually change my design up a little bit and you know where i can point out the flaws in those designs because i think a lot of them just don't go into enough depth and, you know, if you really try and think about kind of the race conditions and some of the issues that they bring up, there are actually quite a few issues with them, and so yeah, ultimately this is kind of what i came up with and i hope you guys like it. Have a good one