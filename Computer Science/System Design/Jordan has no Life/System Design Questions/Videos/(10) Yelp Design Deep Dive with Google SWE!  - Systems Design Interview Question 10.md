---
Source:
  - https://www.youtube.com/watch?v=dSuYtpk59OY
Reviewed: false
---
- ![[Screenshot 2024-11-23 at 7.55.53 PM.png]]
	- Introduction
		- Let's do some systems design. I'm going to be talking about yelp today and basically consider this a precursor to uber. If i weren't too cheap to ever eat out, then i would probably use yelp to go ahead and find recommendations for places to eat out near me or any other type of recommendation for local establishments. It's good for getting a sense of building something like a geospatial index and again, that's going to come in very handy when we talk about Uber. I have a conceptual video of this that's much older on my channel that i recommend you all watch before watching this video, but even still, I'm going to go over the majority of the content albeit probably just in a little bit less depth. With that being said, let's go ahead and get into this. Again, I hope you guys all had a good weekend and now it's time to get back to the grind. I certainly killed my macros through the drinking of a bunch of white claws and the eating of at least 30 Linden cookies, but nonetheless, we're back on it boys. Let's go.
	- Yelp (Functional Requirements)
		- Description
			- Functional Requirements
				- Add places, and review them
				- Query for nearby establishments
		- So as far as functional requirements should go, as i mentioned before, the whole point of a website like yelp is to basically see a bunch of establishments relatively close by your location and to be able to get a sense of the reviews of them or perhaps add a review if you're either super satisfied or super upset with them. So, the first functional requirement with all this being said should be that users should be able to add places or perhaps the owner should be able to add places but there should be places on yelp that are reviewable. I should be able to give them a certain number of stars and a description for my review and comment on them and others should be able to see that. Secondly and this is the use case we're really going to be optimizing for, is the user should be able to see nearby places within a given radius. So what that basically means is that if i have a certain zip code, if i have a certain latitude and longitude, I should be able to see, you know, just with one query, you know, 100 places at a time, all of which have reviews, that are all near me, and I can easily get to.
		- This is kind of the main use case that we're going to be optimizing for as it is generally the more complex query in this problem that we haven't really seen before. Namely, we're going to be trying to basically get all of these places in our database that have some sort of geographic location that is close to mine. Obviously we want to be doing this with really low latency, we need some fault tolerance for our service, and you know, the normal stuff. So we'll talk about that in a bit but for now, let's talk about some capacity estimates. 
	- Yelp (Capacity Estimates)
		- Description
			- Capacity Estimates
				- 500 million places, 100k queries/s
				- 4 byte establishmentID/geoHash
				- 500 million $*$ 8 bytes = 4gb
		- Generally speaking, yelp is a pretty read heavy application. Like I mentioned, we're going to be optimizing for the use case of, let me see all the places nearby me, let me check out reviews of them as opposed to something like writing a review or actually updating the location of a given place, and that's kind of the main, you know, separator between something like this problem and one like uber or some delivery app where you're also dealing with a bunch of location data. However, there are going to be a lot of updates to that data and as a result, it requires a lot of extra considerations to deal with it which is why you can kind of consider a problem like this as a primer to that uber problem, but like i said, read heavy application. So let's assume they're going to be 500 million places with 100,000 queries per second, huge read load as we know, and now let's imagine we can represent some location using some sort of ID like that's four bytes, and we're going to plot that in our database and then, you know, if we have four bytes for that location ID and then maybe four bytes for like basically um a location ID that represents kind of the geographic data of that place, then basically we're imagining it's going to take eight bytes to go ahead and store some sort of place in this geographic index that we're going to be building and then ultimately 500 million times 8 bytes is going to be 4 gigabytes. So we could put all of this information on one machine, but we're going to still talk about sharding to potentially spread out the load of these requests and go ahead and speed things up because obviously we want really low latency.
		- Additionally, even though I basically insinuated that we're only going to have like the database ID in our location index, what we could further do is actually just duplicate that entire database row in our index and then we won't require some sort of join or second query call to our database every single time we're querying a bunch of location data. So then that would obviously increase the amount of data that we have to store in our geographic index but it could speed things up and especially if we want to be able to do searches by the actual name of an establishment, not just its location, then we're definitely going to want to be able to put our name in something like our geographic index, so we can kind of perform these dual queries both on name and location. Anyways, let's talk about the API design 
	- Yelp (API Design)
		- Description
			- API Schema
				- add_review(location_id, user_id, stars, text)
				- add_location(lat, `lng`, desc, category)
				- search(lat, `lng`, radius, queryTerms)
		- So for our api, I can basically think of three endpoints where the last is going to be the most important. The first is going to be to add a review to our service. That's easy enough. We basically have this, you know, location ID where a location is something like a restaurant or a massage parlor, anything that might be on yelp that is reviewable. We also have a user id because the user is going to be adding the review, we're going to have to join on that user information in order to basically say what user was actually reviewing the place. A number of stars for the review and then also the actual text content of it, so simple enough. Kind of trivial, we're not going to be focusing on it that much because that's just, you know, normal like rest endpoint type deal. Additionally, we have adding a location, so this is one of the things where we're going to be writing to our database with um a certain establishment that has a name, a latitude, a longitude, a description, and something like a category in order to help filter things down a little bit more on our searches because, like i said, this is kind of more of a search based website. Everything the user does is going to be querying for certain locations that fit a bunch of parameters, and so, you know, if the concept of a search index starts popping into your mind, we're probably going to need one of those, but even still we should kind of break down the algorithm that's going to be used here in order to do the geographic part of the query. Then additionally, we're going to want to be able to show locations. This is the main endpoint we're going to be focusing on and we can go ahead and parameterize this by something like a user latitude, a user longitude, a search radius and a search term and then you could also, you know, do some pagination related stuff here to limit the amount of results you get and then maybe have like a page two or something, but whatever, that's kind of semantics and you guys kind of understand that by this point. So let's move on to our database schema. 
	- Yelp (Database Schema)
		- Description
			- Database Tables
				- User(user_id, email, `pw_hash`)
				- Location(id, lat, `lng`, name, desc, category)
				- Review(user_id, loc_id, stars, text)
		- In terms of the database tables that we're probably going to need at one point or another, um we're obviously going to have this concept of a user which is going to have like a user id and an email. Maybe we would also have something like a type, like owner, or just, you know, typical customer, and then additionally, from this concept of place or location where we have a place ID, a latitude, a longitude, a name, a description, a category, and then finally we have a review which can have indexes on two columns: one for place id, and maybe one for user id if you know, you want to do like a profile view where you search all the reviews for a given user, but that's just going to have the number of stars and the text of the review. So simple enough, even though we're going to have these as our tables, it doesn't mean that we're not going to have some derived data sets from all of this that are going to kind of support faster querying of our application data, and we'll talk about that in a little bit 
- ![[Screenshot 2024-11-23 at 8.04.35 PM.png|500]]
	- Yelp (Architectural Overview)
		- All right, let's talk about how we can now go ahead and kind of create an optimal solution here for this yelp problem to go ahead and find a ton of points near a given user. So i'm going to kind of preface this all by saying, i'm a visual learner and i'm sure many of you guys are too, and i think it's going to help a lot if i kind of use the whiteboard here to pair with this problem and demonstrate to you guys how we're best going to solve it.
		- (1) So to basically preface everything, I am basically going to say, I am point `j` on this 2d plane that i have pictured on the whiteboard and i have a bunch of points around me, and what i want is to find all the points within a given radius, so that's going to be some sort of circular area. 
		- (2) So as you can see, I have my circle basically on that 2d plane. Now, what if we were to basically have it such that all of the points on our 2d plane were just in a traditional database, right? So we're using some sort of index that is comprised of either an SSTable, an LSM-tree or a b-tree, and to remind you all what that means is we have basically very efficient range queries on fields that we have indexed and as of now, basically, all of these points has a latitude and a longitude where we can basically just imagine the latitude and the longitude are kind of like their x and y coordinates but, you know, obviously it's a little more complicated than that, but for now, we can kind of just abstract that away. So we have these kind of x and y coordinates of every single point and we have indexes on both which means we can perform these really efficient range queries over the x and y coordinates of all the points but not both at the same time. 
		- (3) So what are our options? Well basically, all we can really do here in this kind of naive solution case is we can find all of the points that are going to be a certain x distance away from our original user and all of the points that are going to be a certain y distance away from our original user. Then once we have those, we basically aggregate them together and filter all of those places out that aren't actually within that circle of a given radius. As you can see, that's obviously going to be a very naive and not great solution because it means that we now have this kind of quadratic complexity where we're considering basically a lot more points and as opposed to getting this really small area that's just surrounding our original user, we're opening ourselves up to potentially having to search the entire globe at a given latitude and given longitude and the globe is obviously huge. There are going to be a ton of points there and now we have to do all of this server-side filtering and it's going to take a really long time and be very inefficient. So what is actually going to be a better solution? 
		- (4) So in an ideal world, what we would have basically is some sort of bounding box, right? Where we take our user, we place him in the bounding box and then just by virtue of having a bunch of points mapped into this bounding box, we can return all of those points really quickly and that's kind of the philosophy behind the [[geographic index]] that we're going to be building. So instead of just having one bounding box, we're going to actually have a ton of bounding boxes all of various sizes and all of basically varying locations. And the goal of this more or less is to make it such that as opposed to having this big 2d search problem where we're searching across two possible coordinates, instead, we're basically just going to search for a given user's bounding box or perhaps the surrounding bounding boxes of it and then that way we can really quickly access all of the points that they contain. So, we're going to basically go ahead and split our 2d plane into a bunch of these bounding boxes, but we're going to do it into a clever way that makes it easy to search using our typical database index. So let's see how we can actually do that.
- ![[Screenshot 2024-11-23 at 8.30.52 PM.png]]
	- Yelp (Architectural Overview) Part 2
		- (1) So i'm now going to bring up this other image and it's going to look a little bit weird if you haven't kind of watched my primer video on geographic indexes that i made a while back, but basically the premise is this. As you can see, we have this entire 2d coordinated space split into a bunch of boxes and each of those boxes can have sub boxes and each of those sub boxes can have sub boxes and the premise is this more or less, each of those boxes is going to have an ID. Every single one of its child sub boxes is going to have a similar ID, however it's going to have some sort of suffix. So for example, you can see the root box that i have there has a hash of `j12z1` and one of its children, say the middle one in this case is going to have the hash of `j12z1a` and that child, one of its children, is going to have the hash of `j12z1a5`. So as you can see we kind of keep appending to this original string. The reason for that is as follows, if we have a database and we keep all of these IDs sorted, it means that, say we have a big bounding box. We can really quickly find all of the points inside of it by performing a range query. So for example, if i wanted to see all of the points that i've drawn out in this part of the coordinate plane, I can go ahead and perform a range query from `j12z1` to `j12z2` and that way, I can get every single one of these points contained in that big bounding box. Additionally, it means that if we want to basically get one of the smaller bounding boxes, we know that all we have to do is append some sort of suffix to it and we can really quickly search through our database index knowing that everything is sorted and this gives us really great time complexity such that we no longer have to do this 2d search anymore and it completely lowers the search space if we're trying to look for a point. So what i have drawn out here below is known as basically a [[geohash]] and typically in a [[geohash]], what you'll see is something like each bounding box will have nine children but you may have also heard of something known as a [[quadtree]] where each bounding box has four children, and the way you would delineate a child is by taking basically the hash of the parent node, and then adding either a `0100` or a `11` or a `10` to it delineating one of the four directions that the child node could basically be in. And then what you can do is, you can take all of the sizes of the squares and normalize them such that for a given length of the hash, we actually know how big that bounding box is and that's going to give us a lot of really cool capabilities. So now basically, if you know we're looking at user `j` and we want to find all the places within say a three mile radius, what we can first do is find the actual geohash of user `j` from the `latlong` and that can either be something that is kind of cached in a database or it can be calculated on the fly which isn't too hard because basically this entire thing is effectively a tree and so as a result, if we're searching for the geohash for a given `latlong`, we can do that using logarithmic time complexity, and so, once i have the geohash for a given point, we can use the surrounding bounded boxes in order to find all of the points or locations that are nearby and that's going to be super useful and it's very fast to do. You may note that it's very possible that user `j` is at the edge of a bounding box, so we may have to consider multiple bounding boxes that are neighbors of it in order to make sure that we're really encompassing all of the nearby points and that's something that's relatively easy to do if you just keep using bigger and bigger bounding boxes and then do a little bit of just logic as far as picking the right neighbors. This is also something that can be parallelized, so it's relatively easy to do overall. 
		- Okay, so now that we've basically talked about our main algorithm about how we're going to find the surrounding neighboring points given the latitude and longitude of just one point, let's quickly touch upon some other things as the actual technology that we may want to be using to hold our geographic index, as well as some things like partitioning and replication because that's definitely going to be important for supporting very high read throughputs. So firstly, as we can see, um, you can use something like a traditional database and put a geographic index in it. I know that Postgres definitely has a feature in order to do this kind of geographic indexing, however, if you do recall, I mentioned that one of kind of the parameters of this problem was that not only are we going to be searching on geographic location, but we're also going to be searching on the actual name of the place, so with this in mind, perhaps what we're best off doing is actually using a search index to perform this functionality. The reason i say this is that something like Lucene which i mentioned in our previous video has a ton of functionality that can all be integrated in order to perform very complex searches. So not only can Lucene basically support a geographic index, but you can also store the actual name of the place within Lucene and by virtue of doing so, you can kind of cross-reference both the name and the geographic location in order to perform very good searches that will basically give us quick results. Obviously, it's going to be the case that since we have such read heavy workflows, we're going to want a lot of caching abilities in order to basically store the results of certain calls and like i mentioned, what elasticsearch will do, is if you're making these kind of compound queries where it's like hey, both filter on geolocation and filter on name, you may be able to even do it such that you can kind of just cache, you know, part of each query for later reuse. So perhaps you could just cache the geolocation part and then, you know, as people kind of type in different um actual search queries, you can use that cache geolocation query and then kind of combine it with the filter of the search term, but anyways, yeah, I mean that's kind of just my motivation for trying to use a search index here as opposed to just using something like Redis which i'm not sure has the functionality to combine something like a text search and a geolocation index. It may but i just haven't really looked into it. But additionally, we have to make sure that if we're having a search index, it's up to date. Our search index is not going to be our primary database for a variety of reasons, one of which being that the data just isn't as durable in there and so as a result, we want to make sure that any changes that we make to any of our yelp locations are going to eventually be reflected in the search index. It's not overly important that those are instantly reflected and we definitely can't have strong consistency without something like two phase commit, but we can either use something like a batch job to go ahead and update our search index say once a day with all of the changes from the database or perhaps even better, if we want to make those changes visible quicker, is to use some sort of stream processing. So every single time that our locations database gets updated and we can just use like MySQL as a database there for all of our yelp restaurants or anything like that, we can go ahead and stream that change data from our MySQL table to our search index and that's how we can actually keep things nice and up to date and that's a pattern that i've you know implemented many times in some of my designs on this channel and i think it's very feasible here. As far as partitioning goes, this definitely deserves a discussion because you always want to be very careful partitioning your geographic data in a way such that it makes a lot of sense for your end user and getting the lowest latencies possible. So one really intuitive way to partition things is, again, whenever we're sharding data, we want to keep in mind that we want basically all data that's going to be returned in the same query as close to one another as possible. So, if i have a bunch of restaurants in our search index, we want it to be the case that all of the restaurants that are close in proximity to one another are going to be on the same node. It is very possible however though that this is going to lead to hot spots, right? So, say I were to basically say, all of the points in this bounding box should be going on one given node in our say search index cluster. It's very possible that if it happens to be the case that that bounding box was downtown, Manhattan, they're going to be you know hundreds of thousands of points potentially in that relatively small bounding box and so as a result of that, we kind of want to be doing this sort of dynamic sharding where we want to make sure that there aren't any hot spots and if one node were to kind of be overwhelmed by too many places on it, what you would then instead do is basically use a smaller part of our quadtree or our geohash and use a smaller bounding box as the thing to kind of shard on. So in that sense, we're not using fixed size parts of the grid to shard but rather maybe shard based on, hey, as soon as this part of the grid has too many locations in it, we should split that into it's either four or nine child pieces, and then in turn shard those. Obviously it's going to slow down certain queries in the sense that we may have to do some aggregating over multiple nodes in our search cluster, but at the end of the day, we don't really have any choice. Additionally, like I said, we're going to be having a lot of caching and that can help kind of mitigate hot spots in the sense that, you know, if me and a million other people are trying to see our yelp results for downtown manhattan, the cache is going to be returning most of those and it can kind of hide all of that user load from our actual search index itself. As far as replication goes, we obviously do want to ensure fault tolerance, however, since we're not really dealing with the need to have super high write throughput here and nor are we dealing with the need for strong consistency, something like asynchronous single leader replication i think suits this problem well, both for our databases and our search index and you know cache and all of that which can just be LRU by the way because i haven't mentioned it yet. 
	- Yelp Design
		- Alrighty, I got myself a nice little minimal diagram here, so let's go ahead and walk ourselves through it. First of all, if we're the client, we're going to hit a load balancer every single time we have some sort of request to make, we can make that redundant through some sort of active-active or active-passive configuration there. Um, I'm assuming we're going to talk about the write path first, so let's imagine i'm adding a new place to our database or adding some sort of new review, we're going to first hit our write service which we can basically horizontally scale out hence the load balancer and then the first thing that's going to do is just hit our SQL table which contains all of the reviews and locations and, you know, we can obviously have separate tables for that. A quick note to think about is that as far as reviews go, we should probably shard them by location ID just so that whenever you do query a location, you can really quickly pull up all those reviews instead of having to aggregate them in parallel. Then furthermore, as you can see we have some single leader replication going on there, but in addition, all the changes that are going to happen are going to be streamed to our log-based message broker, aka Kafka. The reason we're using a log base message broker here is for durability of our actual writes so that nothing is lost if that message broker were to go down and then those are then in turn going to be propagated to our ElasticSearch cluster. I should mention that even though i'm using Kafka here which insinuates that we're doing some sort of stream processing, all of these updates could be written in batch, say once per day at a time, where you know there's relatively low load on the service and the reason that might help is it might allow us to perform some more aggregations of the data, especially in terms of you know the overall number of stars that a given location has in terms of reviews or something like that and that in turn may help our elastic search cluster return more popular locations as opposed to ones that are less popular. So anyways, like i said, the elasticsearch cluster is going to be updated either in batch or using streaming. I think probably more practically someone would do it in batch but um you know if you want the changes reflected more quickly, then stream processing is the way to go. So i am using Kafka here but it does put a little bit more load on our cluster. So anyways, the elastic search cluster then anytime the user is going to make a search for nearby locations is going to basically go ahead and search through its shard for a given location's grid and especially if that query needs to be expanded because either there aren't enough places or the location that we're searching for is at the edge of the grid we're checking, then you can basically parallelize this query by checking the grids on other nodes of the cluster. Obviously whenever we're doing some sort of sharding, we want to be using basically some sort of consistent hashing scheme to make sure that everyone is getting relatively even load in terms of the nodes in the cluster, at least relatively even load proportional to their compute capacity. Anyways, once those search results are returned, they can basically either be in the location cache or not and either way, they're going to hit the search service and be returned back to our client 
		- Okay, so that's all for the diagram. I hope you guys did enjoy this video, after this we are basically going to do [[Uber]] which is not dissimilar to this problem so we can hopefully breeze through that relatively quickly and then also [[Ticketmaster]] which is interesting because it starts to touch upon the subject of distributed locking a little bit. I'm really looking forward to kind of getting through those last two problems because then we're beyond those grokking the system's design problems which are the really commonly asked ones in systems design interviews since this book kind of just seems to be the standard for you know interview questions that get asked. Even though the book itself is really not that great, they're just very common questions, and as a result i felt kind of obliged to go through them. But yeah, I hope this is helping you guys understand all of these problems. Again, I'm always trying to provide insight that is at least a little bit differentiating to what grokking might be saying, and in this case, I just wanted to provide some more detail on that whole geographic index and a little bit more about the quadtree or how that geohash works in terms of database internals and getting the most out of having something like a range query. So yeah, I hope you guys enjoyed this, and I'll see you in the next one and until then stay frosty 