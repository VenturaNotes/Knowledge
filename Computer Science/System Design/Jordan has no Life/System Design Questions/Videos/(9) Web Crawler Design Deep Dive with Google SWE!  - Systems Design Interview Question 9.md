---
Source:
  - https://www.youtube.com/watch?v=cud-wqZwEk4
---

- Introduction
	- Anyways, welcome back to the channel everybody. Uh today, we're going to make a video about a web crawler which personally, I couldn't be more excited about because i feel like there's not really that great of solution to do this. It takes forever because the web is huge and there's a lot of information that you have to fetch when doing this. Anyways, I just want to make a quick shout out to myself actually for getting a thousand subscribers so if you'll quickly just lower your headphones. Anyways now that we're back from that, uh let me go ahead and focus up and prepare myself to uh go ahead and speak for a little bit. Basically like i said, we're making a web crawler. The entire objective of that is really to just go ahead and visit every single page on the web. However, I will formalize that in the functional requirements section as per usual, I just want to thank everyone for being a viewer of the channel, all the positive feedback has been overwhelmingly great, and yeah I plan on continuing to make these videos as much as I can. With that being said, let's get into it. I don't want to waste your guys time and we can get talking. 
- Web Crawler (Functional Requirements)
	- Okay, as per usual, we will be starting off this fine lovely video with our functional requirements where in the case of a web crawler, we really only have one and that's going to be crawl the web. What that effectively means is that our servers are going to make a get request to every single type of link basically on the internet. We're going to assume only html pages for now and not things like images or audio files or any other type of file, but yeah, basically our goal is, you know, say within a month, we are going to use our servers to basically traverse the entire digital web so that we can somehow process the pages and maybe index them or do something like that. Something that might be more general purpose. Probably for some sort of machine learning application or anything along those lines but the point is, that's kind of abstracted away here and our main concern is how we're actually going to get from link to link to link. The algorithm that we're going to take and maybe some sort of technology choices that can really optimize what we're doing at least a little bit. 
- Web Crawler (Capacity Estimates)
	- As far as capacity estimates go, basically we set our expectations to do this entire process within a month which means if there are 15 billion websites out there and about 100 kilobytes per page, we can basically assume that 15 billion websites per one month is about 6200 pages per second because we're dividing by four weeks, seven days uh you know and then from there 24 hours a day on and on and on, you guys get it. But additionally, not only are we reaching 6200 pages a second, our 15 billion pages with 100 kilobytes per page implies that we're processing 1.5 petabytes of data. So obviously this is going to have to be some sort of partition task in the sense that not only from just like a processing per second standpoint, are we doing a ton of work, but we're also going to be storing a ton of data. And we'll talk about the way that we can do some you know partitioning and replication in order to make our process resilient and in addition to that, to make it quick.
- Web Crawler (Architectural Overview)
	- So basically for this video, I'm going to start out by just giving the initial outline of a web crawling algorithm and then i am going to talk about some places where i feel like it can be optimized a little bit. Um, basically optimizing from the "grokking the systems design" solution which i as per usual feel like doesn't go into as much depth as it could with the actual technologies that should be used in this situation. So anyway, here's the general process for how we would go about building something like a web crawler. If any of you guys are familiar with graph traversals and have ever done basically an iterative breadth-first search or a depth first search, you know that we basically need some sort of data structure that is going to store all of the nodes that we're planning on visiting next. For now, let's call this, the frontier. So imagine we were doing breadth-first search. As we all know from our lovely data structures and algorithms, the way that you implement a bfs in an iterative fashion is by using a queue. Similarly, we're probably going to be using a queue. However, this is just more so going to be in a distributed fashion so if you're already thinking about it, i'm probably going to be talking about message brokers lately and in the sense that we want durability, probably some blog based message brokers as well. So basically we are going to start with some seed urls which we're going to place into our url frontier and basically every single iteration of this web crawler, we're going to select some of these urls from the url frontier that we haven't yet visited, we're then going to consult our domain name services where DNS is basically just a mapping from a human readable host name to an actual server IP address. So for example, wikipedia.com or my personal favorite youtube.com is going to have a single ip address that is held by DNS servers all over the world. Unfortunately, these DNS servers are themselves sharded and so it's probably going to be the case that we're actually going to have to make a network call to them rather than just locally storing the entire thing which would be ideal for processing of this scale, but i'll touch upon that more later. So basically after we call DNS and get the IP address of a given server, what we're then going to do is fetch it on some of our own servers, get that HTML, parse it, in order to find other urls to be added to that frontier assuming we haven't yet seen those URLs and we can basically check to see if we've seen them, and then additionally, what we're now going to do is process that HTML document. Again, this would be the extra stage that we've basically said we're abstracting away for now, we're not thinking about it, but this could be, you know, placing it into a search index. Just taking the HTML itself and putting it somewhere into some blob store or something like that, and at this point what we probably want to do is perform some sort of de-duplication step. The reasoning for this being and this was actually new to me, is that there are literally just tons of websites mirroring other websites on the internet where they have literally the exact same content. So as a result, it's important for both space and just kind of the validity of your results to say, not only do we not want duplicate urls to be searched, but we also have to look at the content corresponding to that URL and make sure that it's not duplicated. You may start thinking now back to something like our youtube problem for example where we were thinking about getting rid of duplicates or even potentially that dropbox problem because kind of the methods that were used in those are going to be pretty relevant here and then finally once that document has been processed or fetched onto our nodes, we can go ahead and add those to some sort of storage that we have. Because we have 1.5 petabytes of possible documents that we're going to need to store, it's probably best that those are going in something like s3 or some other sort of object storage where we basically have limitless elastic scaling abilities and can cheaply just put a bunch of stuff in there. So anyways, I'm now going to talk about some optimizations for this algorithm because I more or less gave a general overview, but i think there are certain technologies that we can use here that is going to really simplify this process and give us a potentially better experience than if we were to just kind of do this all in a really wishy-washy way. So the first consideration that i actually want to talk about is the type of graph traversal that we want to do. Technically, we could do breadth first search using something like a queue. We could also do a depth first search depending on, you know, what data store we want to use as our queue and i guess instead of using a queue, we can basically just implement a stack instead. Um i guess it doesn't really matter as much. I think breadth first search probably makes the most sense to think about conceptually where you're kind of just taking all the URLs from a page, and then probably going to be parsing those around relatively the same time. Another way of actually basically traversing pages is rather than going from all of the links that you find from one given page, once you reach basically a given website, so say we have, you know, `wikipedia.com/a/b`, you also are traversing wikipedia.com and also wikipedia.com/a. The reasoning for this being that there's likely going to be stuff in all of those sub domains and then we can probably quickly iterate through that entire host name. The one thing to note here is that if we are to then basically take all of those links and put them back in our queue for traversal and then soon after traverse all of those, we may incur a lot of load on wikipedia servers and basically break the policy of their robots.txt file which more less says, just don't violate me when you web crawl me. So you know sites have their policies. They don't want to be reamed by our web crawlers as much as we would love to and as a result of that, you have to be careful when you're doing things like this kind of path-based searching. So for our next consideration, the thing i want to talk about is basically sharding and data locality because that's going to be how we're able to get the best possible performance that we can and additionally, there are some other concerns like you know using multi-threading for certain places and i'll talk about that in a bit too but basically we've said already that we're going to be fetching 6200 urls per second which is an absolute ton and we know that and obviously no single computer is capable of doing that. So this workload is going to have to be distributed and now the question is, if we're going to be distributing this workload, how should we do it in a way that makes sense. Well, the answer at least what i think should be the answer is we should probably be distributing it on nodes where basically the work is um hashed out or sharded based on the hostname. So for a URL like wikipedia/a/b/c.txt, the host name is going to be you know `www.wikipedia.com`. So all wikipedia links are going to go to the same computer and there's a couple of reasons for this and i'll explain those right now. 
	- The first is that we don't want to search duplicate urls. So if we're hashing things basically in a way that you know is consistent and not just you know sending random urls to random places, if we ever see a duplicate url, it's going to go to the same node that's processed (Bloom filters on each of the severs could also speed up this lookup!) and that means that we can basically store for a given node locally which other urls it has seen. So that way we don't need some sort of centralized database in order to go ahead and consult that and say hey, have we seen this url yet, but rather every single node can keep a local store of all the links that it's seen before and as a result, it can quickly perform that check of saying hey, have i seen you or have i not. Furthermore, this type of partitioning schema can really help us take advantage of data locality as far as DNS goes. As i mentioned, DNS is huge in the sense that it's basically mapping every single possible host name to an IP address and as a result of that, we probably aren't going to be able to put an entire DNS locally on one of our processing nodes as you know, say, we've said we might do in the past when doing something like stream processing or batch processing and doing a join. So instead, what we're better off doing is using caching. So basically each of these nodes that is processing um, you know, a given set of host names and we can kind of just choose that using consistent hashing so they each get a relatively even subset of the host names, and an even workload, any node doing this processing can locally cache all the DNS results that it's getting. The reason this is really useful is that a hostname may have many sub-urls, right? Like you can have all these slashes after a host name but it's only going to have one ip address that we care about and so by caching the first call to dns, we can then make very few network calls in the future which is really great. The one thing to note is that because of the fact that we don't want to be hitting a given server too many times consecutively, it might limit the efficacy of this cache because, you know, say that result may have expired by the next time we get around to this hostname. So in an ideal world, the cache will be big enough such that we can hold all of the relevant DNS results for that node, but practically speaking, we may have more cache misses than we might hope. So anyways, like i mentioned, we can basically cache a bunch of these DNS results and additionally, we can cache or hold in some sort of local storage all of the urls that we've processed on this node so we can quickly tell whether or not we've seen it. So so far, we've actually been able to take advantage of data locality really well and i'll explain in a little bit what technologies i think we should use in order to do that but the one area where we really can't take advantage of data locality is basically checking for duplicated html files. The reasoning for this is just that I can have wikipedia link "a" over here and then i can have i don't know some random blog over here, and they may be the same html page but at the end of the day, because they're on different servers, their host names are different, and as a result they're not going to be sharded to the same server most likely. So generally speaking, we're probably going to need some sort of centralized metadata store that is going to contain hashes of all of those html files so that way we can check for duplication. That's really annoying obviously because it's going to more or less require extra RPC calls or HTTP network calls in order to check for duplicated files and obviously that's pretty frustrating, but at the end of the day, there's really nothing you can do about it. That being said, the one thing i would recommend here is probably doing this deduplication process in a different thread as the actual fetching of the HTML. The reasoning for this being that even once we've parsed that entire document and seen the links that the document points to, we can really quickly on our main thread or basically our fetching thread tell if, you know, we should actually be searching those links in the future and on the other hand, it's going to take a much longer time to do the deduplication and so we could probably leave that to a separate thread which then goes ahead and checks for whether or not that HTML file is in a centralized database by you know basically making a query based on the hash of it and then if it's not in the database, we can upload it to our blob store such as s3. Just a quick thing to touch upon is that this means that we're probably going to need some sort of metadata table which is matching a host name to some sort of hash or s3 link, probably both. And then additionally what that means is this gives us an opportunity to further parallelize uploads of HTML files to s3 because we can break those into chunks again and what that also means is that it gives us opportunities to further check for basically duplicates of all the chunks and then if only some of the chunks are duplicates and some aren't, then we can only upload the differences and then basically construct that file in our metadata table. So if we are using chunks, then our metadata table should probably be something like a relational database simply because then we're going to want transactions to actually go ahead and make sure that all the metadata chunks have been logged in the database. Maybe you could use something like Cassandra and you know the partitioning would work out nicely but again i feel like having the acidity aspect of it, when you're dealing with chunks in multiple database entries is really useful to have, and then finally in terms of the actual technology that we want to be using, one thing that kept popping into my mind the more i heard about this problem, was you basically want a bunch of nodes that are, you know, doing some sort of processing work. They're keeping some sort of internal state. It's very very important that that internal state is durable and can be reconstructed, right? Like we don't want to be able to parse some urls and then you know one of our parsing machines goes down and then we completely forgot the fact that we parsed that URL or now we're not going to you know parse all the things it leads to, and then in addition to that we have this frontier which has a generally queue-ish nature and that also has to be sharded in a certain way and also has to be replicated and to me, the more i thought about this, it just kept sounding like a perfect scenario to be using something like Flink and Kafka where Kafka is basically going to be our URL frontier and that's going to be a partition queue where every single time you are figuring out new URLs on a flink node that have to go into the frontier, you just send them right back to Kafka and that can also be done in a separate process so as to speed things up a little bit and then every single time you're now pulling in a new URL to process, you pull it in from a Kafka queue to a given flank node which is done one at a time. That is going to be completely durable because the Kafka queue is going to be based on disk and then, you know, if any of our processing nodes ever fails, we can use the basically the way that flink checkpoints state through a bunch of distributed snapshots in order to basically restore our state on either another node or something like that and then continue processing our web crawler. It seems like the Flink/Kafka combo is a really valid way of actually doing this web crawling and obviously like I said, we're going to be caching a bunch of state on each flink consumer such as the scene URLs and previous DNS results for a given hostname. So it seems like that's kind of the best way to do this. So with all this being said, let me go ahead and draw a diagram.
- ![[Screenshot 2024-11-22 at 10.18.54 PM.png]]
	- Web Crawler (System Design Diagram)
		- Okay, finally let's take a look at this diagram. Got a little messy with these arrows but with all things considered, basically we're going to have the following because i'm trying to emphasize this here. As you can see, we've got all these partitions of flink processors corresponding to a partition of a Kafka frontier. So basically the point here is that i only drew out three, but in a real web crawler, we probably have hundreds of these where all the flink processors can write both back to their own queue. They can also write to the other queues because sometimes they're going to parse URLs that are in different partitions than theirs and if so, they're going to have to send that to the right queue. Fortunately Kafka does manage that load balancing for you which is great, and then in addition to that, not only are they basically consulting their local state which you can see based on that little database that i've drawn next to every single flink processor, but also they're going to have to be uploading chunks to s3. They're going to have to be getting DNS results and caching them in their local state, and then finally, they're going to have to put corresponding chunks or at least the hashes of the chunks and their s3 links in a metadata sql table which again, I just use sql because it's simple, everyone knows it, it's transactional, and here i'm just imagining that we'll be using something like single leader replication. Realistically, there's probably going to be too much metadata to just go ahead and have it all in one huge table so that's probably going to have to be sharded in one way or another which is honestly no big deal. It can basically just be done in a manner similar to uh something along the lines of the way that the flink processors are sharded, but yeah, it doesn't really matter. There's no relations here, we just have to get our SQL table up and accessible by anyone so they can do those de-duplication checks.
- Conclusion
	- Okay guys, I hope you enjoyed this video, um i think this was the last one in "grokking the systems design interview" that i don't know, I felt was just like very niche and kind of just like memorize the solution whereas the rest are starting to get into more general stuff which i really like and getting back to just like to come up with a cool design aspect of things and I look forward to doing that. Again, thanks for all the views on this channel. If i manage to not get fired for all these stupid jokes, i'm gonna keep making videos and if i do get fired, then i'll definitely keep making videos because i will have no choice and no other source of income, although i guess i'm not making money on this right now, it's more so for the passion. So with that all being said, I look forward to seeing you guys in the next one, have a good night.