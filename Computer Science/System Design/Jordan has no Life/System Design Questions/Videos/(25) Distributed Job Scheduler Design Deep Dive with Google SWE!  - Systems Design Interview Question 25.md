---
Source:
  - https://www.youtube.com/watch?v=WTxG5880EH8
---
- ![[Screenshot 2024-11-28 at 4.26.33 AM.png]]
	- Introduction
		- I forgot to do a job scheduler and now i'm going to go ahead and do it. This is a pretty tough topic for me to talk about, i know. So anyways like I mentioned, we got one more video here at least for the foreseeable future for the systems design stuff and that is going to be to job schedule. So anyways, let's get into it and do our normal four-step process.
	- Job Scheduler (Functional Requirements)
		- Description
			- Schedule the running of binaries on our dedicated compute cluster
			- Instant and repeated scheduling
			- Jobs must be run at least once
		- Alrighty as per usual starting off with functional requirements. Basically here is going to be what we need to implement. So we have to be able to schedule jobs that are going to run on our dedicated cluster of compute units which are probably just going to be some sort of servers in a data center. In addition to that, we're going to allow multiple different types of scheduling so that could be something like instant scheduling where i want to say, you know, go ahead and run this now or also scheduling according to some sort of like calendar schedule or cron job where you're like, you know run every two weeks, run every month, run every day at five o'clock p.m EST or something along those lines and additionally it's very important that every single job is going to be run at least once. We never want it to be the case that you can kind of have a job and you know it just somehow manages to get overlooked by our system. That's not acceptable. Every single job has to be run and you know we can talk about how we're going to be able to ensure that. I'm not necessarily going to say that jobs won't be run more than once, but we can talk about ways to kind of make things idempotent and deal with them and we've spoken about that a bit in the past as well. 
	- Job Scheduler (Capacity Estimates)
		- Description
			- Binaries are in the order of magnitude of a few megabytes
			- 10,000 jobs per day
		- In terms of the capacity estimates, I'm going to assume that all the jobs we're running are going to basically be compiled binary code. So you know, you run it in the compiler, now you have this binary file, and you can go ahead and upload that to I guess the server somewhere and pull that from there later to actually go ahead and run that, like that's kind of going to be what we're scheduling. So let's assume that the binaries can be up to a few megabytes in size and then additionally that you may have tens of thousands of jobs running per day. So this isn't going to be an easy task, we're definitely going to be needing to do some amount of sharding in order to make sure that all the jobs aren't running on just one server. It's not possible to just run them all sequentially, so start thinking about that. 
	- Job Scheduler (API Design)
		- Description
			- scheduleJob(binary, timeSchedule)
			- jobStatus(jobId)
		- For our API, there are probably going to be two main endpoints that we want to implement at the bare minimum. The first is going to be to go ahead and actually schedule a job and that's just going to take in basically the binary of the the code that we compiled and additionally perhaps some sort of you know like timestamp or you know cron syntax where it kind of specifies how often you want the job to be run. Additionally, we want to be able to check the job status whether it's been completed, failed, being retried, you know in the queue or something and obviously those states are going to be things that we talk about a little bit more later but the general gist is that we need to be able to see kind of what the state is of our job that we wanted to be run.
	- Job Scheduler (Database Schema)
		- Description
			- JobStatusTable`[jobId, binaryUrl, status, retryTimestamp]
		- Okay finally, for our database. Anyways for our job status table which we've already spoken about a little bit, there is going to be a job ID for a unique job. There's going to be a binary url and that's probably going to be because the binary should be stored in S3. Like i said, the binary can be a few megabytes in size and as a result since it's a static file, it should be going in something like S3 which is just an elastic object store. Then additionally, we're going to have a status that could just be something like an enum where it basically says, you know, started, not started, finished, failed but retriable, failed but not retriable, and then also some sort of kind of time stamp or an expiration time stamp where you know that time stamp could basically indicate if the current time is after that timestamp, then it means that you know that job could in theory be run again if the status is something like hasn't been started so we'll talk about the rules or kind of semantics a little bit more in a bit for when you actually want to restart a job.
- ![[Screenshot 2024-11-28 at 5.07.17 AM.png]]
	- Job Scheduler (Architectural Overview)
		- Description
			- (1)
				- Timestamp when status = not started: scheduling timestamp
				- Timestamp when status = enqueued: enqueue timestamp + queue timeout
				- Timestamp when status = claimed: claim timestamp + heartbeat timeout
				- Timestamp indicates a point after which the job can be retried
				- Successfully running jobs will constantly be updated in the DB, no need to retry
				- If we read DB at `9:49`, only jobs `47`, `63`, and `91` will be sent to the queue.
				- Check for jobs with retryTimestamp < currentTime and status != complete
		- All right, so let's talk about this architecture. At least from the client's point of view, the first thing that's going to happen is they're going to actually want to schedule a job. So how can they do that. They're just going to use some sort of HTTP or RPC call to our backend service and we can use something like a reverse proxy and just have some sort of front end server which is going to first handle any incoming requests which is going to contain like I mentioned earlier, both the binary of what it is that we want to schedule and also some sort of like cron type syntax saying when you actually want to run that job or perhaps just something saying i want to run the job as soon as possible. Okay, so the first thing that's going to happen is like i mentioned, since the binary is just a static file our service can go ahead and throw that into S3 no worries and then now we have to deal with the actual scheduling of the job. So intuitively you may think to yourself, well shouldn't we just put the job in a queue and call it a day and we can just have you know servers pulling from the queue and running everything. It does make sense, however there are a couple major issues with it. One being that depending on what type of queue you're using, you're not getting a lot of `retriability` features there and there's also no way to then store the status of what actually you know happened with the specific job. It would just go ahead and get run. So as a result, we need a little bit more of a complex design than that. We're going to need some sort of database and what the database is going to do is store metadata about each job. It can have something like a job ID. 
		- (1) It can go ahead and have something like the url to the binary to be run in S3 and additionally, we're going to need a bunch of timestamps and i'll explain what these timestamps are for right now. So let's say you want to run a job instantly. Then the time stamp corresponding to that job in the database would just be the current time. You're saying, anytime after you know the time stamp corresponding to that job, we should be running this. On the other hand, let's say i want to schedule a job to be run an hour from now. Then i would put a timestamp in the database one hour from now and then that way, any subsequent reads to the database if the current time is less than that time corresponding to the job, it'll know, hey, we're not ready to run the job yet. It's still too early. And so now we're basically going to have all these queries to the database that are trying to pull all of the jobs where the time corresponding to the job is less than the current time, as in it is later than when we were supposed to run this job so now we can actually run it and just based on the pattern of querying that i just described, what that really means is you want an index on that actual timestamp of the job. This way you can really quickly query for all the jobs that are less than a current timestamp and that should be relatively fast. 
		- In theory, you could also use a time series database as well but truthfully the actual querying of the database here is probably not the main performance bottleneck in this whole thing so it's probably not a huge deal if we use MySQL. In addition to the fact that you know then we get some acid compliance and this may help us down the line a little bit and we'll talk about that later. So what is our actual kind of query to the database look like. Well it should be something along the following lines. Find all the jobs in the database where the status is basically not complete. So either it hasn't yet started or it has started. So why do we care about jobs that started? Well the reason we care about started jobs is because the actual execution of a job throughout our scheduler may have failed somewhere along the lines and it's possible that we have to actually go ahead and restart that job by basically re-queueing it again and we'll talk about this in a second, but they're going to be a variety of points throughout our system where jobs can fail. For example, if we were to place a job in a queue and it never got pulled from there, well at some point we're going to have to re-enqueue it, so that we can try it again. If we were executing a job on a machine and the machine went down, well at some point you know that machine is no longer sending heartbeats and we're going to have to go ahead and restart that job and so as a result, there are all of these potential timeouts that we have to think about where even if the job has started, if um it's currently later than kind of the last checkpoint of that job plus the relevant timeout, then we should consider that job failed and we have to go ahead and retry it. So basically the query then that we're making to the database is something like this. Go ahead and take all the jobs that haven't started yet with a timestamp that is less than the current time and also go ahead and find all the jobs that have started, however their timestamp that kind of you know they were last checked into the database so for example you know they're in queueing timestamp plus the actual you know hard set in queueing timeout which we've set in our system is going to be less than the current time which would imply that, you know the job has been in the queue too long, it never got pulled, or something went wrong. We have to re-enqueue it. So that's kind of exactly the query that we should be using. So more or less what's going to be happening now is we have some system you know every few seconds maybe every minute pulling from our database for all of the jobs that have to be run and once they do do that, they can go ahead and upload all of them to a message broker. Now you may be asking well should we be using a log base message broker, an in memory message broker, actually this is going to be one of those cases where an in-message memory broker is actually a little bit more suitable. The reasoning for that being that we're going to be retrying jobs automatically so they don't have to be durable within the queue and additionally an in-memory broker should generally be a little bit faster and we don't really care about the order in which the jobs are run, we just need them run. So in memory message brokers here something like basically SQS or RabbitMQ are probably a little bit more suitable than something like Kafka which is going to be a log based message broker. Okay from there we basically have some sort of load balancer which is going to be directing objects from those queues which probably should be sharded in one way or another so you can actually go ahead and distribute the jobs that are coming off of them, otherwise there would probably be too much load on one individual queue. It doesn't really matter too much exactly how those queues are sharded because truthfully, a lot of these binaries aren't going to share anything in common. If it's the case that we're only running a few different types of jobs, then it would be ideal if the same consumer nodes were kind of running the same jobs because then you could take advantage of you know data locality, caching, you wouldn't have to load the binary from S3 many different times but assuming we're running all different sorts of jobs , hen it probably doesn't really matter how they're sharded. So anyways, basically you have a load balancer that is directing elements from all of these queues to a bunch of consumer nodes. Once the consumer node goes ahead and pulls a job from the queue, it's going to load it in from S3 and then basically make a claim to our claim service saying I am running this job. The reasoning for this being that we don't want multiple nodes running that same job at the same time (This is definitely possible because the same job may be enqueued many times - think about if a job gets enqueued but the write to the database to change its status fails... since we aren't using two phase commit, this is possible.). Effectively what they're doing is they are hitting a distributed lock which we can manage behind the scenes with something like zookeeper and saying i am currently the only one running this job and i should be the only one running this job. One more thing to actually touch on is, it is possible that if we want to kind of deal with priorities within jobs and you know give certain jobs higher priorities, you could even do multiple different queues where each queue has you know a different priority label (operating systems do something somewhat similar to this with multilevel queues for CPU scheduling!) and then the basically the jobs and the highest priority queue are going to be pulled from first. Um obviously this can be an issue in the sense that certain jobs are starved because you know they're in a lower priority queue and high priority jobs keep coming in. One thing you could do there, something that's kind of used in operating systems where you basically have mobility between the queues, you know you could have one consumer basically reading from the low priority queues and saying shoot this thing hasn't been run for 20 minutes, but it's low priority, i'm gonna have to move it to a higher priority queue. Again, that's probably a little more complicated than it needs to be for this type of system but it's just one thing to note. So basically anyways now once we have all of these consumers running jobs, they have claimed that job using a distributed lock, a few things are going to happen. They're going to basically periodically be sending heartbeats to that zookeeper or service and the reasoning for this is basically zookeeper needs to know if the consumer running the job is down or not. Assuming it's not down, then the basically the consumer can continue to run that job and if the job either completes because you know it was successful or it fails due to some you know coding error, then it can go ahead and update the status of that in our metadata database. We know that completed jobs or jobs that failed due to a non-retriable error are not going to be retried the next time around which is good. On the other hand, if it's the case that basically our consumer server goes down because you know it just happens to fail, then the zookeeper server is going to stop receiving heartbeats and then we can go ahead and update the metadata database to basically say that even though the job failed, it's a retriable error and eventually we know that our basically long polling of that database is going to eventually pick up that job again to run it again. What if it were the case that the consumer node went down so eventually basically the job ends up getting retried but then the consumer comes back to life and continues to try and work on the job. This could be problematic because now we potentially have two consumer nodes servicing the same job at once which shouldn't be happening. Well again, that's kind of the point of the distributed lock. We basically need to make sure that every single time one of these nodes comes up, it's sending heartbeats and trying to grab the distributed lock and you know by virtue of having something like a fencing token in the background, we make sure that it's never the case that two things can both be working on the same job at the same time which is very important. Now you may be asking yourself, okay well we've ensured that through a bunch of time outs, every job is going to be run once, we've ensured that basically you know each job probably won't be running more than once via the use of distributed locking, are there any other concerns. And the answer to that is technically still yes. In the sense that you know what if one job is in the queue, uh the job eventually gets sent from the queue to one of the consumers, however it seems like to our service, that it was never actually sent from the queue and then we re-enqueue that job from our database. Now what's going to happen is we may have had two of the same job being run but they just weren't run at the same time. So now we've had the same job running twice. Well basically the only way i can think of to prevent this is to take some extra measures to ensure idempotency of your code where basically perhaps you're consulting like a different database table or something to see if something with that exact request ID has been run before, but you know this isn't necessarily a requirement. It's just something to think of if you want to ensure that none of your jobs are being run more than once. It is still technically possible here. Okay, and then the final piece of the puzzle is that if you recall from much earlier, I mentioned that it's possible for jobs to be scheduled in a way that it's not just saying run this now, but it could be you know run this every two weeks, run this once every year, or just in general on some sort of schedule. I would say then that the best way to probably get this done is you know if you think about it, it's actually really hard to do this problem because it would insinuate that you have to basically populate the database with all the jobs that are going to have to be run in the future up to a certain point and that's not possible if you're saying run this job indefinitely on some schedule. So i think the best way to do it is to go ahead and actually kind of amortize this computation where let's say i have a job that's supposed to be run every two weeks. When one version of that job completes, you know like the the first one and it's successful or fails then, basically what's going to happen is the service that goes ahead and updates the metadata base to say that this job succeeded or failed is also going to add an additional row in the database saying here's the next one that has to be done two weeks from now. So basically the previous job is going to put the next job that has to be run in the database and you can use a transaction to do this in order to ensure that basically both of those updates are either going to succeed or fail.
	- Diagram
		- Alrighty so as per usual let's take a look at the diagram so we can go ahead and visualize this a little bit easier. Like i said, the client is going to hit our front-end scheduling service with some sort of binary and also a schedule that they want the job to be run on. The binary is going to be uploaded to S3 and then once that happens, you can go ahead and put the job in a row of the MySQL job status database. Once we're in there, we can do the following: our queueing service is basically going to be polling that job status database every you know same minute and taking all of the new basically jobs that were either failed and need to be retried or just have never been run in the first place and once they do so, they can place them all in the RabbitMQ or SQS. Just a reminder, this is just going to be an enqueue that is in memory and then from there all of those jobs in the multiple partitions of queues and it doesn't really matter how the partitions are made as long as they're relatively even in size, you can use consistent hashing for that are going to be load balanced to the various workers based on I guess their current load. That's probably the best way to do it and then from there the workers are basically going to go ahead and grab a distributed lock for this job and then finally as they're doing the job, are going to continuously be sending heartbeats to zookeeper so that if either basically the job is succeeded or failed or the worker happens to go down, at least someone will know about it and they can update the metadata data store accordingly so that the job can eventually be retried down the line. 
		- All right guys, I hope you enjoyed the video. I'll take any more suggestions for systems design videos. The worst thing i'm going to do is say no and you know that's how it goes. Shoot your shot. Let's see it. So anyways, uh yeah i'm glad i still get to make these to some extent and looking forward to continuing to learn with you all and making some more heinous and disgusting content 
