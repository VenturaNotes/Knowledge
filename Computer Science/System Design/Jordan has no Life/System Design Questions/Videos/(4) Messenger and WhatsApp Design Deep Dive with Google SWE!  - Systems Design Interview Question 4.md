---
Source:
  - https://www.youtube.com/watch?v=2Ejdj6QdDD0
---
-  ![[Screenshot 2024-11-19 at 9.31.45 PM.png]]
	- Introduction 
		- So today, we're going to be doing a systems design of basically facebook messenger or whatsapp or any other you know real-time chat messaging service, and i guess the kind of point behind this is that generally speaking here, I agree with most of what "grokking the system design interview" has said about this problem and so i'm generally going to be following their guidance if you've seen basically what they say about it, then it's pretty similar, but i do think i have a couple things to add in terms of the actual database choice, the partitioning of the data, and ultimately a little bit about kind of the race conditions and you know whether or not to use distributed transactions because the data has to be sent to multiple places. So you know, I'll try and focus on those areas but as always, I'm gonna do a generally holistic approach to the problem and give a hopefully good overview of it all, so without further ado, let's start actually talking about some functional requirements. Oh and also just a quick shout out to `Ragnarok` who basically commented on my last video more or less saying that by watching this channel, he was able to actually get an L4 offer at uber which is freaking sweet dude, like congrats to you. I'm actually so happy for you and it really makes my day to know that this channel has any semblance of an impact. It's really not that important to me at the end of the day what the gross number of subscribers is as much as you know that i can provide value to every single one of you by virtue of watching these videos and you know it makes me so happy to be able to interact with you guys in the comment sections. I had a pretty privileged upbringing and the truth of the matter is, you know, to see someone without a college degree pulling off these jobs that are obviously paying a lot of money, that's sweet and uh yeah dude, so congrats and i hope you're still watching these videos. Let's get the spread together.
	- Messenger and Whatsapp (Functional Requirements) 
		- Description 
			- Functional Requirements 
			- Group Chats
			- Send and Receive Messages in Real Time
			- Keep Old Messages in Database
		- Okay, so let's start talking about messenger or whatsapp and their functional requirements. I guess the two are slightly different in the sense that whatsapp, I don't think actually stores your messages in the background, but the one thing that they do have in common is the fact that if you hit girls up on either one, they're not going to be into you because apparently you can only send blue messages these days. I digress, let's actually get into some functional requirements. So for the main ones that i'm going to be covering um typically this is considered an extended requirement in the past but i'm going to consider it a functional requirement because it's pretty simple, group chats. So a chat can have a bunch of members, it's kind of one of the main features of both these apps. In addition, you want to be able to send and receive messages in real time, get them in low latency, don't have to worry about that and then finally persist storage chat history. Like i said, this may be a little bit different in whatsapp where you only kind of persist the chat history until it's been loaded on a device but for my sake i'm going to be storing all of these messages in the database. I can't really think of too many extended requirements, um, the original "grokking the systems design interview" had basically storing the online and offline status of users as a functional requirement and truthfully after reading that section, it felt more like that was kind of just like a front end section where, you know, you had the data in the database but it was kind of just like mostly optimizing on the front end, like when you actually wanted to pull it. So i didn't really feel like that part was too important to be included, maybe if you were a mobile engineer that would be more relevant here, but yeah, as far as extended requirements go, maybe push notifications or something like that, but overall, I think the functional requirements are what's important here, so let's go ahead and get those down and now that we do have them listed, we can talk about some capacity estimates.
	- Messenger and Whatsapp (Capacity Estimates)
		- In terms of capacity estimates, I'm again just stealing these numbers from "grokking the systems design" because it just makes my life a little bit easier and it's probably what you know an interviewer might give you. Assume 500 million daily active users with about 40 messages per user per day, so i guess we're all hitting up everyone. Comes out to basically 20 billion messages per day if you do the math there, just multiply those two figures and then if one message is 100 bytes because, you know, i don't know let's assume the average message is like 30 characters or something, you have a little bit of metadata, that comes out to two terabytes of storage per day because you basically just take the 20 billion messages per day multiply by 100 bytes and then if you multiply that over five years, you know, 365 days a year times five, you get 3.6 petabytes of total storage. So obviously that's a lot. We're gonna have to be dealing with some partitioning at some point or another. Um with that out of the way, we can talk about some of the api endpoints that we might expect to have in our service. 
	- Messenger and Whatsapp (Api Design)
		- Okay in terms of the end points that we have to talk about for a problem like this, there are a couple that aren't as important so i'm just going to get those out of the way. Things like, you know, creating a user or also just adding or removing a user from a given chat, but those aren't too central to i guess the problem itself, so the main two that i'm going to be discussing are actually sending a message which is probably just going to be a post request with something like a user id, a chat id, a timestamp for the message, and basically the message content, the text. Maybe an s3 link if we're allowing images or something like that but for this problem, i'll just leave it to text and then also an endpoint to fetch messages. So that should have something like a chat id and also some sort of pagination token. The pagination token should just delineate basically you know after what message are we now fetching new ones, so i guess this could be good if we're using some sort of polling system in order to fetch new messages. I don't think we're going to use a polling system and i'll discuss that in a little bit but the point is, being able to have pagination is useful because at some point, we're going to be fetching messages from the server and when we do, we don't want to be fetching, you know, thousands at a time. It'd be better to just fetch like 10 or 20. 
	- Messenger and Whatsapp (Database Schema)
		- For our fourth section of the video, we are going to be talking about database tables. So the ones that i could come up with, obviously we're going to need some sort of users table which is going to have something like a user id , an email, a hash of the user password to be able to store there. The one thing that I noted here is that even though we would probably just end up using a relational database for this out of simplicity and data denormalization, I think that it could be really useful to use something like a graph database here and i'll discuss this a little bit later, but you know, just by virtue of being able to kind of partition that graph database in a way that you could put all related users on the same partition, I think that could have some interesting implications for speeding things up a little bit but i'll talk about that later. Additionally, we'll have a chats table which is just like chat ID, chat name, chat photo. Can also be relational, it doesn't really matter and by the way, all these tables so far can just use single leader replication. No need to deal with write conflicts for you know any of the tables that performance isn't like the main thing here, right? There's also a user chats table which can just have two fields, user ID and chat ID, and we can put indexes on both such that for a given user, we can quickly see all of their chats and for a given chat, we can quickly figure out all of the users that relate to it and then finally, this is where i think we should deviate from SQL is the actual messages table itself which i think should have something like a partition key of chat ID, um a field for the message timestamp and also the content of the message. So basically, I think that as opposed to using a relational database, we should most certainly be using a NoSQL database like Cassandra or HBase for this messages table. Now the reason for that, is that, there are obviously going to be a ton of writes because sending messages are all a write to the database and due to the LSM-tree architecture, a NoSQL database like Cassandra or hbase here, is going to work much better. All those writes are going to go to an in-memory buffer first. However, I should note the following. Um, it is the case that "grokking the systems design interview" felt that HBase should be used, and truthfully i'm not sure i fully agree with that. This was basically their logic. They said HBase is good because of the fact that um you know it's schema-less which i agree but Cassandra is the same thing and in addition to that, it's actually column oriented storage and i don't really know that this gets us that much of a benefit depending on what's actually going in the rows of the messages table because column oriented storage is really useful when we only need a couple columns of each row at a time but if we're going to be fetching the entire row of that message table at once, then it doesn't really get us any benefit to have column oriented storage. If we're going to be you know doing some analysis over all of these messages at some later time and you know to do so it's really useful to just export a given column, then i understand that but i feel like generally speaking when you fetch messages, you want their metadata, you want the content of the message, you need the timestamp, so i don't know how much column-oriented storage would help us out here. On the contrary, I think that Cassandra may be a little bit of a better choice here than HBase because Cassandra uses a leaderless replication schema which means that especially for people who are geographically distributed pretty far away from one another you know if i'm messaging my friend who's in japan or something, he can go ahead and write to basically a bunch of databases in his local data center and that write is going to go through a little bit earlier. It does mean that i'm going to have to wait for some sort of anti-entropy or read repair process before i'm able to actually see those writes but actually even then that's not necessarily true and that's because in addition to writing to the database, we're also going to be discussing how the messages are going to get actually pushed to the other members of a given group chat. So that's why i think Cassandra is the better choice here than HBase and i kind of diverge with "grokking the systems design interview" in that take right there. The one more thing i was going to say is that we probably do need some sort of data store whether that's a database or a local data store to keep track of basically um which clients, so you know which devices that are going to be receiving messages are mapped to which chat servers so the chat servers will eventually be able to push those messages to them. However, I think that's something that doesn't necessarily need like a database instance like say redis or something, because that would be the fastest one, but instead could be done via the use of a load balancer and some sort of consistent hashing algorithm such that servers could quickly basically figure out where a client is connected to and then map that message to them. But we'll talk about that a little bit more in the design section and let's go ahead and move on to that now.
	- Messenger and Whatsapp (Architectural Overview)
		- So basically, the first consideration i want to get to is this. We know that we want to be pushing our messages real time to clients as they come in, after you know say i send my message to the server and then i'm in a chat with someone else and they want to be able to receive that message in real time. So what technology should we actually be using for pushing the messages? Well, I'm going to propose a few considerations. So the first thing is that as we know we have 500 million daily active users and we can basically go ahead and estimate that a server is capable of handling around 50, 000 maybe 65, 000 open connections at once right? Which means that if that's the case it is impossible for one server to be handling you know 500 million web sockets at once. Instead all of those WebSocket connections or whatever type of connection we end up using, will have to be distributed horizontally over a bunch of application servers. So in order to do that, we should probably be using a load balancer with some sort of consistent hashing mechanism in order to go ahead and route a given user id to a given application server um for you know web hooks or server side events or long polling to go to. Um the one thing i would say is that there's not really a great way to do this right, such that all the users of a given chat are together on the same server because that would minimize the network latency that we would have to use but that's kind of why i made that remark earlier about potentially using a graph database for users because if we could somehow partition up like our users database in a way that you know all related users are going to be next to one another on the same partition, then maybe we could load balance those WebSocket connections as well such that all related users, all users who might be in chats together are going to have their WebSockets on the same servers, but obviously that's pretty complex and i think it's probably beyond the scope of this video. Generally speaking, I think we can just assume that our load balancer is going to use round robin or something and that for a given user, it could be connected to basically any application server to get its messages received from. So going on from there, sure we understand that basically every single user has to be load balanced to some sort of chat server to receive messages, however the question is now should we use WebSockets to do this, should we use long polling, or should we basically use server sent events? Um and so i wouldn't say there's a conclusive great answer obviously you don't want to use short polling which is basically you know hitting the server every five seconds and saying give me something new, long polling could work but it comes at the issue of having to resend headers every single time which is annoying because it means that every single message that is basically going to be sent to that device, it's going to have a bigger payload which means that there's going to be more network latency. On the contrary, web sockets and server sent events are both kind of persistent connections which is good, however, it means that if that chat server were to go down, we would have to go ahead and reload balance all of those clients to a different chat server for them to instantly re-establish that handshake and what that may result in is a thundering herd problem where the increased load of all these clients trying to create a handshake with one of our application servers may lead to a cascading failure and actually take it down. So there is an argument to be made here for using long polling as opposed to one of these slightly more complex real-time i guess technologies like web sockets or server sent events. As far as web sockets or SSE goes, WebSockets are a bi-directional connection which we don't necessarily need here because even though clients are going to be sending messages, they can just do that over a typical post endpoint and additionally the one cool thing about server sent events is that they will actually re-establish a connection automatically if they're to fail and web sockets don't do that. So perhaps with all these things in mind, I would stick maybe between server sent events or long polling as opposed to web sockets because i don't want to deal with the thundering herd problem and even though that may happen with SSE, at least we know that if an SSE connection were to fail in the short term, it's going to be re-established on its own and um again it's just a one-directional communication but you know you can make an argument here either way. Okay additionally now that we've spoke about receiving messages a little bit, let's actually go through the process of how we might send them. So obviously one client such as myself is going to want to send a message and in order to do so, i'm going to make some sort of post request to one of the chat servers which i'll just go ahead and you know i'll hit the load balancer and then it'll send me to a chat server to go ahead and post that message. So once the message reaches the chat server, it's going to have to go ahead and do two things. One of those two things is push the message to the database and the other is that it's going to have to push the message to all of the clients in the other group chat, so the way it would push the message to the other clients in the group chat is by just going and hitting the load balancer saying, please send all of these messages basically to the proper chat server so that it can send them via the WebSocket or the server sent event or the long polling and then additionally, it also just has to go ahead and make a typical http request to the database server to go ahead and add that message in there. The one thing here is these are technically going to be concurrent processes and what happens if one of them works and say the other one fails. Do we want a distributed transaction here? Probably not because it's not that important. It's not like we're dealing with financial data or anything that both of them go through, but perhaps you know if you care about the case where all the clients receive the message but then the message isn't persisted in the database we could say i don't know there's an error with the database connection, it's kind of problematic that all the clients might receive this message have it cached locally and then the message isn't in the database so maybe what you could do is upload the message to the database first and then go ahead and upon receiving the success message, send that out to all these different clients but i think there is a consideration basically between whether you want to do the database and the client uploading concurrently or whether you want to basically wait for the message to be in the database before sending them out to the clients. Additionally another concern i saw that "grokking the systems design interview" brought up which i don't really think is a valid one is ordering messages amongst all the members in the chat. They basically said that even if you were using timestamps for the messaging, that not all of the participants of the chat would see the same order of the messages and this isn't true, it just kind of depends on how you use your front end because they were basically just saying um if i were to send a message and then received a message from someone else, even if that received message had basically an earlier time stamp than the one i sent, I would still see the received message later and I would just respond to that, well why not just use a front end that maps out the messages in the order of their timestamps and then that way everyone sees all the messages in the same order. Obviously though, timestamps aren't perfect. What timestamp do we actually use? Do we use the timestamp on the device that sent the message, do we use the timestamp of the server that received the message. As we know, timestamps aren't perfect and any distributed setting, um, timestamps aren't going to be perfectly aligned with one another and so we can't ensure that the ordering of the messages is correct with like the actual time that i clicked the send button, but we can ensure that using timestamps, it's at least consistent amongst all the members of the chat. Like i said, it just kind of depends on how you map them out with your front end. With that in mind, I think we generally speaking covered all of the you know kind of deep distributed systems concepts that can come up in a problem like this, so as a result, I would like to go into the diagram and walk us through that.
- ![[Screenshot 2024-11-19 at 9.35.06 PM.png]]
	- Diagram
		- Okay let's actually go ahead and start talking about this diagram that i came up with. It's pretty in line with what i've been saying this whole video and we can examine it. So let's imagine we have a sender aka me as far as this diagram goes and the first thing i'm going to do upon sending a message is it's going to go ahead and reach the load balancer. As per usual, the load balancer is a single point of failure and what i've gone ahead and done is said that we could put it in active-passive configuration for redundancy. If we need to share some state between the two, we could, you know, pull some sort of high availability stunt and use a shared coordination service between them such as zookeeper and the first thing that's going to do, is it's going to go ahead and hit our chat server. Now the chat server upon receiving this message is going to see the chat id for it and then go ahead and query our basically user chat table in order to figure out all of the user ids that it's going to need to go ahead and forward this to. Hopefully that data is going to be in the user chat cache which is just going to be a replicated partition cache based on chat id but if it's not, we can go ahead and hit the table itself. Once the chat server has this information, it can go ahead using the load balancer to forward all of these messages to the various receivers right after it first puts the message in our message table which is just going to be some sort of Cassandra instance. As you can see, it is leaderless and also going to be partitioned by the chat id with a sort key of the timestamp so that we can properly sort those messages on our client-facing front-ends. Then finally, once that happens, those chat servers are going to communicate with each other using the consistent hashing pattern delegated by the load balancer and then basically via server side events, deliver the messages properly. In this case, the receivers are going to be Gigi Hadid and Kylie Jenner because i am a fiend in the DMs. 
	- Conclusion
		- Hopefully that is a pretty simple description. Obviously, it's not too different from anything provided by the "grokking the systems design interview" or any other video that you could have found on youtube but the truth of the matter is i just wanted to put a little bit more discussion into the actual type of database that we might choose here. Some things like race conditions and also a little bit into things like the timestamps. So i hope this video did provide some additional value. Like i've been saying guys, it really makes my day to hear these videos are helping you out, so if any of you have more positive feedback interview news, um i'll happily give you a shout out on the channel and you know just makes me a happy guy to hear it. So best of luck to all you with your own stuff. I'm still learning myself and uh we'll get through it together