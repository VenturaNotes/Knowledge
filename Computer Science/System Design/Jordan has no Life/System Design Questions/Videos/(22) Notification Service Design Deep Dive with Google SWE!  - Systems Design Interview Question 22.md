---
Source:
  - https://www.youtube.com/watch?v=TpugGhXhdaU
---
- ![[Screenshot 2024-11-27 at 3.57.56 AM.png]]
	- Introduction
		- Today we're going to be covering building a notification service and for whatever reason I've kind of been putting this one off for a while because I thought it would be kind of similar to just building something like Facebook Messenger but there are some interesting nuances and I'll be sure to go over all of those. Um, there's some videos in the past that I've seen on this topic that kind of touched a little bit more into I guess some peripheral things that you do when building a notification service like ensuring there's no spam or you know authentication and certain things like you know compression, messages, things like that, for now I'm going to stick more so towards the general architecture and you know let me know if I need to dive deeper into the woodworks, but I think this video should hopefully be sufficient. Anyways let's get into it and start doing some functional requirements. 
	- Notification Service (Functional Requirements)
		- Description
			- Large scale notification service (per topic)
			- No duplicate messages
			- Store messages until they can be delivered
		- Okay, functional requirements. So the first thing is going to be that we want to build basically a large scale notification service for our gigantic user base. We can assume that we're some sort of service that is not just for one company, but we're building like a third party service and people can go ahead and actually reach out to us to publish messages for a given topic that they basically have built out. Additionally, we want to ensure that no duplicate messages are being sent and this isn't the easiest thing ever. It means there's a lot of extra things that you have to keep track of but it is something we'll be talking about. And then finally, the last thing to note that is that if a client is going to be offline, and you know they're not capable of receiving the notification at the moment, then go ahead and store that message somewhere so that we can deliver it to them later.
	- Notification Service (Capacity Estimates)
		- Description
			- 1 billion users who receive 100 notifications per day
			- 1 kb per notification = 100 TB per day
		- As far as considering the capacity of a service like this, imagine there are going to be a billion users because we're basically going to be you know doing notifications for the entire world and that each gets around 100 notifications per day. If the average notification size is about a kilobyte, then we're looking at around 100 terabytes of notifications per day which in addition to being a lot of notifications is also meaning that we're going to have to store a good amount of those because a lot of those users may be offline for whatever reason. Maybe their phone's turned off, maybe they're on an airplane or whatever but we're going to have to store a lot of these in some database somewhere and we'll talk about that later.
	- Notification Service (API Design)
		- Description
			- SubscribeUser(userId, topicId)
			- PublishNotification(topicID, message)
			- FetchNotifications(userId)
		- Um as far as designing an API for our service or giving some sort of spec or schema to base our work off of, we can imagine that we have basically three endpoints. The first is just going to be add a user to a topic and that can take in a topic ID or a topic name and also a user ID. Additionally, we want the main endpoint which is actually going to be to do publishing. So for a given topic, we want to be able to publish some message to it and then you know once that message has been published, all the users that are subscribed to that topic will go ahead and eventually um basically you know receive that message some point down the line and then finally we want to be able to fetch notifications for a given user ID. This is really only going to be called when a user was offline and they've basically come back and said shoot I was offline, what have I missed. 
	- Notification Service (Database Schema)
		- Description
			- Users(id, email, phone, webSocketConnect)
			- Topics(id, name, userList)
			- UnseenMsgs(userId, message, timestamp)
		- As far as the database schema goes, I can also think of probably three tables that we're going to need. The first one is some sort of users table in order to track things like their email and their phone number if we have to reach out to them through that type of communication and additionally if we're doing something like WebSockets or server sent events, which server they may be subscribed to in the event that you know we have to either re-establish that connection or send a message to a proper place. I'm not saying it's going to be 100% necessary. We may abstract that kind of logic behind something like zookeeper in order to do the kind of mapping for us, but we'll talk about that more later. Additionally, there's going to be a topics table where the topics are basically just every single topic has an ID, a name, and then basically a list of user IDs that correspond to it that are going to be receiving that message when it's being sent. And then finally, we're also going to have an unseen notifications table where the unseen notifications are just going to record all the messages that a user has missed in the time that they were offline and that's going to be, you know, like a user ID, the actual message itself, and then perhaps the timestamp of the message if we want to return them in an assorted order.
- ![[Screenshot 2024-11-27 at 7.27.41 AM.png]]
	- Notification Service (Architectural Overview)
		- Description (Refer to note in paragraph below)
			- Sorry if this part was unclear
				- Consumers are sharded by user ID and read from topic queues
				- They keep track of which userIds that they are responsible for are subscribed to that topic in local state (use Flink)
				- They then forward the messages to the user queue with the same range of userIds
			- I misspoke and said these consumers don't keep local state but they keep a mapping of which topics all of their userIds are interested in
				- e.g.
					- user1: `[announcements, football]`
		- Alrighty, so let's get into the actual design of our service. Basically this is going to have a lot of parallels both to kind of the Facebook Messenger design in some ways in the sense that we have to route all these notifications to the right users and in addition it's going to have some parallels to the Twitter kind of feed design and that's because every single time that we publish a message, there's all this logic that we have to do in order to go ahead and fetch all of the user IDs for a given topic and go ahead and publish that message to all of them. So basically we know that we want to be able to send each message potentially in many ways. That could be something like an email, it could be text, it could be sent directly to the client via something like a WebSocket, a long poll or a server sent event and as a result, the thing you should probably start thinking of is going to be stream processing because we have all these kind of complicated loads and fetches from the database that we have to do for a given topic in order to get all those users, and it's not really something that we can just do on demand if a user were to go ahead and say, okay we'll publish this message. We couldn't just do that in a second and then say you're good now. So what we probably have to do instead is do a lot of work in the background and as a result we're going to have to require some sort of stream processing framework. So the way that we should probably be doing things is using a message broker and I've talked about these plenty in the past and there are basically two main options here: you can either use an in-memory message broker which is really good when you have a bunch of tasks that need to be completed but you don't really care about the order that they're completed in and you don't really care about replying them if you know they don't work out and on the other hand you have log base message Brokers which are a little bit slower. Typically the messages only go to one or perhaps a couple of consumers but the good thing about them is that if for some reason you need to replay a message because something downstream broke down, you can go ahead and easily replay those messages because all of them are stored on disk and in addition to that, on the actual broker itself especially if you only have one consumer, it means that you can effectively order the way that those messages are processed, so we'll talk about that in a little bit. So like I mentioned, now we basically have all of our notifications and the first thing that we're going to do when we want to publish them is throw them in some sort of log base message broker like Apache Kafka and so one thing that's going to happen now is say one of our services down the line goes down, right. You know maybe our email sending service is broke and a day later, we're going to say shoot, we need to take all these notifications again and resend them out so we go ahead and replay all those messages but what's going to happen now if we replayed all these messages and that entire time the tech server was working, so we've texted all of our users already but we haven't emailed them. How can we go ahead and ensure basically that we're not going to be sending duplicate notifications. This is going to be a big problem that comes up. So in the past if you've watched a lot of my videos, there are a couple ways of doing this and one is kind of the right way to do it and the other is not but it works just so much better. So the official way that you would kind of keep these systems in sync is using something called two-phase commit. Two-phase commit is basically just a bunch of distributed systems uh you know first basically saying I'm going to do something together and then once they all commit to doing that, then the coordinator node of the two-phase commit is going to say, all right, go ahead and execute that message. However, two-phase commit is extremely slow as it requires a lot of over the network communication and you know they're just literally two phases of network protocols that you have to go through and as a result of that, it's really not very efficient to use in practice (It also isn't fault tolerant if the coordinator goes down) so instead a better thing to use in order to ensure that there are no duplicate messages being sent is something like idempotency. So idempotence basically means that you can have the same operation occurring multiple times and it only having an effect one time. So in this case, what we would do, is for every single notification, we could attach some you know unique or random number or key (could also be a hash of the message) that we can call an idempotency key and all this idempotency key is going to do is for all the downstream systems, you can keep track of all the idempotency keys that you've seen and if you've seen an idempotency key before, it basically tells you don't handle this message again because we've already sent it out. So that's good, however, there are going to be a lot of messages being sent out and there's a chance that you may have to end up storing these idempotency keys on disk because there's so many of them. Well what's a way that we know of that can actually really quickly tell you if we've seen something or you haven't or at least provide a good guess of it such that you don't have to do a search on disk. Well there's Bloom filters and that's actually a really nice optimization of saying, hey, have we seen this idempotency key before? Well, when we hash it, it's not in our Bloom filter, so no we haven't, let's go ahead and send out this message. So that's kind of a useful optimization to do if there are too many idempotency keys such that you have to store them on disk, but in an ideal world, we could just keep those all in memory. So that's kind of how we would deal with the whole duplicate messages aspect of this problem. So anyways, let's go back to the actual publishing step of this entire problem. We know we have our client, our client wants to publish to a given topic. Let's imagine now that we have all these Kafka queues where the queues themselves are partitioned on the topic ID. So you know if I want to publish to the topic like announcements, then I would publish to the queue that represents the announcements topic and we could kind of handle this distribution of partitioning via something like a zookeeper coordination service which is going to make sure that you know, every single service in our system is kind of aware what the active partitioning is and where to send each message. This is going to be very important in a kind of fan out system like ours where we're going to have so many different types of partitions and shardings because we're working at such a large scale, and having a system like zookeeper is going to really help us keep track of where every single message should be going. Ultimately however, even though every single message is going to just one queue per topic, we know that a topic can have multiple users associated with it and at the end of the day, what we really want is for all the messages for a given user to probably be ending up on a single end node or a single consumer node and the reasoning for that is that if we are going to be establishing something like a WebSocket between our client and our notification service, it's very important that the client can basically use a load balancer to only establish a single persistent connection with one node in our notification service as opposed to you know having to establish potentially many if they're subscribed to multiple topics. So it's important that we can basically start our system by having a notification being passed into a given topic and end our system with that notification being fanned out to all the different consumer nodes corresponding to every single user ID, so how can we actually go ahead and do this. Well from our topic queue, we can have a bunch of consumers where the message is fanned out to each of those consumers and every single one of the consumers basically represents a list of user IDs and all those consumers are going to do is basically take that message which corresponds to a list of relevant user IDs and basically push it to basically a second set of queues which I will call a user queue, and the user queue (refer to description) can have messages from a bunch of different topics but the point is for all of the user IDs that correspond to a given shard of the user queue, it basically means that we're going to have all of the notifications built in there so that we can then have a second set of consumer nodes which is ultimately going to be responsible for handling those messages and sending them out to the right place 
		- Finally, once we have our consumer actually going ahead and pulling from those user queues, we can do the following: we can either forward that message to something like an email service or an SMS service., we can try and use our persistent connection with a client if that's kind of the type of service that we're building in order to pass that message right to our client and keep in mind, the client is going to establish the proper connection with the right consumer node based on something like a load balancer and zookeeper and you know consistent hashing and then finally if that client isn't available, they don't have an active WebSocket kind of currently established with our consumer node and the consumer node can check this based on you know its local state. It can keep track of which open connections it has, then the consumer node can go ahead and send that message to a notifications database where the client will then be able to access it later. And so the notifications database is pretty simple. I think that um for the sake of fast ingestion, it's better for us to use something like a NoSQL database because then we have an LSM-tree based architecture and you can write to an in-memory buffer and then finally, in order to basically get those really fast reads across a couple columns of data where the columns are probably going to be like, app name, timestamp, user ID and then also like the actual message content, it probably makes sense that we want something like column oriented storage and so to me at least, I think that hbase is a really good solution here in the sense that it's also built on top of Hadoop, so we have a ton of possible storage. We're going to get replication and sharding kind of built in for us and yeah I just think it's a really good solution for this. It would be fine if you said something like Cassandra or Riak or whatever but I think that uh kind of the single leader replication aspect of this makes it good such that you're not getting any stale reads and ultimately I think HBase is a good solution for the notifications database. Finally the one last thing to kind of cover is the actual topics database because um you know that's something that we're going to update frequently and then those updates are going to have to be passed over to zookeeper to be kind of dealing with how the consistent hashing is going to work in this system and I think as far as that database goes, MySQL should just be fine so that you can kind of have this strong consistency when reading from the leader and that way you don't have any users getting notifications that they don't want or not getting notifications that they do want. Um so I think the strong consistency is important there. Anyways, I've rambled on for a little bit now let's go ahead and take a look at a diagram so I can go ahead and visualize this for all of you.
	- Diagram
		- Okay so firstly, we are going to have our client which can go ahead and hit a load balancer and then from the load balancer, we can either reach out to the topic server which will allow us to kind of modify the metadata for a given topic in terms of which user IDs it corresponds to, and then furthermore, in addition to this whole topic service, we have our actual publisher service which is where the majority of our work is going to be going on. The publisher service is going to allow a client to send a message for a given topic which will then go into one of the topic log based message brokers which can be something like Kafka. Again, I've gone over the reason that I want to use "log based in-memory message brokers?" which mainly has to do with kind of replayability of messages and then from there, we can fan out all of those messages to a bunch of different Flink consumers or truthfully I don't really even know that state is very necessary here unless we're dealing with kind of idempotents and holding that idempotency key but um yeah ultimately kind of the way that these messages are going to be load balanced or basically the way these consumers are going to be sharded is based on kind of the Zookeeper configuration that ultimately is decided and the consistent hashing ring that's ultimately going to be generated from the user IDs available. So now we have all of these Flink consumers basically pulling in messages from all of these different topic queues and then from there, they're going to in turn be uploading messages to a second set of queues which are sharded based on user ID. Now we have all of these ranges of user IDs per queue and we can have sufficient cues such that we can actually handle the scale of our problem, and then finally for each one of those queues, we can basically have another consumer which of course we can replicate in order to make it fault tolerant, or you know just have kind of a backup which will be there to take over if it were to ever go down and the second consumer is basically just going to go ahead and take in that message and either forward it to an additional notification kind of building block such as an email or an SMS service, it's either going to provide a real-time message or notification to the client based on something like a server sent event or a WebSocket or finally, it can go ahead and upload that message to a persistent database in HBase for a notification service such that the client can always pull the database later if it was offline during this entire time and it can basically just say, you know, give me all the messages from a given timestamp onwards and that should be really easy for HBase to do because it has that concept of a partition key which can be done based on user ID and then a sort key which can be done based on the timestamp of the actual notification. Hope this makes sense guys and uh as always let me know if you have any questions in the comments. 
		- Has three or four more videos in mind and might make an hour long system design video covering all the subjects.