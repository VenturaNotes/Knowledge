---
Source:
  - https://www.youtube.com/watch?v=NF-Mw-V-yfM
Reviewed: false
---
- ![[Screenshot 2024-11-26 at 2.25.30 AM.png]]
	- Introduction
		- Today we're going to be doing a systems design of twitch or any other sort of live [[streaming service]] like that and this isn't an overly complicated design but there are some interesting nuances that we'll get to talk about later in the video. So with all of these things in mind, let's go ahead and jump into the design, I don't like wasting your guys time. I know it's precious. Let's do it. 
	- Twitch (Functional Requirements)
		- Description
			- Stream live video to global audiences
			- No buffering or jitter during bad connections
		- In terms of functional requirements for our service, let's just keep these nice and short. We want to be able to broadcast live video to users around the globe. I might be able to stream from anywhere including my mom's basement but at the same time, I want people from literally any corner of the world to be able to see it in relatively low latency. Next, we also want to be able to maintain consistent streams in the event of network issues. This means that if for some reason i'm streaming from my phone and i'm switching internet connections from something like my 5g to wi-fi or maybe if you're sitting at home watching my stream and all of a sudden your internet just gets a little bit slower because your brother just hopped on YouTube, so with all of these things considered, we want the stream to be able to be relatively low in terms of jitters and stay as consistent as possible. Let's get into our capacity estimates next.
	- Twitch (Capacity Estimates)
		- Description
			- 100 million total daily active users
			- Up to 1 million concurrent viewers per stream
		- Um in terms of estimating the capacity of a service like this, let's imagine that maybe we have a hundred million users and that for any given stream, we might even have up to a million users watching, so there's going to be a ton of load on our databases if we're not using something like a caching solution so obviously we're going to have to do something like that and we'll talk about that later. And then in addition to having these you know globally distributed caches that we're going to need all over the world, another thing that we can think about is the fact that a lot of these live streaming services will actually record videos that were live streamed and put them up for later playback and if this is the case, there's going to be probably petabytes maybe even exabytes of footage that you have to store in the cloud and that'll probably go to something like s3 but again we can talk about that later. S3 just tends to be a really good solution for static media considering how cheap it is and how basically infinitely scalable it is. Okay, let's go towards our API design. 
	- Twitch (API Design)
		- Description
			- start_stream(userId, name)
			- watch_stream(streamId)
		- For our API spec, let's keep this one simple as well. Basically i'm just going to have two end points, the first one is to start a stream if you're a given user trying to you know perform a live stream and to pass that in, you can put in a user id or some sort of authentication token and then provide everyone with a stream name. I'm sure there would be a lot more parameters in a real-life service but these two are going to be sufficient for our extremely basic streaming service. The second one would be if i'm a viewer and i want to look at someone else's stream, I would just go ahead and provide a stream id to a get [[endpoint]]. We can call it something like watch stream and then i'll start actually receiving basically the video footage that i want. 
	- Twitch (Database Schema)
		- Description
			- Users(id, email, `pw_hash`)
			- VideoMetadata(id, s3_url, streamerId)
		- Okay, moving on to our database schema. Although in terms of a streaming service, there's nothing inherently that we have to actually store in some sort of database or long-term persistent storage for the sake of adding more features down the line, something like a chat or just having those recorded videos, we probably want some tables which i guess at the bare minimum, we probably at least want something like a users table with a user ID, you know password hash, email, things like that and then we also maybe want something like a recording metadata table which keeps track of something like S3 urls for a given recording or you know if that's split into chunks, then basically all of those chunks and then the chunks each would have an S3 URL, but yeah something to just keep track of where our data is actually stored. 
- ![[Screenshot 2024-11-26 at 2.42.35 AM.png]]
	- Twitch (Architectural Overview)
		- Okay now we can actually go and start getting into this algorithm. So the first thing i want us to think about is if you haven't watched the video chat video which was the the prior one to to this video on the channel which was about zoom, then i'll give you a quick background but basically the point is in a video chat, you want clients to be able to communicate directly with one another and that's because everyone is sending and receiving video. In the case of a live stream on the other hand, it's only just one person sending video and everyone else is just ingesting that. So it gives us a little bit more capabilities in terms of the fact that we have a little bit more wiggle room in terms of the stream buffering and as a result of that, what it means is that we can actually go and provide a better quality stream. What i mean by this was a video chat should be sending that video footage all over UDP because it's very important that we're only getting the most up-to-date possible video frames whereas with a live stream, you know a streamer let's think twitch for example is potentially playing a game in very high definition and they want their viewers to be able to see that. So what we can actually do is instead of sending all this video footage and all these encoded frames over UPD which can drop a lot of them and then you know we're just kind of throwing them out and saying whatever, what we can do instead is send them over something based on TCP and this is where something known as RTMP comes in or the real-time messaging protocol which is very widely used in streaming. RTMP is based upon TCP and all it is is basically saying, hey if you're trying to live stream a bunch of video footage, go and send it over a persistent TCP connection to some central server. That's really all it is. So it's TCP which means that obviously the network data is going to be sent a little bit slower than it would have been otherwise but at the same time, you can pretty much assume that even though there's going to be some sort of initial delay between when you send that video frame and when the viewer receives it, that delay is going to be consistent so you can actually use something like TCP and as a result of having this delay, it also means that if any frames are to be dropped there is a little bit of buffer time to go ahead and resend those so it should be able to be the case that you could have higher quality video. So what do we actually do if the streamer's connection is not that great though and you know either you're dropping a lot of frames or they just don't have the bandwidth to send all this high quality footage. Well then you know if say you're on a mobile phone and you're trying to live stream but now you're in an area with a crappy connection, you can actually lower the resolution of the footage that the streamer is sending out and by doing all sorts of client optimizations, you can at least attempt to ensure that the viewer is getting a relatively consistent experience on their side without having a bunch of jitters, drop frames, or buffering or anything like that. So we know that the streamer is going to now be sending all this footage over RTMP and the question of this is where do they send it to? Well we know that a streamer might you know at any point drop a connection and additionally they're probably going to be tons of streamers with tons of video footage going out there, so not all the streamers can be sending their footage to the same central server. That would overload things and it would cause a very bad experience, so we're going to want to horizontally shard the servers on our end that are at least going to be handling this RTMP data. Well obviously that provides us with an opportunity to use something like consistent hashing and we can use consistent hashing on a stream id that is provided to a given streamer once they start a stream such that they're making sure that all of that video footage is being sent to the same central server in our back-end basically and what our central server is going to do is it's going to take this RTMP footage and it's going to start encoding it in multiple different formats and bitrates. Why is this important? Well once we have multiple formats of this given video, what it means is that we're able to go ahead and give the proper bitrate footage to a client with varying internet speeds. So if I have a really good internet speed at home, I can watch the high bit rate version of the footage. On the other hand, if i'm on a bus on my cell phone and the connection's not great, I can watch a version of the stream with a lower bit rate which is really great. So we understand now how we might actually be ingesting footage into our encoding servers but how can we actually go and pass that out to all of our viewers. Well it wouldn't be ideal if the server with you know all this load on it already doing a ton of encoding every single second is directly passing off all that footage to all of our viewers. Instead, what we want is a lot of this functionality to be cached because we can not only basically cache the footage itself which is going to be split into you know very small segments say about a second each, but we can actually cache the metadata that basically tells a client, uh you know, for this segment, here's kind of like the link to it or here's the location in a cache. 
		- (1) So basically what we have is that once the [[encoder]] starts you know creating this video footage because it's compiling all the [[RTMP]] frames, it's going to split the video footage into one second segments, and it's going to update something known as a manifest. Now a manifest is specifically part of kind of a protocol known as [[MPEG-DASH]] which is how a lot of these live stream services tend to send out their videos but you know just any sort of general kind of type of metadata where you're saying okay here is the chunk link for this segment of footage which is one second long and here's the chunk link for this segment of footage which is also one second long. So why is it really useful to kind of have a list of the metadata like this where we're keeping track of the link for each segment. Well this is really useful in case you have to actually kind of adapt the bit rate of your stream on the client because if that's the case, the client can basically use this metadata log kind of almost like a log based message broker where you keep track of the last segment that you've seen on the client (Manifest is periodically updated and pushed to cache!) and then you say oh shoot you know what i have to go to a different bit right now. Uh where is this located and then even though you're fetching a different bit rate, you can ensure that you're still fetching the proper one in terms of like the second synchronization so your footage is never going to jump around, right? If we didn't have this kind of table of contents, I might you know get my stream at one bitrate and then switch to another bitrate and now i'm five seconds delayed so it's really useful to kind of split the stream into segments like this and then go ahead and load those one at a time and kind of, that allows us to do this whole idea of adaptive bitrate. 
		- So I mentioned already that a lot of this is going to be in a cache, and what that means is that it can greatly reduce the load on our encoding servers because it means that the encoding server in theory should only have to actually kind of generate the data one time to give to a client and then after that, our cache is going to be holding it and that's going to kind of be shielding our central servers from having to answer any more requests. Now this is great, it means that the first user that basically wants a chunk of a stream is going to load it into the cache and from then on, we're all good. Everyone else is going to be hitting the cache. However, it's not actually that simple. Why is it not that simple? Well because we've opened ourselves up to a race condition. What if you know say like some huge celebrity like i don't know like drake does it live stream which he'll do like every once in a while and then all of a sudden, we now have you know a hundred thousand people who instantly opened up that stream and are all trying to hit the cache at once but the cache hasn't loaded anything yet and so now all these hundred thousand requests are being forwarded directly to our central server. This is known as a thundering herd problem and you know, you may have heard that term thundering heard before. The thundering herd is also very relevant when a bunch of people are trying to hit a non-populated cache. So what can we actually do? Well one solution that i've seen in the past is you set something known as a cache lock or cache block that basically says only the first person to hit that cache is going to be allowed to continue on to our actual server and the rest of those processes are basically going to be locked for some timeout and obviously you're going to have to configure this timeout such that the central server has enough time to send back the footage because if you don't, then the thundering herd's just going to come in right after that, and so there is a little bit of work in terms of kind of configuring the timeout but ultimately by using this kind of blocking mechanism, we're able to generally get rid of the scenario where you know many many thousands of requests are reaching the server at the same time as that would completely overload it and then our stream might go down. So that's kind of dealing with the thundering herd problem um, and that kind of just encapsulates right there how we would actually deal with um getting good latency on receiving the stream from the viewer's end. Obviously we want these caches to be distributed all over the globe. Normally they're distributed in things called point of presences which are effectively the same as CDNs. It's just a distributed content network full of caches that you can use to quickly access your content and then yeah, that's kind of the streaming side of things. I'll briefly touch upon the actual recording and the messaging side of things but they're probably not very central to this video and also i've touched upon those in more depth in other videos. For example for messaging, I recommend just watching the messenger video but the point is, as far as recording goes, from the encoding server since it's splitting things into pretty small chunks in about a second, you can probably just have another process running that directly uploads that segment to S3 and then maybe changes a row in a metadata database table. I suppose you could also put it in some sort of queue and then a different process could go ahead and upload that to S3 if you really want to reduce the load on the encoder, but overall, it's probably not a huge deal either way. Similarly, as far as kind of maintaining all these real-time messages go, because there can be a ton of people in a given stream, as opposed to just having like solely WebSockets be used, a good method of doing things is for a given stream ID, you can have a bunch of like log based message queues basically corresponding to that stream ID and then every single message is going to be published to one of those queues where at which point someone is going to subscribe to those queues. Someone being you know some of our chat servers on the back end, and then the chat servers themselves can connect to clients via WebSocket and you know send over those messages to them in real time, but yeah that's kind of the general background scheme to it. Like i said for this question, not overly important because if you're asking someone for a live streaming service, you probably care more about how the actual live stream is getting done and that is more so to do with RTMP and a whole lot of caching. So anyways, with that out of the way, I hope it made sense. Um, I didn't really show an image this time around because i didn't really feel like there was you know much visual to envision here but i am going to show a diagram right now to hopefully help you guys get your head around everything.
	- Diagram
		- Alrighty, let's take a look at this here diagram of twitch design. So let's imagine i am a streamer. So the streamer is going to be uploading some frames through RTMP to one of our encoders. Now you see i'm only connecting to one encoder and that's because we're using consistent hashing. This can be done by something like a gossip protocol where you're forwarding the request or perhaps we could have a load balancer that is kind of forwarding everything as well. The reason we're doing kind of this partitioning on a stream ID as opposed to an IP address has to do a lot with what i touched upon last episode about that whole concept of NAT where NAT basically means that we don't really have access easily to the source IP of our streamer. We really only have access to kind of the router that they're connected to and as a result of that, if they're switching network sources while they're streaming, it could result in them going to a different encoding server which would be problematic for creating these segments. We want all of the frames to be going to the same encoding server. So anyway, once we reach one of these encoding servers, we can go ahead and start creating both that manifest file which is effectively just a table of contents with metadata saying, for this second, here's the link to the segment file and then additionally, that encoder is going to start creating a bunch of different various bit rates of the stream to send over to a viewer. On the other side of things, once the encoder wants to send data to the viewer, first it's going to populate that segment into [[S3]] and possibly into a database row to go ahead and deal with the metadata of things and then we have this cache. I probably could have drawn a little bit more here to reflect the fact that this cache should be distributed all over the world because viewers are going to be pulling from different caches but the general point is this. Our viewers 99.999999% of the time should be pulling the frames from the cache. If they're not, only one viewer should be getting the original uh call from the server and that should instantly be populating the cache. Otherwise, we're going to have ourselves a thundering herd problem and to mitigate that we can use that sort of cache locking. So anyways like i said the, cache is basically there to shield all of our encoders from the tons and tons of traffic that they might have if there were no caches around. So caching and CDNs are hugely important for great performance when it comes to live streaming as long as you're not the first viewer, you're probably going to be hitting a cache for pretty much every segment which is really good 
		- Anyways yeah, like i said, not too complicated of a design. This is kind of live streaming encapsulated. I don't think interviewers really ask about this too often but um i guess it makes sense. It's just another one of those kind of TCP versus UDP discussions and also it's kind of nice to talk about [[distributed caching]] a little bit with a little bit of race conditioning magic with the whole thundering herd thing. Okay guys, I hope you really enjoyed this video. Um again, I'm truly honored that so many of you have chosen to subscribe to this channel.