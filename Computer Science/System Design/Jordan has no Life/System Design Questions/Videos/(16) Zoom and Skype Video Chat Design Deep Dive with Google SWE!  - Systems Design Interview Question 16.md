---
Source:
  - https://www.youtube.com/watch?v=Pkl9RHq0yDk
Reviewed: false
---
- ![[Screenshot 2024-11-25 at 10.14.00 PM.png]]
	- Introduction
		- Now I'm gonna go ahead and start talking systems design because that's what we're all here for. So today we're going to be talking about building a video chatting service like Zoom or Skype. Truthfully um if you're an interviewer and you ask a question like this, you're kind of bad just because like unless you specifically researched how to make a video chatting service, I just don't see why you would know the things that i'm going to be talking about in this video and so there are kind of aspects of it where, you know, having some systems design background is going to help but for the most part, like we're talking about knowing a specific technology here and i'll get through that but i don't know just like, why would you ask this question. I'm still gonna do it, but, whatever let's get into it. 
	- Zoom/Skype (Functional Requirements)
		- Description
			- Realtime video and audio chat over the internet
			- Group chats
			- Server side chat recording
		- Okay so let's talk about some functional requirements of our service. Obviously what we want is real-time video and audio communication between multiple parties over the internet um and we want that to be in as low latency as possible and additionally we're going to be allowing group chats and i'll discuss in our capacity estimates section how big those group chats can get but just for the sake of adding an additional requirement to our service just so that our diagram isn't super basic, let's imagine that we're also going to be recording these conversations and as opposed to zoom which i think just typically records on something like one client device, let's imagine that all of the video chat recording is going to be done server side and then will be accessible to all the members of the call after the fact, so that way we can at least complicate things a little bit. All right let's move on to our capacity estimations,  we can get a sense of the scale with which we're going to be working.
	- Zoom/Skype (Capacity Estimates)
		- Description
			- 1 billion users
			- Up to 100 people per video chat
			- 100 mb $*$ 100,000 = `10tb` of video per day
		- Okay, when it comes to estimating the size of the service that we're going to be building, let's imagine that we have about a billion users, we can have group calls say up to 100 people, and then additionally let's assume if we're going to have a recording maybe each recording is going to be like 100 megabytes and we're making say i don't know 100,000 of those a day or something so obviously a lot of data is going to be stored there but generally when we're working with static recordings like that, we're just going to be throwing those in S3 anyway, so we're going to be dealing with a lot of data but for the most part we're not going to be storing most of it, so our main issue is going to be in terms of how can we deal with low latency on our video calls and less about how can we properly kind of replicate and partition our storage such that that is going to be efficient. So we'll talk about that in a little bit 
	- Zoom/Skype (API Design)
		- Description
			- `InitiateCall(userIDList)`
		- Okay so in terms of our API spec or the schema that we want to be implementing, there's really only one api that is going to kind of define our service other than you know kind of creating and fetching recordings and that's going to be to actually initiate a call. After the call initiation itself is done, we're generally going to be using a lot of non-just http endpoints and communicating in a different manner of the network, so really we just want some sort of endpoint to initiate a call between a list of user ids. Another option would be to something just like join a room and then you know part of that is actually initiating the call itself but that's kind of the only functionality i can think of that should really be abstracted away to an http endpoint. 
	- Zoom/Skype (Database Schema)
		- Description
			- Users(id, email, `pw_hash`)
			- Recordings(id, s3_url) - could be split to chunks
			- RecordingAccess(userId, recordingId)
		- As far as databases go, even though we're not going to be persisting too much information, we are obviously going to need a table for our users so that users can go ahead and add one another or you know we just want to be keeping their information in general if it's something like a paid service and then perhaps a recordings table as well and more likely to be a recording metadata table and then that's where we can perhaps have like a user recording table between them to do a join in order to get the proper basically video permissions for a given recording that we have. And this can all be done in SQL because at the end of the day, the main kind of case that we're trying to optimize on here is the actual transmission of video and audio footage, more so than it is you know just making really fast reads and writes to our databases. So i think you know single leader replication MySQL is totally fine for this. It shouldn't really be a huge deal if we use that. Let's actually start talking about how we can get some really fast video calls going.
- ![[Screenshot 2024-11-25 at 10.31.57 PM.png]]
	- Zoom/Skype (Architectural Overview)
		- Okay so starting to actually dive into our design, let's first begin by thinking about what happens when two users or possibly even more than two users want to initiate a video call with one another. Basically the initiation can happen in a couple of ways, if you're thinking about zoom what'll typically happen is one user is going to create a chat room and then another user will go ahead and join it and there's kind of going to be some functionality that'll happen once that second user joins or another possible option is i guess this happens more so in like Skype or FaceTime or something where you directly dial another user and when that happens what you probably need is some sort of um kind of real-time server-to-client events framework like either a WebSocket or a server sent event to alert the person on the receiving end of the call that they're actually being notified and then a WebSocket would probably be best here because then you know if a person is to accept or reject a call, they're going to have to alert the server again so that the original server can go ahead and alert the person who initiated the call of the result of whether or not the second person accepted or rejected. So it seems like you know kind of this intermediary server with a WebSocket is going to be inevitable at some point at least to establish a call in a lot of cases. So let's think about the actual kind of way that we want to be sending our video and audio data itself. In an ideal world, what we basically want to ensure maximum efficiency of our calls is for two clients to be able to directly you know send data over the network to one another, right? It would add a lot of latency if we had to put some intermediary centralized server in between the two of them because then we're sending data over the network to some random place for that only to then just be forwarded to the other end user. So that's what we would be doing in an ideal world, but the truth of the matter is, there are actually a lot of challenges to this. So let's start going over what some of those are. For starters, they have to actually know one another's ip address and this is actually a lot tougher than it seems. In the current state of the internet, a lot of devices basically have an ip address which is the equivalent of just like an address over the internet using the IPV4 protocol and what that means is in IPV4, devices tend to not expose their public address over the network but rather they're connected to some sort of other router through something like their internet service provider and that's more so what's exposed and then when data gets sent back to the internet service provider, the ISP uses something called a NAT or a network address translator to take that data from their kind of main router and then go ahead and send it to your end device. So ultimately it's not so easy to just get the actual addresses of these two devices so that they can communicate with one another directly. What you instead need is to use some sort of intermediary server and what both of these clients will do is they'll reach out to the intermediary server which is often called a stun server and after reaching out to the stun server, they can both get their own kind of public addresses and then in turn, use the stun server to pass them to one another. So now, once both of these clients are informed of their own addresses, they should in theory at least be able to communicate directly with one another. However, even in reality, it's not so easy. Why is that? Well a lot of these kind of ISPS or kind of just network protocols in general will go ahead and establish some sort of firewall so that these two devices can't directly communicate with one another. They want to make it so that you can really only be getting network connections funneled through that main router and so ultimately, what ends up happening is they don't want this direct client to client communication but they try to ensure that every single bit of network data coming to your device is going through a server first to make sure that you're kind of more protected, less vulnerable to hackers or things like that. So in addition to a stun server, something that's come up is called a turn server as well, and the turn server basically acts as a relay of your video data where you're basically sending your data stream over to that turn server and the turn server is now going to be forwarding it to the client. This is all just to get around this firewall, now it's not always the case that a turn server is needed but i've seen some statistics like around 10 to 20 percent of all VOIP calls, so all video calls over the internet are going to require a turn server and you can't just create zoom and have 10% of your users not be able to call one another. So it's very important that you have these relaying servers in this case where clients can't directly communicate. Actually now that we've talked about kind of the way that clients can speak to one another, what type of network data should they actually be sending? Well if you watched my stock trading video, hopefully that was a good refresher on TCP versus UDP, but if you didn't, I'll quickly mention it again. Basically the point of TCP and the reason that it's so popularly used is that every single message that's being sent over the network, every single packet, has a sequence number and what the sequence number allows us to do is ensure that packets are not going to be dropped. If one client is missing a packet with a given sequence number, it realizes that that is missing and it can go ahead and re-request that and get it retransmitted. Additionally, TCP has other sorts of precautionary measures to ensure that you're basically not over congesting the network using something known as flow control. On the other hand, UDP has basically none of this and UDP is basically just taking like a machine gun and spraying all your bullets of network packets over the network. The reason however why even though UDP is less used in practice is really good for video is because if i'm going to drop a few frames in a video call, I'm not going to ask for those to be retransmitted. I don't care. Those are in the past now, you know if i happen to lag for a second in this video and you just missed what i said, you're not going to want that video frame five seconds later. It's no good for you anymore, you're just gonna say hey what happened and then they'll try and say it again and hopefully it gets over the network. So that's why it happens to be the case that UDP is often the one used in video chatting because due to the fact that there are no things like sequence numbers, flow control or, retransmitting, UDP can often be a little bit faster and have less latency and so we'll see it in very specific cases where super low latency networking is needed like this case and also we saw that in our stock exchange as well. So again, we basically know now how we can have two clients communicating with each other and also the type of data we're going to be sending whether that's from client to client or ultimately whether it has to be relayed by a turn server and kind of the process that i've described all the way up to this point is known as something called WebRTC. That's kind of encapsulating the idea of that stun and that turn server and it's very nice to have and it basically gets a lot of this process done for you. However, one of the issues with what i've mentioned up to this point is that even though this works very very well when we have just two clients, the second we have say you know five ten, fifty, a hundred clients working together in a video call, then we're going to start running into some issues and why is that? Well we've said that every single client would be communicating with every single other client sending their entire UDP stream of their video footage and also receiving streams from everyone else, so there's a ton of bandwidth that's going to be used in the sense that for end clients, there's going to be n squared connections Basically, you know, we're all going to be communicating with one another and it's going to form this very complex looking web of data being sent over the internet. So what are a few options that we actually have in order to kind of reduce the bandwidth on our clients and making it a better experience for all of them. Well generally speaking, what we're going to have to do is actually incorporate some sort of centralized server. So let's think about how we can do that. Well the first option which is kind of a naive solution is we just throw in a central server. Every single one of the clients is going to upload their video footage to that central server as it's you know basically being recorded, and then the central server is going to combine it all into one single stream. Now there are a couple issues with this even though it does have some huge pros. The pros are, if all of the clients are just sending one video stream to the central server and they're just receiving one video stream from the central server because the central server is basically compiling them all into this one view, it means that each client is going to be using very little bandwidth and in theory, the outputting kind of video call is going to be very efficient. However, it comes with the cost of having very little flexibility. For example, if the central server is you know making a layout where everybody is equally sized in that outgoing video stream, but you want to kind of pin one user, you're not going to be able to do that. You're only getting one incoming video stream and as a result of that, you have no flexibility there. So what is another option that we can take? Well perhaps what if we simply just have every single client sending their outgoing video stream to the centralized server and the centralized server is still sending every single basically incoming video stream back to you? So it is still reducing the total bandwidth that you have to deal with in the sense that you're sending your own video stream to fewer places but you're still receiving just as many. So in theory, there can be a lot of load on your individual client just by virtue of having to take in all of these incoming streams. So what if we could find some sort of middle ground where you know we're able to reduce kind of that incoming bandwidth but our outgoing bandwidth still isn't terrible as it is you know if we're just sending our stream to every single other possible client. This is where another option called i believe simulcast comes in and basically what you do is on my client, i'll send a few different versions of my own stream One in higher resolution and one in much lower resolution which is probably going to be good if you want like a little mini thumbnail stream of someone else and then the reason this is super useful is that now if i'm sending both of these two streams to the central server, what the central server can now do is it will only send you back one hd version of someone else's stream and everyone else is going to be thumbnails so it should not only decrease your kind of incoming bandwidth, but compared to the all clients communicate directly with one another method, it's going to be decreasing your outgoing bandwidth by a lot as well, so this is super useful. It's a great compromise, and it's great for something like zoom or google meet or anything like that where generally you're only going to see one person in the bigger picture, that's going to be the active speaker, and everyone else, you kind of see this little thumbnail of them you know like looking at their phone or doing whatever it is but they're not speaking so who cares, you don't need them in high definition. So this is a great optimization to actually use in practice. Another thing to note is that there is going to be a lot of load on the central server. For starters, there's going to have to be a lot of decoding done by the central server because certain devices are going to send video formats in different codecs or resolutions or things like that and the central server may have to kind of convert this to a different format. It may even use kind of other central servers that it kind of passes off that information to, to do the decoding and then pass it to the proper devices. So there's a lot of room to kind of make this process more efficient in terms of distributing that computing. Additionally, just by virtue of you know all the clients potentially being in different areas around the globe, there's room to kind of geographically distribute your centralized server and that way this media server is able to kind of give users that are close to one another potentially better latency. I guess another in theory possible optimization that you could make is you could maybe have clients send out their media to say three centralized servers and that way the incoming data you're just going to only be pulling from the one that is geographically closest to you or something like that. And then furthermore, by you know sending data to multiple central servers or just having you know multiple central servers on standby, you know that if that central server which is a single point of failure were to go down, all the clients can then start sending their video streams to a different server and you know maybe you could use something like a coordination service or some sort of consensus algorithm to perform that failover if that were to actually happen. 
		- Now the final piece of this puzzle that i have mentioned is that we are going to be able to record footage and this is really nice when we have a centralized server because it's getting all those video streams coming to it and as a result of that, it can compile some sort of video recording and ultimately leave that available in the cloud for users to watch down the line. Now because we mentioned that our central server has a lot of load on it and it wants to be doing all this decrypting in real time, it's probably better to decouple this whole recording compilation process from our central server. So instead maybe that server should be uploading all of these frames of video and audio to something like a message queue and simultaneously uploading it to some sort of elastic file storage like amazon s3 where it's cheap and you can basically store as much as you need to and then down the line, basically certain stateful consumers can pull from the correct partitions of these message queues in order to recompile those recordings at the proper resolutions that they need to be and then upload the completed recording back to something like s3 which i'll show off in my diagram right after this. Again, I hope this all made sense to you guys but i'll make sure to kind of double down on it in the diagram. I hope it also makes sense why i don't think this is a great interview question, it requires knowing a lot about networking that truthfully i don't think the average software engineer necessarily needs to know for their day-to-day and even though i personally think it's cool, it seems like it's a lot extra compared to i don't know certain distributed systems knowledge that we may have. So anyways, let's check out this diagram and get this video done. 
	- Diagram
		- All right so, let's take a look at this diagram. I've kind of drawn out both of the possible ways of actually communicating with one another whether that's directly from client to client or through some sort of centralized turn server or just media server that a company like zoom might run. So as you can see i've got client one and client two written down and first off assuming they're not using some sort of centralized video server, they can use our stun server to get each other's you know public ip addresses and then communicate with one another directly. That's kind of solution (A) which is very simple and we don't have to talk about that too much, so let's talk about the more complicated solution of having these huge group calls where every single one of the video streams is going through that central video server. So in this case, client one and client two are probably going to be sending multiple copies of outgoing video streams where they're either at different resolutions or maybe more compressed than you know a different resolution or something like that, such that one is going to require less load on the network to actually transfer and then from that central video server, they should be receiving multiple UDP streams so you see i have only one arrow here but the reality of it is they're going to be receiving separate UDP streams for all of the other clients even though there are only two here, if there were say 10 clients, you would expect about like nine other incoming arrows but the truth of the matter is, out of those nine arrows, eight of them would be pretty low definition and as a result it wouldn't be too hard to load them, so there are a lot of client optimizations that are going to be going on in order to kind of reduce the bandwidth that a given client is experiencing and you know that's kind of how we can really optimize all this is just making sure that we're only loading the video feeds that we need within a given moment. So anyways, like i said that central video server is going to be doing a lot of potentially decoding and re-encoding to different formats and then in addition to that, if recording is a feasible thing, it's probably going to be taking a lot of these frames, placing them in s3, possibly also keeping track of them in some sort of MySQL metadata database and then after that, putting an entry in Kafka which is going to be partitioned in a way such that all of basically the frames from the same call based on something like a call ID can be put in the same queue where ultimately the same stateful stream processing consumer i've chosen Flink here but it could be something like spark streaming are going to recompile all of that video footage from the recording and then place it back in s3 for later access. I did definitely skip i guess a couple steps of this diagram as far as the database part of it, but i hope at least my verbal description makes it make a little bit more sense. 
		- I'm kind of in a bad position right now where the the new magic marker i got is a little bit harder to write with and so it's hard for me to express as much detail so i got to get on top of that but yeah, as you can see i hope you guys get the point. Well anyways, I hope everybody enjoyed watching, um, for those of you who are new here, welcome to the channel. Uh it's a good time, you know, I'm gonna get myself fired eventually or you know canceled or something like that but until then, we're going strong