---
Source:
  - https://www.youtube.com/watch?v=eUvwRAxmEgY
Reviewed: false
---
- ![[Screenshot 2024-11-22 at 7.27.03 PM.png]]
	- Introduction
		- hello giga chads and welcome back to the channel. As you can see today, I'm sporting my lovely Miami tank top to represent one of my favorite YouTubers, Dom Mazzetti, aka BroScienceLife but uh anyways, today we're going to be doing yet another distributed systems systems design interview question on this channel. Just a quick couple of announcements. I am getting really damn close to a thousand subscribers so i'd like to thank all of you for that because uh you know i need the ego boost. I'm a very insecure person and this keeps me going. Additionally, uh i recently saw someone post another day in the life software engineer video and um basically from that alone got more subscribers than i have after me posting 50 plus videos at this point and i'm now depressed but for the time being, i'm still not going to make one of those because i think it's cringy and weird but you know if i ever really get desperate for subscribers, i'll do it. You know, if i ever end up applying to graduate school, it's happening because then i will be able to put it on my resume. But nonetheless, let's actually go ahead and talk about building a distributed search engine. I think that "grokking the systems design interview" does honestly a really bad job with this question in terms of the depth that they go into for it and frankly i feel like i blow them out of the water. So as a result of that, I hope to actually get into this problem. I'm not going to go as in depth as I did in my actual dedicated like search index concepts video so i recommend that you check that out if you have time. That's i think episode 27 of my systems design concept series but i am going to definitely bring up a lot of the content from that video as a refresher so hopefully it serves as being useful. Anyways, let's talk about ElasticSearch, Twitter search. How we might implement something like that and we can go ahead and do our normal steps. 
	- Twitter Search (Functional Requirements)
		- Description
			- Functional Requirements
				- Query plaintext documents based on (a combination of) search terms that appear in them
		- Okay, let's talk about some functional requirements. So when you're building a search index, the reason you're doing so generally speaking is because you have a bunch of plain text documents and you want to be able to query across all of these documents based on certain search terms. So for example, you know, if it is the presidential election and I want to go ahead and search by small hands, I want to see all of the tweets of people who have gone and used that term specifically. Obviously as we're in a distributed setting, it's very important that we want to be able to have super high reliability and super low latency when finding all of these documents as we're going to be dealing with an absolute ton of users and it's going to be very important for our service. So we'll get a sense of, you know, the scale with which we need to go ahead and build out our search index and we can do that more so in the next section where we're going to estimate the capacity of what we're going to need to handle. 
	- Twitter Search (Capacity Estimates)
		- Description
			- 400 million tweets per day
			- 300 bytes per tweet
			- 120gb of tweets per day
			- 730 billion tweets per 5 years, 200 Tb / 5 yr
			- 40 bits = 5 bytes per tweet ID
			- 15 search terms per tweet
			- 55TB of storage in index / 5 years
		- All right, welcome back. You may have notice i changed my shirt because i felt like a douche, but anyways let's do some capacity estimates. Uh for starters, we can basically go ahead and assume they're going to be 400 million tweets per day. I'm just stealing these numbers from grokking even though they do suck. Um 300 bytes per tweet because about 140 characters per tweet which means there are about 120 gigabytes of tweets per day which is quite a bit. In addition, that means over five years because we're now multiplying by 365 and then by five for the five years, we have 730 billion tweets and also 200 terabytes worth of tweets, wow. Then finally, if we're going to basically say that we need five bytes to identify a tweet, the reason for that being that 5 bytes is 40 bits and if you do 2 to the 40th you can basically get a bigger number than 730 billion so we can use 40 bits to basically uniquely identify every single tweet and then if we assume that there are basically 500,000 english terms that we have to index where each of those is corresponding to a list of document ids which we just said are going to be five bytes each, and then if each tweet ID is five bytes and corresponds to 15 search terms, now we can basically go ahead and say there's 730 billion tweets where each tweet ID is five bytes long, corresponds to 15 search terms in the database which means it's going to appear 15 separate times throughout our index, that leads us to have about 55 terabytes worth of storage in our index. What that means is we're definitely going to need sharding and if we wanted to do this in memory, we're going to need a ton of sharding so that is definitely one thing to consider right there. Additionally you may have noticed that i basically insinuated that we would have some sort of mapping from a term id to a list of document ids and that's something we'll speak about more in the next section but that's kind of the centralized idea here with the search index. And then finally, my last remark is that i just threw out a ton of numbers and i really don't want to put all of them in post-processing for this video so i may skip on some of those, sorry. 
	- Twitter Search (API Design)
		- Description
			- API Endpoints
				- search(term, sort_order, page_num, page_size)
		- Our api design is going to be pretty tiny um and we basically are just going to have one get endpoint which is going to be, we can just call it search, where we have some term or i don't know some combination of terms. A sort order, a page number and a page size and we can talk about, you know, some possibilities later for how we might sort our data, possibly by giving it an arbitrary scoring or maybe just sorting by document id or the actual terms themselves but um yeah. Typically this is a pretty simple just search API here and we're just going to assume that's the only functionality we're implementing and then in the next section, I will generally do an overview of how we might build a distributed search index. 
	- Twitter Search (Architectural Overview)
		- All right, so, normally in this part of the video, I would do something like a database schema but it's actually the case that we're building a search index here which means there is no database. So let's talk about what a search index might actually look like. First what it might look like on a single node and then also at scale because obviously like i mentioned we have so much data here and so many users that we're having to do this over multiple nodes in a cluster and as a result of that, there are more considerations to think about. So like I said, there's no database schema. What we're actually going to be using instead as opposed to a traditional index where documents are indexed by their primary key or some sort of ID is we're using something called an inverted index. Now the inverted index is basically the central part of why search indexes actually work so well. Fun fact, I actually broke an interview because i didn't know this and i feel dumb as a result of it. Basically, it's going to have something like a HashMap where the key is going to be a term, so for example Donald Trump and there are plenty of tweets that correspond to this and then all of the tweet ids that have that term in there and so basically now what happens is that every single time a document is uploaded into the search index, it gets parsed into its relevant terms and traditionally there's going to be some sort of extra logic here, right? Like we don't care about terms like "the", "a", "and", things like that and we also want to do some additional pre-processing on some of the terms, you know, taking out "`ings`". Taking out `'s` and the way that we parse these documents is very central to kind of what comes up when we do a search because it directly impacts for which terms, which document ids are going to come up. So this actual [[lexing]] stage is extremely important. There's a lot of thought that needs to go into how those documents are parsed. In addition to that, the terms are actually going to be sorted in the index. So for example, you know, the term aardvark is going to come first, then you'll have apple, and you'll have Bob, then you'll have Charlie and the reason for doing this is that it makes it really easy to do prefix searches. So like i've said in some of my previous videos, one of the cool things about indexes is that you can actually go ahead and binary search them. So if i have like a term that i say i want all of the documents that have like the letter like `ap` in them. So what i'm going to do is i'm going to binary search in our index between `ap` and then it goes between `ap` and `aq` and i'm going to binary search in our index. So what that's going to do is it's going to reach basically apple, apricot all of the other `ap` words and then we can go get all of the documents that have that prefix in there which is really great. Additionally, you can do a similar search with suffixes where you basically reverse those document terms and then you do that same exact prefix search. So you would store a second copy of the index where all of those search terms are reversed and that would allow you to do a binary search to search for suffixes, and so a lot of the kind of logic for building a search index is how you can turn all of these basically string manipulation problems into prefix searches so you can binary search through the index. This is um all kind of handled by this one really popular open source search indexing engine called [[Lucene]]. Now Lucene is built a while ago, but it has a bunch of super cool features. Like I mentioned, there's prefix searching, there's suffix searching, you can do [[Levenshtein distance]] which basically allows you to do these things called [[fuzzy searche|fuzzy searches]] so you can find similar words. So if i were to you know do a search but i spelled the word wrong, it would give me documents where they contained a search term that were similar to my misspelled word and then additionally, it also does some ranking based on a bunch of you know NLP algorithms. So if you've ever taken an NLP course or have any familiarity with that, it uses things like term frequency and also inverse document frequency to try and gain the general relevance of a document for a given query. I'm not going to go into those too much, but i figure these are things that you can always look into on your own if you want because they are interesting. So we understand now generally how that single node search index may work but the point is, I mentioned that it's not just one node. We have to distribute this thing over a ton of nodes because we have many many terabytes of data that have to be able to be searched. So now if you recall from again another one of my concept videos a super long time ago, whenever we're going to be building out some sort of distributed index that's like a secondary index, we always have to ask ourselves the following question. Do you want a local index or a global index and this is something that's extremely relevant here. So in our case for the search index, what a local index would be is basically saying, every single node is just going to handle a range of documents and then on that node, we're going to have every possible search term and only have the listings of the documents that are held on that node. A global index might be, say, you know, the term for Jordan is on one node and every single possible document is going to be listed on that node and then I don't know the term for uh Michelle Obama is going to be on another node and every single document that lists her is going to be on that node. Now you might say to yourself, okay well actually the global thing sounds really great here because what we want to be doing is actually optimizing for read speed and the nice thing about a global secondary index is that you can quickly grab all of those ids at once without having to rely on making multiple network connections to all of these nodes in the cluster whereas with a local index, it optimizes for writes because you can basically just quickly write the document to one node and then know that it's going to go into that local secondary index without having to do any extra network coordination like two-face commit or anything like that. However, the truth of the matter here is that a global secondary index for something at this scale is probably not feasible. The reasoning for this being that certain search terms are literally just too popular. I keep using the example of Donald Trump because there are so many tweets about them, but if you ever are going to do a search index of trump on twitter, you're going to see millions or maybe even more tweets and as a result of that, you probably can't hold all of those ids on a single server and even if you did, that server would be so hot it would be such a hot spot because so many people would be querying it that it would generally probably just go down due to the load and even though you could probably handle this to an extent with something like caching, it really doesn't help that there's going to be so much storage on the server as a result of having to hold literally tons of document ids and as a result just generally it seems with these super large search indexes that a global index doesn't make sense and as a result you kind of have to just partition it in a way that only certain documents can go on certain nodes which is unfortunate but um that is the way things go. I have seen some research papers that i've run into discussing a hybrid approach where you kind of do some sort of global partitioning, some sort of local partitioning, but at least the super big distributed search engines like ElasticSearch happen to do everything locally and so that probably seems like the best way to go here. In terms of the way that we may actually end up sharding those documents because like i mentioned, we're going to be doing this locally, we can either shard them completely randomly where we might do some sort of consistent hashing, we can shard them in a way that's based on time, temporally because if we ever just only want to search the newest posts maybe it would be best to kind of batch all the documents um by like one day at a time so that all the documents from a given day are on the same instance of basically a node in the cluster and that way we can search them all really quickly or perhaps we could somehow put related documents together with you know search terms in common on the same cluster. Obviously that would be the most ideal thing, but it's not always possible without some sort of, you know, algorithm and then we would have to keep track of the mapping of where each document is actually held in the search index. So with this in mind, now every single time that we call our API and want to get the search results, we're going to have to aggregate the results from a bunch of these individual nodes in the cluster which is unfortunate but at least on the bright side, it can be parallelized. We don't have to wait for one request and then sequentially get another request and sequentially do that. We can have an aggregator server that's going to in parallel make requests to all of these indexes and then you know aggregate the results based on some sort of sort function that the user is provided. That could be on something based on score, that could be on you know the date the post was made, the number of likes it has, anything along those lines but the point is we're just going to have to aggregate the data. The last thing that i really wanted to point out because i didn't really like the way that "grokking the systems design" talked about this or rather they didn't talk about it is that it seems that um basically whenever you're using something like a search index, what we're putting in there is derived data. Now derived data means that the search index itself is not our main data store and we shouldn't be using a search index as our main data store. The reasoning for this is that generally speaking, the documents are not going to be indexed by some sort of primary ID and as a result of that, you know, just being able to search for documents say based on a user or something, is not as good and oftentimes a lot of this stuff is in memory and so the data is not as durable and generally speaking, it's just not a good idea to use a search index as your primary data store. So what we do have is a database where tweets are going to be held. What we should be doing is instead of just every single time a tweet is uploaded, hitting our server and saying, hey send, this tweet to both the database and send it to the search index, um hopefully we realize that something like that is problematic. The reasoning being that we have then an atomic commit situation on our hands. What if the database write were to succeed and the search index one were to fail. What if the search index were to succeed and the database were to fail. These two could become out of sync and as a result, whenever we're dealing with derived data, one cool thing that we can do is actually use a queue here. So we can basically take something known as the change data from our database, so every single time the database receives a write, it can propagate that write to something like a log base message broker by passing something known as its change data. So it takes the change data, it passes that write to a log based message queue such as Kafka and then that can then go ahead and be propagated into a search index which will automatically update itself. By doing this, we can ensure that the search index is going to stay basically in sync with our database. We don't need an expensive two-phase commit protocol and additionally assuming that we have idempotent operations on our search index meaning that they only occur once if we were to upload a document to it or you know if basically having to upload a document more than once only results in it seeming like it was uploaded once, then all should be good there and we won't have any consequences if a message were to be replayed in our log-based message broker. In the actual twitter example that we built ourselves in a previous video, what we had is all of the tweets first going to a message broker before actually hitting the database because we wanted them to also be sent to all of these Redis caches representing the feeds of users, so if that were the case, we could also just pull from that message broker and send it to our search index effectively keeping everything in sync which is going to be great. What i've just described is basically what ElasticSearch does and ElasticSearch these days is effectively the leading distributed search index and kind of the one thing that i haven't really mentioned that makes ElasticSearch able to run as fast as it is, is basically the very very heavy use of a ton of caching. Obviously if we're going to be using a search index, we want the fastest possible reads and we don't really care as much about write speed. With that in mind, we want to basically be caching everything as much as possible. Of course if we have popular terms that are being frequently searched, we want something like an LRU cache to be serving the majority of our requests. Additionally, another cool thing that elasticsearch actually does is as opposed to just caching individual pages of an index because basically ElasticSearch runs using um SSTables and LSM-trees or I should say [[Lucene]] does. As opposed to just putting the entire SSTable in memory which is what most caches would do for a database, ElasticSearch actually caches the entire results of a query. So it might cache something that's even more complex than just a full SSTable and as a result of that, it's really able to speed things up by saving a ton of computation time by not just caching a page of the index but actually caching that whole query result itself. Like i said, these are all things that i touched about pretty in depth in my original search index video but i figured i would bring them back up because i really feel like "grokking the systems design interview" does not do justice to kind of the intricacies of building a distributed search index and kind of justifying the decisions of why they did what they did.
- ![[Screenshot 2024-11-22 at 7.29.16 PM.png]]
	- Twitter Search (System Diagram)
		- Okay finally as always, we're going to draw a quick little diagram. This one is going to lack some detail as to the actual upload path of tweets because we've covered that in another video which i recommend checking out. I think it's number two in this series, and uh yeah, if you're more concerned with how tweets are actually uploaded and how they're sent to feeds, then definitely go check that out but for now, we're going to mostly be focusing on the search aspect of this design. So let's just go ahead and take a look. 
		- The client is basically going to hit the load balancer when uploading a tweet which is going to hit our upload service which puts our tweet into some sort of log-based message broker for both replication, partitioning and most importantly durability and replayability of our tweet objects and so that is first getting sent to our feed caches which are Redis instances which again i'm not really discussing much here because i've discussed them in another video. It's also in parallel being sent to the tweet database which again i'm not discussing much because i've discussed it in another video. We can just assume that's relational, replicated, partitioned etc etc and then finally and most importantly for this video, it is going to be sent to the proper partition of our search index. So as you can see, I have p1, p2, and p3 labeled. Perhaps they're probably going to be a lot more partitions in real life and each of those are going to be replicated as well. Single leader replication is probably going to suffice in this case and another thing to note is these could even be in memory and not necessarily on disk. It really depends how much you want to spend in your service but, you know, using a HashMap here is totally acceptable and that's actually the solution that "grokking the systems design interview " recommends. However i think it's also fine if you were to use, you know, a disk based solution and then just a lot of caching. So as you can see coming from the search index, we have our search cache which will hopefully be holding the most popular search results and then finally, we have our search service which is going to aggregate any results from the cache and also the search indexes themselves and sort them and put them together in a way that we can properly return the results to the client. 
		- I hope this video was informative for you guys. Again, I think it's going to add a lot of information over "grokking the system's design interview" which i felt was particularly lazy for this question, and yeah, again guys, thanks for all the support on the channel. Um, let me know if there's anything you want me to change. But anyways, I look forward to making another one soon, I'm uh probably going to ramp up a little bit more in work soon so hopefully i can keep up the flow of videos but uh if they go down a little bit, you'll know why. All right fella,  have a good one