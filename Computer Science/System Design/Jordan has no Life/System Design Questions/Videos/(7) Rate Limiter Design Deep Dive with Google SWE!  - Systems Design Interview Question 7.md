---
Source:
  - https://www.youtube.com/watch?v=MDH7xybVr8o
---
- ![[Screenshot 2024-11-22 at 5.19.58 AM.png]]
	- Introduction
		- hello everybody and welcome back to the channel. I have a couple of announcements to make, um if you remember from the last video, uh my left eye is pretty much almost back to normal. Um additionally uh there may be some new people here on the channel due to a potential collaboration that will hopefully be getting posted soon. Um, if that's the case and it's been posted, well, welcome here. As you can see, we're all going to have a fun time together learning and getting good at this stuff. Uh today is another video that I'm not overly passionate about because it's less about systems design. It's more about just you know actual kind of doing things with an algorithm but uh still important nonetheless and that's going to be building a [[rate limiter]]. So a rate limiter is pretty important just in terms of basically making sure that you don't get violated by other people using your service. That could be people trying to DDoS you. So basically you know, just spamming your servers. Um, it could be just a malicious or not even a malicious script but rather um a script that basically has gone haywire and all of a sudden is making a ton of requests. It could be a user who doesn't even realize that, um, you know they're overloading or something like that, and just in general it's good for both saving your resources, saving you money, making sure that your servers aren't going down. So it's important that, you know, we know how to build one of these. I didn't make a dedicated video to it back when I was kind of making my topic videos and so as a result, I felt like I should give a little bit of an introduction to um an API limiter. Typically, these are things that may even be attached to your load balancer so we'll kind of see that in um something like the final diagram but we'll get into all that as we go and uh yeah enjoy the video and in the meantime, let's get into it.
	- Rate Limiter (Functional Requirements)
		- Description
			- Limit entity to a certain amount of API requests over a given period of time
		- All right, as far as the functional requirements go for a rate limiter , here's really only one and that's basically, we pick an amount of time, we pick a limit and then we say, within this amount of time, no user can basically exceed that limit for you know a given API call. So there's obviously that requirement but then in addition to that, the entire point of a rate limiter is that you can more or less stick it into your backend without it having a huge impact on the performance of your service. So keep in mind that super low latency rate limiting is kind of the main consideration to have here and in addition to that, it obviously can't be a single point of failure. We need some sort of fault tolerance such that, you know, if one rate limiter goes down, we have either redundant rate limiter, or something that we can use in order to ensure that our entire service doesn't crash and we have a bunch of cascading failures. Um, obviously pretty quick but let's get into some capacity estimates.  
	- Rate Limiter (Capacity Estimates)
		- Assumptions
			- 1 million users
			- 8 byte userID
			- 4 byte timestamp
			- 4 byte counter
			- Limit of 100 requests per user
			- 1 mil $\times$ (8 + 4 + 4) bytes = 16MB 
				- if storing all requests using a counter
			- 1 mil $\times$ 100 req $\times$ (8+4) bytes/req = 1.2gb 
				- if storing every single request
		- Okay, as per usual, um for the capacity estimates, I'm going to be stealing a lot of these numbers from "grokking the systems design interview" and so we'll just take some of the assumptions, and we can talk about them a little bit. So the first thing is that um, they're probably going to be around a million users, sure. Could even be more than that, but for now, let's stick to a million. Let's assume that a user ID or maybe even an API key if that's what we're using to do the rate limiting is going to be 8 bytes so 64 bits. Um additionally, we're going to have to have some sort of timestamp component which we can just say is 32 bits or 4 bytes. Um if we're going to use a counter to keep track of all of the existing API calls for a given user, that can be 4 bytes or if we're going to say basically you know, I'm going to rate limit you to 100 requests per minute and as a result, I'm keeping track of the last 100 requests, then you know, that is also going to potentially change the size of the amount of data that we need to hold. So if we're just holding a counter, like I said, that's just four bytes and so then the entire basically table of those million users with the user ID, timestamp, and counter, is going to be 16 megabytes because you just go ahead and multiply a million by 8 + 4 + 4 bytes. So that's 16 bytes times a million: 16 megabytes. Additionally, on the other hand, if we were to basically be holding those last 100 requests that a user has been making so that we can see if they've made 100 requests per minute and we'll discuss why we would do either of these strategies, that is going to result in 1.2 gigabytes of data because now we no longer need the counter but basically we would have 8 bytes for the user ID, four bytes for the timestamp, a million users, and then also multiply that by another 100 requests per minute, ultimately getting us to 1.2 GB. So obviously that can all fit on one machine, doesn't necessarily mean that we shouldn't be doing partitioning because that could obviously speed things up a little bit in certain scenarios, but we can discuss that all later and uh for now let's talk about some API design.
	- Rate Limiter (API Design)
		- Description
			- API Endpoints
				- `rate_limit(user_id, endpoint_name, timestamp)`
		- So like I said earlier, there's only one functional requirement. So if there's only going to be one functional requirement, there really only needs to be one API endpoint and we can basically just call that a get endpoint and I'll just call it rate limit where basically it's going to consist of either a user ID or you know an API key if that's how we're going to be rate limiting or possibly even an IP address and we'll discuss the tradeoffs of you know limiting users based on those later. Um, an endpoint name. So basically what API endpoint are you calling because each endpoint may have different rate limits for how often you can call it. Um, you know, in the tinyURL video we discussed, you know, it's pretty logical that you may want to have different URL limits for you know creating a tiny URL versus deleting a tiny URL and then finally also a timestamp. Obviously we're going to need some sort of timestamp to basically say, is this um API call that this users making valid within our rate limiting policy or is it not? Okay, like I said, just one endpoint. That's out of the way. We can talk about the database schema.
	- Rate Limiter (Database Schema)
		- Now here's a little surprise for you guys. The database schema and the covid-19 pandemic have something in common which is that neither of them exists because the pandemic is just a scheme by Bill Gates to put microchips in all of us obviously. Um no, but actually uh yeah the database schema. We probably don't want to be using one here. Why? Because we need such low latency for this algorithm that we're actually just probably going to be best off keeping everything in memory in something like a Redis database or Memcached database and as result of that, we're probably going to be using like a hashmap based on a user ID instead of having um basically a table that we can query via SQL or some sort of querying language. So with all of these kind of prerequisites out of the way, we can get into the actual algorithms that we should be using in memory to implement uh basically multiple different types of rate limiters and then once we do that, we can discuss I guess some of the more nitty-gritty things that come along with those.
	- Rate Limiter (Architectural Overview)
		- All right, so before we go ahead and discuss how we might implement the rate limiter, let's first discuss two possible types of rate limiters that we can go ahead and actually create. So the first is going to be a [[fixed window rate limiter]]. So this basically means the following: imagine that I want to limit a user to three requests per minute. However, the minute is not a continuous minute but rather they're at fixed intervals. So for example I can make three requests anytime from 12:00 to 12:01 but even if I were to make three requests at 12:00 and 59 seconds, I could then still make three more requests at 12:01 and 1 second even though I would technically be making six requests in a 2 second span because of the fact that they're outside of that same fixed window which like I said are the exact minute intervals, it doesn't matter. So that's what a fixed window implementation would be of a rate limiter. Additionally, we have a rolling window implementation of a rate limiter which is basically exactly what I described before. It means that if I make three requests to my service and the rate limit is three, if they're in the same minute from the first to the last one, regardless of, you know, which fixed window they're in, it means that they are going to be limited. So let's say at um you know minute 12 15. You know minute 12 and 15 seconds, I make my first request uh minute 12 and 30 seconds, I make uh my second request and then minute 13 um and at you know 10 seconds, I make my third request, that third one is going to fail because I've now hit my limit and we're basically 55 seconds apart from the first request so I wouldn't be able to make another request for 5 more seconds. So that's kind of the key difference between a fixed window and a rolling window style of rate limiter and obviously it's going to take basically two different styles of code in order to implement those.
		- So let's talk about the fixed window implementation because that one is generally pretty easy to do and all it basically is, is you're going to go ahead and use a HashMap where the key of the hashmap is the user ID and then the value of the HashMap is going to be a tuple containing two things. One is going to be a counter and then the other is going to be a Timestamp. So basically, we're going to do the following. Let's imagine that um we have I don't know I'm going to make a call to an API and the first thing that's going to happen is it's going to hit my hashmap and if I basically have never called this API before, the hashmap with my user ID, the corresponding value there is going to be empty. So what I would do is I would take whatever timestamp was associated with basically my API call. Let's say um we're rate limiting by the hour, right? And so um the timestamp then when I normalize it to the hour is going to be 12:00. So the count is going to be 1, 12:00. Now let's say I make another request at 12:30 and the rate limit is basically three per hour on a fixed window. Now what I'm going to do is I'm going to go find my own user ID in that um HashMap and I'm going to say, okay, let me increment the counter here because the timestamp is still the same because 12:30, you cut off off the minutes part and it's still 12:00. So now I have 2, 12:00 and let's say the rate limit is 3. So I make another request at 12:45. I'm going to go ahead and see now that when I try to increment the counter on my hashmap, I'm going to hit three and I'm going to have to actually return um the fact that this request can't be made because I've hit my rate limit. Now what if I were to then wait a little bit and make another request at 1:01? Well we would go into the HashMap and I would see now that basically after I normalize 1:01 to the hour, that 1:00 is different than the counter currently in the hashmap. Well basically what I would do then is I would say, okay, change the value for the HashMap to the counter being one and the time corresponding to that counter is going to be 1:00 because now basically we've reset ourselves to a new fixed window and we can start keeping track of all the events in this fixed window. 
		- So that one is pretty simple but it's got a couple of things to think about. First of all, like I said, obviously when you have a fixed window implementation, you can make more requests than the API limit as long as you're just kind of clever about it and, you know, the first set of requests is towards the end of an interval and then the second set of requests is right at the beginning of an interval. Um similarly, there are actually some race conditions to consider here in the sense that we have ourselves um a read-modify-update cycle because anytime you're incrementing a counter especially if we have two concurrent threads trying to do so, one of those updates may be lost because it's going to read the old value of the counter. So let's say the counter was one and then I make two requests in really quick succession and then um you know my in-memory database like Redis concurrently handles those, they might both read that the counter was one, update them to two, and then once those updates are both made, even though I've technically now made three requests, uh the database seems to believe that I've only made two. Uh, the one thing to note here is that while this is definitely a valid consideration for a race condition, I actually looked into it a little bit and it seems like Redis is running on a single thread for all basically um single key operations and as a result of that, we don't really have to worry about this here. However, this may come up in an interview that there is a potential race condition in any possible situation here in memory where you're going to possibly have multiple threads updating that same value and doing that read-modify-update cycle. You always have to be very conscious of when you may have to use a lock here because if Redis was concurrent and allowed basically some parallelism and having multiple threads interleaved and um editing this counter at the same time, then we would need to put a lock on the counter before doing anything, and that would obviously slow our service down quite a bit. 
- ![[Screenshot 2024-11-22 at 6.44.33 PM.png]]
	- Rate Limiter (Architectural Overview) Part 2
		- Okay next, let's talk about how we would Implement a [[sliding window]]. So basically, now what we're going to do and this is where we are going to more or less maintain a list or a sorted set is something you can also do in Redis per user so you'd basically have that HashMap again with the user ID as the key and then the value being the list or the sorted set and all you would do is you make sure that every single time you want to add a timestamp to that list or sorted set, you're going to remove all of the elements of the list or sorted set that are basically outside of the relevant time interval from that newest element. So to kind of make this concrete because I didn't really word that too well, let's say that um I have a rolling window of 1 hour where I can make three requests and in that list currently, we have a request at 1:00, we have a request at 1:30, and then I am going to put in another request at 2:15 which is valid because 2:15 is more than an hour away from 1:00. Once we put in the 2:15, we would have something saying okay, go back and actually go into the sorted set and remove 1:00 because it's further than an hour away from 2:15. So it's not relevant for our rate limiting.
		- Um again I'll discuss this in a second but that potentially opens the possibility for more race conditioning issues, um like I said Redis is single threaded but anytime you have something like a list or a sorted set, you know, there's going to be some sort of in-memory pointer there especially if it's a linked list or for a sorted set you tend to use a HashMap plus like a skip list under the hood and if there's pointers there and two things are kind of concurrently editing those data structures, um, one thing may believe that one node say doesn't have a child node and as a result, both of those threads are going to say, oh I'm the child of this node and now basically one thread has overridden the other. But um yeah, keep in mind there are race conditioning issues here. The point is this though, um this sliding window um is going to allow us to basically take any chunk of time say it's an hour but it doesn't have to be now within a fixed interval because we can go ahead and figure out basically, hey, are there more than the limit of events basically 1 hour prior to this most recent entry of the list. Additionally, another thing I should note is that it doesn't necessarily have to be the case that you are um basically deleting all of those old entries when you insert it. Another thing you could do is basically um insert an element at the end of the list, count back, so say we're limiting ourselves to 100 requests an hour, you would count back through the list to see if there are 100 entries where the timestamps are within an hour, um and then you could be running on some separate thread, a cleanup process that basically takes out all of the old irrelevant entries of the list that are no longer important for rate limiting. A second way of actually implementing um this rolling window is by basically using a bunch of small fixed windows. So if you remember from the stream processing videos, you can actually basically calculate a bunch of really small fixed windows. Let's say you do a fixed window for every single minute and then if you want any rolling window of 5 minutes, you can just aggregate the numbers from those five fixed windows of one minute that are relevant to your rolling window. So that's another way of getting things done. The point is though, you just may need a lot of really small fixed windows.
		- Okay and then finally there's a question on basically are we partitioning, are we replicating, all these things. Um naturally if we're going to partition, it makes a lot of sense that we might partition on the user ID or the API key or whatever we're using or possibly the IP address. A quick note is that um, it makes a lot of sense to API limit on both a user ID and also an IP address. The reasoning being that um a user ID can only stop people who are logged in, um if I wanted to, I could basically DDoS the login endpoint and you wouldn't be able to stop me because it's just not a rate limited service. The issue with rate limiting based on IP is I might you know have an evil brother who's doing this from his room and then I want to go ahead and get on the website and you guys won't let me anymore. So you know there's that to know it. You want to always be rate limiting the right person basically as well. So obviously a hybrid approach makes the most sense where you basically will rate limit on the user ID or the dev API key sometimes and also the IP address other times. So like I said, obviously that's a very natural thing to partition on because everything is self-contained for one user and you can kind of just use um a hash of the user ID and then um partition based on hash range. Another really nice thing then is that you can also use your load balancer to load balance on hash range and then as a result of that, what you can do is for your application servers, you can keep a cache of basically all of the API limiting information. What this allows you to do is it means that you don't have to hit an external server for your API limiter every single time and the server itself can basically go ahead and perform the throttling functionality. You can do this with a write-back cache so that when um you know you make an API call on the server and it keeps note of that, it doesn't have to instantly write this back to your centralized rate limiting server but instead can just keep track of it locally and then write it back eventually at some fixed interval which is kind of a huge benefit in performance. As for replication, we obviously mention that we want really high availability and the big thing here is we probably just want to do something like a single leader replication. The reasoning for this being that we want all the writes going to the same node because API limiting is something that can happen in a really short time span and as a result of that, we don't really have time for something like an anti-entropy process to go down. It's more important that all the writes are going to the same place so they can be aggregated and you can basically say, am I going to throttle you? or am I not going to throttle you. Um additionally, it's not really a huge deal if the master node is to fail and we just lose a few API calls that have happened. We can just go ahead and have one of the replicas take over and then all should be well again.
	- Rate Limiter (System Diagram Review)
		- Okay, super simple diagram here. Basically the client is obviously going to make a request which is going to first hit the load balancer and before the load balancer forwards that to the API service, whatever that may be, it's going to first go ahead and send it to the rate limiter which it's going to hit the server which will check Its write back cache to basically go ahead and see if that entry is in there already and if so, basically say, are we going to allow you to proceed with this API request or not. If it's not in the write back cache, then we are going to go ahead and hit our actual database which is going to be some sort of Redis instance which we have configured in single leader replication. This is obviously a pretty simple diagram for the most part and uh I've probably explained it pretty thoroughly like I said, it's mostly the algorithm in this one that's kind of important which is why I think it's, I don't know, a little bit of a stupid question for systems design. It would probably just be better of like a leetcode type question but either way, I think that um this is uh probably pretty sufficient here. So with that being said guys, I hope that it was a functional video, and that you enjoyed it and to those of you who are new here, like always, welcome pretty much at about 1,000 subscribers at this point and uh that's wild. So let's get this bread