---
Source:
  - https://www.youtube.com/watch?v=vPP2aoY7D4E
Reviewed: false
---
- ![[Screenshot 2024-11-13 at 9.49.36 PM.png]]
	- TCP and UDP (Background)
		- Description
			- While the network is an extremely complex area that can't be covered within one video, a good chunk of it has been abstracted away via protocols. in particular, TCP and UDP are two transport layer network protocols that operate on top of an IP network. However, while they operate in the same space, there are significant differences between the two that make them suitable for different tasks
		- Let's get through TCP versus UDP and we'll call it a day. Okay TCP and UDP. What are they? Well, basically the network is this super complex thing, right, that you have a bunch of cables going around, the routers in between and you're sending bits over these networks and um they're totally unstructured to start and somehow we've made it such that you know the network is what it is today where you can basically send you know ordered information and useful information over what's effectively just ones and zeros which is pretty crazy but in order to basically do this, the network has established a bunch of protocols which means it's established basically a bunch of standards of how data is sent and and how it should be sent and how it should be formatted so that all devices on the network can you know send this data in a standard way and then you know actually be able to use it. So in particular, the two protocols that i'll be talking about today are TCP and UDP which both operate at the transport layer and so basically there are significant differences between the two of them but at the end of the day, the job of both is to basically just go ahead and ensure that packets are delivered but as you can see, TCP guarantees a lot more about the way that these packets are going to be delivered than UDP and so as a result, we're going to start by breaking down what TCP is and how it actually works. 
	- Transmission Control Protocol (TCP)
		- Description
			- TCP provides reliable, ordered (via sequence numbers), and error checked (via checksums) streams of bytes between two computers in a network.
			- We will cover a few major components of TCP:
				- Handshake
				- Reliable Transmission
				- Flow Control/Congestion Control
				- Connection Termination
		- Okay, so TCP basically goes ahead and provides reliable ordered and error checked streams of bytes between two computers in a network. It uses something called sequence numbers to deal with ordering and i'll discuss that in a minute, um it uses something called checksums which is basically just like every single you know amount of bytes that's self-contained should have the sum of those bytes and so that way you know if something along the network happens to cause one bit to flip, it goes ahead and alerts you that the checksum is incorrect and as a result you know there's some error in the message. So that's one way of making sure that bits just aren't flipping around randomly and then basically the way that tcp is able to kind of make all these guarantees that i just mentioned above are through a few things um and we're gonna discuss all of those, so that's handshake, reliable transmission, flow and congestion control, and also connection termination.
	- Three Way Handshake
		- Description
			- Parts
				- Client sends SYN to server with some random sequence value x
				- Server responds with ACK with acknowledgement value x+1 and sequence number y
				- Client sends SYN + ACK with sequence number x+1 and acknowledgement value y+1
			- Note: Every acknowledgement value says what a computer expects the next sequence number it receives to be!
		- So first off, let's talk about the handshake. So basically, instead of something like UDP where you basically just send messages indiscriminately, in order for two nodes over the network to go ahead and send messages to one another, they must first establish something known as a three-way handshake and the computer that's going to go ahead and start out this three-way handshake is known as the client, the one receiving it at first is known as the server. So basically it's composed of three commands, hence the three-way handshake, where you have a SYN, ACK and then a SYN ACK. So first the client sends the SYN to the server with some randomly generated sequence number. It's useful to have the sequence number be randomly generated so that in the future, other people can't just like guess the sequence number and then send their own packets to the server and then the server thinks they're coming from the client, so that's important. The server is then going to respond with an acknowledgement with the acknowledgement value being the previous sequence number plus one. So x plus one. And then its own sequence number y. So what that acknowledgement number is basically saying is the next time i get a message from client to server, I want the sequence number to be x + 1 and what it's also saying is saying that i'm starting my own sequence with y and when we send this syn ack back with y plus one on the SYN ACK,  it's saying that the server is now only going to send data to the client or the next data message from the the server to the client is going to have the sequence number y plus 1. So the acknowledgement header basically always says what you expect the sequence number of the next received message to be. So finally basically like i said the client then receives that ack from the server and then sends a SYN + ACK, so the SYN has the sequence number x + 1  and then the acknowledgement has the value y + 1. So as you can see, everything should be in order here and then like i said, every acknowledgement value just goes ahead and says what a computer expects the next sequence, whenever it receives, to be and we can go ahead and use this to basically prevent messages from being duplicated and also keeping messages in order.
	- Reliable Transmission
		- Description
			- After the handshake is complete, we can start sending packets
			- Dup Ack retransmission
				- Sender sends packet with `seq` number 99, packet gets lost
				- If receiver had received message, sender would have gotten ACK back with number = 100
				- So when sender receives next message with `seq` number 100, and gets back ACK with number = 99, it knows it needs to first send packet 99 again
			- Timeout based estimates
				- Sender will resend packet if it doesn't receive ACK within some timeout
					- Timeout is based on estimate of round trip time
					- On expiration double the round trip time
				- Helps protect against DDOS attacks
		- So how do we prevent message duplication. Well we have this concept of a Dup Ack retransmission. So basically imagine i'm the sender and i send a packet with sequence number 99 and that packet goes ahead and gets lost. I don't know that it's been lost, and you know, I'm not waiting for the acknowledgement just yet but what i am going to do now is go ahead and send the next packet which has sequence number 100. Let's imagine this one gets through, well the receiver is going to say, well actually, I never got number 99 and so i'm going to send an acknowledgement back with number equals basically 99 itself and so now I know that i first need to send packet 99 again because 99 never actually got there. So in that sense, now the sender can basically say, oh shoot, you know like, I have to go ahead and re-transmit this message because i never got the proper acknowledgement for it. So that's kind of important to know. Um additionally, we now have these um timeout based estimates where basically if you don't receive an ack within some timeout which is basically based on the estimate of the round-trip time of the network from the sender to the receiver, then you're going to basically go ahead and resend that packet. Um, to prevent this from you know just happening too often, say one one of the nodes is down and you don't want to overload the network, we're basically using something called an exponential back off where every single time that we have a timeout expiration, we double our estimate of the round trip time so that we're not just sending tons and tons and tons of messages but rather you know like we're retrying at an increasing or a decreasing frequency and this helps protect against DDOS attacks, that kind of exponential back off.
- ![[Screenshot 2024-11-13 at 10.06.11 PM.png]]
	- Flow Control
		- Description
			- TCP regulates network flow so that one device does not send too much data over a network and overwhelm another device.
			- Receiving device specifics how much space it has in bytes to buffer new messages, and sending device can only send that much more until new acknowledgement comes in
			- If receiving device has no room left, sender waits for some period of time and then sends a small packet to see how much space it has left
		- Okay, next we have this concept of flow control. So every single node in the network basically has a dedicated buffer that um you know the buffer is in memory and what this buffer does is it's saying okay, i'm getting all of these messages over the network and i have to put them in this queue in memory so that i can eventually process them because otherwise they would just get thrown out. You have to store them somewhere before you can actually process them. So this can become an issue if say you know i'm on the receiving end of a message and i'm getting so many messages that my buffer gets filled up and now i just have to either start deleting or dropping messages. So TCP, what it'll actually do is it'll regulate network flow so that the receiving device doesn't get too much data and the way that it actually does this is every single one of those acknowledgments has a separate piece of information that basically says how many bytes are left in that flow buffer and that way the sender will know not to go ahead and overwhelm the receiving device. If it's the case that the receiving device has no room left, then the sender is going to wait some period of time and basically send a super small message in order to get an acknowledgement basically saying how much space is left in the buffer. So obviously you don't want to be overloading buffers because if you're overloading the buffer and then you're retrying all these messages, not only now are you potentially dropping messages, but you may be congesting the network and this is what we're going to talk about next.
	- Congestion Control
		- Description
			- It is possible to overwhelm a network link by sending too many packets over it, known as congestive collapse, lowers network throughput
				- Can occur in TCP if a bunch of packets need to be resent (can happen from full queues)
			- Mitigation strategy
				- Congestion window keeps track of number of packets currently being sent over a connection and limits them (similar to flow control)
				- Size of window builds up over time using additive increase multiplicative decrease strategy (increases in size linearly, decreases in size exponentially if network overloaded)
		- So we have congestion control. So basically not only is it possible like i said to overwhelm a receiver, you can actually overwhelm network links if we're sending too many packets over them and what this does is it lowers network throughput pretty significantly and since in tcp for you know a variety of different reasons that i've mentioned, packets might need to be reset you know either the receiver has too much congestion or too many messages in the buffer, the receiver might be down or you know one message got dropped and now it has to be retransmitted as a result. Basically you know, we're resending all these messages and we want to make sure that our sending node is not congesting the network. So what do we do to actually mitigate this? Typically what tcp will do is use something known as a congestion window where it's basically like almost a buffer on the sender that says how many packets are currently being sent over a network connection that you know haven't yet been acknowledged and so the congestion window starts out really small, right, it might actually only be like you know one packet's worth. However, it basically builds up over time. So as long as acknowledgements are being sent successfully, basically what we do is use something called an additive increase multiplicative decrease strategy where the size of that congestive window might grow linearly over time by receiving acknowledgments but say that we then realize that um you know the network is becoming congested, then what we're going to do is take that congestion window and half it and we're going to keep doing that until you know we're not congesting the network anymore. So, you know, it's kind of like we're very conservatively making that window a little bit bigger but the second we're too big, we're like, oh we gotta back off and back off really quickly. 
	- Connection Termination
		- Description
			- "Four Way Handshake"
				- Really just each half of the connection termination independently
				- Cannot terminate both connections at the same time, see two general problem
			- Upon the first FIN sender sending the final ACK, wait some time period before closing
				- Ensures that any messages in the connection that may not have been delivered yet can be discarded before another connection opens up on this port and sees them
		- Okay, what about in terms of connection termination because it's important that you know people can open up ports again and make sure that a client and server can eventually both agree on the fact that they're no longer sending messages to one another. Well, in order to do this, we kind of have this four-way handshake which is not really as much as like a four-way process but more so just two independent two-way handshakes and this kind of stems from the fact that each half of the connection is going to terminate independently and the reason we can't terminate both of them at the same time is kind of something to do with this idea of a two generals problem. Where two disjoint nodes or connections over an unreliable network can't agree on something because if i were to tell you know if i were one server and i told the other server you need to you know terminate this connection, how do i know that it did? Well maybe they send an acknowledgement to me and i get that but then how does the server know that i actually received the acknowledgement and that problem just keeps going going on and on and on and on forever and so basically there's no way to really be sure that both sides can coordinate and close the connection at the same time so really you just close each half of the connection one at a time. So basically the the initiator is going to send a FIN message and the receiver is going to send an ack and also its own FIN message to the initiator, which is also going to send an ack. Then the initiator of the the termination is going to actually wait a certain amount of time. The reason for this being that before actually closing the message, you want to wait a little bit of time so that any messages that may have actually been you know kind of like stuck in the path between the initiator and the receiver or vice versa can actually make sure to be discarded because if you were to say reopen a subsequent connection on the same port, you don't actually want to be receiving those old messages because that would just kind of throw things out of whack. It wouldn't make sense why all these unrelated old messages were still coming through. So you wait a little bit of time on the initiator and then you close the connection. 
	- User Datagram Protocol (UDP)
		- Description
			- One node on the network indiscriminately just starts sending data to another node with out any consideration for flow control, dropped packets, out of order packets, error detection, or congestion control
		- Okay, what about UDP. Well you see, I have this kind of scarface picture here of just literally indiscriminately firing a machine gun because that's basically what UDP is. You have your sender and your sender is basically just going to you know shoot all over the place and send all the packets they want to the receiving node and they don't care if they're congesting the network. They don't care if they're sending too many messages for the receiver's buffer. They don't care if they're dropped messages or they're out of order or anything like that. They're just sending them. They're full sending it. And so UDP is kind of chaotic but the truth of the matter is it has its uses, so let's go into those. 
- ![[Screenshot 2024-11-13 at 10.13.50 PM.png]]
	- TCP vs. UDP comparison and use cases
		- Description
			- UDP:
				- Much faster, no need for acknowledgements, just send messages
					- Makes it useful for any real time applications that do not care about dropped data such as video chatting, video games, DNS servers, stock prices (if you just want current)
			- TCP:
				- Slower but has guarantees about data delivery and ordering, and ensures that the packets do not overload the receiver or the network
					- Better for most typical REST servers that we will be using
					- Provide guarantees against DDOS attacks
		- Basically, UDP, the reason for using it is the fact that it's much faster. You don't have to establish any handshakes, you don't have to do any connection termination, you're literally just sending messages and so there are applications where this can be useful. You know, think about video chats or you know voice over the internet in the sense that you know we don't really care about delayed video chat data getting in. If we have a buffer, it's already too late, just you know hopefully we get the subsequent video chat messages. Same goes for video games, same goes for stock prices if you just want to see the most up-to-date price and so UDP definitely has its uses but that being said, in the general case, TCP is probably going to be better for most systems design problems where you care about, you know, the correctness of the bytes being sent. It's obviously very important that you're not losing any data or that your data is out of order especially when making something like a REST request and, so as a result you know most http requests are going to be done over tcp and they also provide better guarantees against DDOS attacks because of that flow and congestion control whereas UDP doesn't have any of that.
	- Conclusion
		- Description
			- While it is unlikely that you will be working directly with TCP or UDP in your job, it is important to know the differences between them, as being able to remove some of the guarantees provided by TCP can make huge differences in performance in latency focused applications. Nonetheless, for the most part TCP is probably the better choice, as the abstractions that it makes regarding network data are extremely useful, and without them they would have to be programmed in our application itself.
		- Okay, so in conclusion um these are pretty well established protocols at this point and unless you're a network engineer, they're probably generally going to be an afterthought. But that being said, there are definitely some applications where using TCP versus UDP might have extremely um big benefits to performance or additionally possibly even the correctness of your application and so as a result, it's kind of important to know the difference between these. They're definitely going to be systems design questions out there where you're kind of asked to explain the difference between them and in the event that you are, it's important to know them.