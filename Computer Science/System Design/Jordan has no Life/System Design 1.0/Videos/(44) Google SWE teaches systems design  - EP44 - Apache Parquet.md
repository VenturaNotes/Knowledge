---
Source:
  - https://www.youtube.com/watch?v=Ne9Gq3dQpW8
---
- ![[Screenshot 2024-11-13 at 9.06.08 PM.png]]
	- Parquet (Background)
		- Description
			- Previously we discussed libraries for efficiently serializing row based data: data with different fields. However, as we have mentioned in the past, for certain analytics use cases, we also want to be able to store data in a column oriented manner. Like the row oriented case, we are generally doing some sort of exporting/processing of this data (such as with MapReduce or Spark), and therefore we want to be able to store it as compactly as possible so that transferring it over the network is easy! This is where Parquet comes in
		- Okay [[Parquet]], well basically in the past or actually last video, I discussed Avro, thrift, and protocol buffers which are all ways of serializing data in a row-based format. So that's really useful for any type of time where you're going to be uploading or you know retrieving a couple of rows of data at a time right? You want the entire row. However, as i've mentioned in the past, a very common use case in a lot of big companies is to basically be doing analytics processing, and when you do that, you want to be retrieving data that is basically from some set of columns in a huge gigantic Hadoop table or SQL table or something along those lines and as a result it's very important to kind of have data locality in those columns and so what Parquet basically is, is another library that actually goes and helps manage that you know storing of data and columns in a way that not only allows for more efficient querying of data within those columns, but in addition to that, reduces the I/O of sending all of that data over the network by using some pretty cool compression techniques. So let's go ahead and talk about that
	- Hybrid Oriented Storage
		- Description
			- Issue with row oriented storage:
				- Bad for aggregations across a subset of columns
			- Issue with column oriented storage:
				- Lack of disk locality between elements of each row makes it hard to reassemble row
			- So instead Parquet has chosen to use a hybrid approach!
				- Partition the row space, and within each partition use column oriented storage
		- Okay, so Parquet, even though i mentioned column oriented storage thus far is really more of a hybrid oriented storage. So let me discuss what that means. Well we've spoken about row-oriented and storage in the past which is basically just saying all the entries in a single row of data are stored together and the issue with that is for analytics processing where we really only care about you know the values of one column over all the rows, that's pretty useless to us because we don't get data locality. We have column oriented storage which is basically where you're taking all of the rows in the table and storing the values of you know the same column next to one another. So that's really nice for analytics processing but it comes with the cost of if you're ever trying to do random accesses or maybe just trying to go ahead and put one row back together, all of those values from you know the same row are going to be super far from one another on the disk and as a result, having to do all those seeks is going to take a really long time. So instead, we have this third approach called the hybrid model where basically that involves going ahead and taking a table and splitting it first of all row wise into you know a bunch of different partitions and then on each partition using column oriented storage. So looking at the image below, imagine that we split between row two and row three and then within that, we're using column oriented storage so a0, a1, and a2 since they're in the same column are stored together b0, b1, and b2 same deal. c0, c1, c2 also stored together and then we start that partition and do the same exact thing for the latter three rows 
	- Parquet Introduction
		- Description
			- Data split into Parquet files, which contain multiple "row groups"
				- Row groups are partitions of the rows in the actual data
				- Also contain a footer with some metadata pertaining to the information in the row groups
			- For each column in the row group, split into chunks
				- Each chunk of a given column contains 1 MB pages which also contain some metadata about the encoded data
					- Min, max, or count of the data
					- A dictionary representing the elements in the data
					- Bloom filters?
		- Okay, so what is um parquet? How do we store the data? Why is it useful? Basically, here we go. Data is going to be split into these parquet files which contain multiple row groups where row groups are basically those partitions that i just mentioned prior of those row. So you know it might be three rows, it might be you know 100 rows, probably it's going to be a pretty big amount of them because i think the default value for a row group is 128 megabytes worth of row data which is actually quite a bit but obviously if you're looking at a table, that's gigabytes or terabytes in size that gives you a lot of room to have a bunch of partitions. Additionally, each one of these parquet files is going to have a footer which contains some metadata pertaining to the information in the row groups but basically within each row group, we obviously have a bunch of columns that we have to represent and then within each of those columns, columns are broken down into chunks and even further into one megabyte pages. So the thing with these pages is that all of these pages are going to contain some metadata about the encoded data. So the encoded data is going to be, you know that small amount of columnar data that's local to one another and so things that you might include in that basically in that metadata section are something like a min or a max of that data set, a count of the number of points in there, possibly even a dictionary representing the elements of the data and if you want like a more space efficient approximation of a dictionary, you could perhaps use a bloom filter and that's something that was recently introduced to Parquet. But i'll explain in a little bit how that's all useful.
	- Data Encoding
		- Description
			- Plain Encoding
				- If all data is of the same length, just store the raw data
				- If data can be of varying lengths, store an integer representing the length and then the data
			- Dictionary Compression/Run Length Encoding/ Bit Packing
				- Good for when many duplicate adjacent values in a column
		- Okay, so let's talk about data encoding. So imagine right now we have a list of values, right? And there are basically all the values in um one column for a set of adjacent rows well basically the naive implementation is to use a plain encoding where you know if all the data is the same length, you know say it's like a timestamp or something and every timestamp is I don't know 20 digits, you can basically just go ahead and store the raw data without any limitations in terms of you know how long is the actual timestamp itself because we know we can just run a loop over every 20 digits and then you know pull out our timestamp. That being said if the data is of varying lengths, you know maybe we're storing strings there instead of something like a timestamp, we can go ahead and first store an integer before every single string basically representing the length of it and then store the string itself. So that's how we would go ahead and decode that plain encoding but that being said, the reality is we can generally do a lot better than this. The cool thing about columnar data is that generally speaking (A) there are a lot of duplicates because over one column, there just probably are going to be duplicates over all those rows and then (B) is that if we have you know duplicates that are adjacent to one another as in we have a lot of repeated values next to one another then we can really start to compress it and we do this with dictionary compression, run length encoding, and bit packing and i'll show this off in the next slide.
- ![[Screenshot 2024-11-13 at 9.25.41 PM.png]]
	- Dictionary Compression
		- Description
			- (1) If many duplicate terms, place them in a dictionary with a corresponding number key
			- (2) Replace all occurrences of term with corresponding integer (only use the number of bits that it takes to represent all of the numbers in the dictionary - in this case 2 bits)
			- (3) If there are adjacent duplicate values, just use a tuple of (value, number of times it has occurred)
		- So imagine we have the following columnar data that starts out on the left there. We have this uncompressed data of a bunch of strings of countries and what we see here that within those strings of countries, we have a ton of duplicates such that there are really only four unique values. Well the first thing then that we would actually do is create a dictionary that goes ahead and gives each country a unique index in the dictionary. So now every time i see zero, I know that corresponds to the United States, one corresponds to France and so on. And so i can actually take that uncompressed columnar data and replace every single one of those strings with an actual number which i know takes a lot less space. Additionally, something cool that we can do with this dictionary is instead of using a full 64-bit number to represent each of these numbers, I can actually represent all of these numbers using just two bits. The reason for that are that there are only four entries in the dictionary and as we know just based on powers of two, I can represent four numbers using just two bits, so in that way, that's called bit packing so we're able to save even more space there. Then finally what we have is run length encoding. So this is really useful when we have not only just duplicate values, but duplicate values that are next to each other. So you can see there that in that final compression in the bottom right, we have zero, one, two, three comma three, one and zero and the reason that we're able to do three comma three and compress things further is the fact that there are three threes that are next to one another. So basically, we just go ahead and create a tuple that says the number of times that this number occurs and the actual number itself. So in this case, there are three consecutive threes hence three comma three. So this process allows us to hugely compress all of this data which is really nice and it gets even better if you know we were to go ahead and pre-sort the data in some way that would um allow us to compress this column even further.
	- Optimal Chunk Size
		- Description
			- If chunks become too big, we will not be able to fit all of the values in the dictionary (because there may be too many), and then Parquet will have to use plain encodings in order to encode the rest of the data
			- If there are many small chunks, we will incur extra overhead by virtue of having to store metadata for each chunk.
		- Okay so what about just like the chunk size because i mentioned that columns are split into chunks and there are advantages and disadvantages to having big chunks and small chunks from each of these columns in a certain row space. So the first thing is that if chunks are too big, basically dictionary compression is going to not work as well because we only have so much space to store our dictionary in and so if there are too many values in that chunk, too many unique values, we can't fit them all in the dictionary and so it not only is it going to take more bits to represent each entry of the dictionary but if we can't even fit every single value in the dictionary, then the values that can't be fit in the dictionary are just going to have to use plain encoding and we're just going to have to store them as they were originally written so we would get no benefit there. Hence, sometimes it's better to actually decrease the size of those chunks so that you can actually use a dictionary to represent all of the unique values of the columnar data in that chunk. Furthermore, however, it's not great to use overly small chunks. The reason being that i mentioned every single chunk has a metadata section and if we have too many small chunks, we're going to end up storing a ton of metadata and that incurs a lot of overhead in terms of the amount of disk space that we use and obviously the amount of um basically information that we have to send over the network if we were to run like a batch process on this. 
	- Predicate Pushdown
		- Description
			- What if we want to run a query over all rows of a given column?
				- e.g. find all rows where x > 69
			- Recall: each page has metadata telling us the min and max, so we may be able to skip some pages! (Can pre-sort data so these stats are more useful)
				- e.g. find all rows where x == 69
			- Recall: some pages contain dictionaries listing the elements in them so we can skip the ones where x is not present!
				- Recent support for bloom filters as well for more space efficient set approximation! Can also partition Parquet files such that they uphold certain predicates about the data and thus you can read only certain files.
		- Okay, so now we're actually going to talk about the metadata because this enables us to do something known as predicate push down and this is actually a really useful feature of parquet beyond just the encoding that i've talked about in perhaps past videos. So let's say we want to run a query over all of the rows of a given column and so we basically want to say find all rows where some value x is greater than 69. Basically we know that every single page has metadata telling us the min and the max of each row so if we see that you know the min is zero and the max is 20, we know we can skip over that entire chunk of data and so this greatly allows us to speed up the process and it's super useful to kind of have this metadata. Additionally, in cases where we actually pre-sorted the data like i mentioned before, this would allow us to skip even more stuff over because then, you know, our first chunk would have say values zero to two, the second chunk two to four, the third chunk four to six, and the point is that we could then skip over pretty much you know say the entire first half of the data and only start reading from the second half of the chunks because our sorted data is basically telling us there's nothing that we're interested in in the first half of all of this data. Another cool thing that we can do is instead of just looking at rows where you know we want a value greater than some threshold, if we're trying to find all rows where some certain value is contained in that chunk, we can actually use the dictionary that is being used to compress the chunk to basically go ahead and tell us if a certain value is in a chunk, so you know like from our country's example before, if i wanted to find all the rows where you know country equals united states, I can see that united states is present in the dictionary for that chunk and as a result i know i can go ahead and check that chunk and if i see the dictionary of a different chunk where united states is not in there, then i would go ahead and skip over that thus saving me some processing time. You may actually think to yourself and this is a pretty cool thing is that oh you know if the dictionaries are taking up too much space, we don't actually want to keep track of all those unique values, one optimization that we could do here for set approximation without taking up too much space would actually be to use a bloom filter and that's something that's recently been supported by Parquet and it is kind of an interesting optimization to see these come into play yet again in order to perform that set approximation for predicate pushdown. And then furthermore you can also partition the Parquet files themselves because the Parquet files can basically have directories and subdirectories and so what you could do for example is, say you know you have a bunch of data corresponding to the dates, you could basically have all the data, all the columnar data corresponding to say like January 1st in one Parquet file and then all the columnar data corresponding to January 2nd in a different Parquet file and then that way you could further use that to enforce predicates because if we only wanted the January 1st data, we could just check that Parquet file and thus that greatly reduces the amount of search or query time that we're actually going to be doing.
	- Conclusion
		- Description
			- While we have touched upon column oriented storage in the past, and some possible techniques to compress the data, we can now see how Parquet uses a more hybrid approach to provide some metadata for each column chunk in order to allow for more efficient querying. Parquet is not something that necessarily needs to be used instead of Avro, Thrift, or ProtoBuf, but rather in conjunction with one of them (use them for row compression).
			- Ultimately, using Parquet can provide huge advantages in batch processing as we want to be able to export large amounts of columnar data over the network as quickly as possible, and query it as well.
		- Okay, so in conclusion, I know this is a pretty quick video, but Parquet is actually super useful in either a data warehousing or data-like situation in the sense that anytime you want to be performing a bunch of batch processes for either something like an ETL job or just analytics, being able to compress data stored in a column oriented format as well as quickly query it using this extra metadata for predicate push down is super useful. Parquet is extremely commonly used like I mentioned in data lakes and you know there's a company called [[Databricks]] that basically is heavily supporting the use of Parquet within their own data lakes and as a result Parquet is not something that is you know inherently a rival to Avro, thrift, or protocol buffers but in fact what you would do is you would use them in conjunction with one another where you would first compress every single row using Avro, thrifter, or protocol buffers and then taking that compressed row data, you could go ahead and use something like Parquet to compress it in a column wise manner and that way you're making sure that you're incurring minimal network latency whenever you're doing these huge big data tasks because at the end of the day, generally speaking, it's faster to use CPU cycles to serialize and deserialize certain data especially in a batch processing context than it is to basically go ahead and send a bunch of rubbish over the network. 