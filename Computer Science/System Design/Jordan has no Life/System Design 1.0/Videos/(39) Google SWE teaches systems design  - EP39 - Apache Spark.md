---
Source:
  - https://www.youtube.com/watch?v=BsqM_neLJbM
---
- ![[Screenshot 2024-11-13 at 5.30.37 AM.png]]
	- [[Spark]] (background)
		- Description
			- Spark is a dataflow engine that is massively popular for performing batch computations. Its architecture has allowed it to demonstrate major performance improvements over [[MapReduce]], a more naive solution to batch processing. Spark is extremely commonly used in industry, and as a result it is important to understand how it is able to achieve these performance boosts.
		- Okay, I am back for another video. I'm still stalling and doing more concepts until about June first-ish but uh today is actually a pretty important topic which is going to be [[Apache Spark]]. So let's go ahead and talk about that. Okay so Spark, what is it? Spark is basically another um open source batch processing framework that was invented by a bunch of researchers at UC Berkeley I think around 10 years ago. It's an example of a dataflow engine and it is capable of providing major performance increases over something that is a more naive implementation of batch processing like MapReduce. So, let's go ahead and talk about how it's able to do this and explain all of that. 
	- Problems with MapReduce
		- Description
			- Recall: MapReduce is extremely inefficient for jobs that require many MapReduce calls/iterations (for example, page rank):
				- Tons of disk I/O due to materializing intermediate state to HDFS
					- Writing the result of a MapReduce job
					- Reading the result of a previous MapReduce job
				- Many redundant maps when not actually necessary
		- So what are the problems of MapReduce? This is kind of like a throwback to an older video. If you recall, MapReduce is really bad for iterative jobs or any jobs that are chaining together a bunch of map and reduces. Uh for example page rank so you know if you're iterating over some for loop and basically making adjustments to i don't know maybe a set of nodes in a graph or anything like that, MapReduce is really poor for that. So why is this? Well for starters, MapReduce materializes intermediary state. What that means is that you're doing a ton of disk reads and writes every single time that you do an actual MapReduce job because both inputs and outputs of MapReduce are all going to be written to the underlying file store which means that you're doing tons of writes to the disk and the truth is the majority of the time, we don't really care about the intermediate rights. We just care about the end result. In addition to that, generally speaking when we're doing MapReduce, the first time mappers are useful but after that generally we only care about reducing. The thing is though that we have all these unnecessary sorts and kind of re-partitions and a lot of the time it's not necessary and it just wastes a ton of computation. So Spark provides us with a lot more flexibility by just allowing you to define operators and give you a more free form just you know write some code and we're going to make it happen in a parallelized type of way. But let's go into some more detail.
	- Resilient Distributed Datasets
		- Description
			- Instead of materializing intermediate state, Spark uses RDDs:
				- In memory data structure representing the contents of a variable in a Spark program
					- In reality, the data is probably held across multiple nodes in the cluster!
				- Can create RDDs from data on disk or by using an operation on another RDD
				- Spark keeps track of the lineage of each RDD
					- This means that it knows what computations needed to be performed to get the value contained in an RDD
		- So like i mentioned, the big issue with MapReduce was manifesting this intermediate state and so what Spark chooses to do instead is implement something called RDDs or resilient distributed data sets. So basically these are just going to be in memory data structures where their lineage is tracked and they represent the contents of some Spark variable. So that variable is originally probably going to be gotten from HDFS or whatever the underlying data store is and then you can do transformations on an RDD in order to go ahead and derive other RDDs. So by going and mapping out this data flow, we're able to actually get a really efficient system going where the majority of the data is actually kept in memory assuming they can fit there and then that way we don't have to go ahead and materialize everything. 
	- Fault Tolerance
		- Description
			- Recall: If a MapReduce task fails, we can just resume from the output of the previous MapReduce calls. But Spark stores intermediate state in memory, and as a result we can't just resume so easily. Instead, we actually have to do some recomputation, and recompute certain values based on their dependencies
		- So, let's talk about fault tolerance because basically what I just said is that Spark does a bunch of computations, but it goes ahead and just keeps them in memory and so the kind of the important thing here though is if things are being kept in memory and we're running these huge computations, aren't nodes going to be failing all the time and then losing that data? So yes nodes are going to be failing all the time and losing data, but Spark kind of uses these dependencies or the graph of lineages for each RDD in order to kind of make good decisions about fault tolerance and how to handle things. So generally speaking what Spark tends to do is instead of storing all of the computation at every single step in a persistent manner it stores computation sometimes and then goes ahead and recomputes it when need be. So let's look at two situations basically of failures where Spark has to go ahead and recompute data. 
- ![[Screenshot 2024-11-13 at 5.42.48 AM.png]]
	- Fault Tolerance - Narrow Dependencies
		- So the first one is called a [[narrow dependency]]. So imagine we have these three nodes right here and the first thing that i'm going to do is basically just a map job. We've got all this data for browser history partitioned by the date that it actually happened and the only thing i'm first going to do is go ahead and map that browser data from you know url date tuple to just the url. So that's just going to be called the narrow dependency. The reason for this is that there's no inter node communication. The node is doing all this computation locally only for the data that it's storing in its actual underlying hard drive. 
	- Fault Tolerance - Narrow Dependencies
		- Description
			- Narrow dependency: computation of each RDD local to one node. If node 3 fails and only has narrow dependencies, we can just re-perform its computations on node 1 and node 2, without having to actually alter the existing state on node 1 and node 2.
		- So now imagine if node 3 were to fail. So basically what would happen is that if node 3 failed, we know that node 1 and node 2 can basically just take some extra load from what should have been node 3's data and then go ahead and calculate it itself. The reason for this is that node 3 was making those computations completely independently of what node 1 and node 2 were doing and so as a result, by splitting up node 3's computation between node 1 and node 2, we're able to kind of parallelize that fault and go ahead and quickly recompute it. However, this gets a lot more complex when there's, you know, greater interleaving between the actual data dependencies and we'll see that in the case of something where we're grouping together by key. 
	- Fault Tolerance - Wide Dependencies
		- So this is known as a wide dependency. So imagine instead of just taking all of these urls together and you know removing the dates from them, that in addition we did something where we grouped by key and put every you know unique key on a single node. So as you can see, we have all three urls for Jordan, Donald and Joe each on a node and those are each coming from node one node two and node three and this is a very common thing that happens in both MapReduce and just any type of Spark job because generally speaking you want similar keys together. Like i said, this is known as a wide dependency. So imagine the case where node 3 actually fails here. This is going to be a little bit tougher to recover from for the following reason.
	- Fault Tolerance - Wide Dependencies
		- Description
			- In this case, the final state of node 3 has wide dependencies in the sense that it relies on computations from node 1 and node 2 to be calculated. If node 3 crashes, we must rerun computation on both nodes 1 and 2 in order to regenerate this state. This takes much longer to do!!
			- To avoid this situation, Spark allows developers to checkpoint certain application state to disk so that if node 3 were to crash, we would have the result of the wide dependency saved.
		- So if node 3 fails, we see that the final state of node three was basically all these urls for Joe and so in order to go ahead and recompute all of that, we can't actually go ahead and just basically say, okay let's go ahead and parallelize the computation of node three over the remaining nodes that are still alive in the cluster. The reason for this being that we still have all this over the network communication that needs to happen in order to end up materializing that final state. So simply parallelizing things isn't going to work. You basically have to go back from the start and just recompute everything before that wide dependency and then go ahead and do the wide dependency again. So this is really inefficient when this happens. When there's a wide dependency and it kind of begs the question, okay, well how can we you know do better here and actually make it such that um you know we don't have to go ahead and recompute everything. 
		- Well the answer to this is basically checkpointing. I've mentioned checkpointing in the past, but all it really does is take you know the given state of a system and go ahead and save that to some sort of persistent data store like, i don't know, HDFS in this case assuming that's what you're running Spark on. So what we would actually do is you know assuming node three was still alive and we had this data at one point where we have those final states of node one, node two, and node three, and we knew that we were going to be doing more with it, but we didn't want to potentially run the risk of you know one of these nodes crashing and then having to recompute everything all over again, what we would do is actually go ahead and checkpoint the result of this wide dependency or this join and as a result of that, if node 3 were to ever crash, we could go ahead and just access HDFS and use that checkpoint to kind of re-establish a new node that's taking over the data of what node three had and then that way we're able to kind of quickly recover from that fault as opposed to having to recompute every single thing. So this is kind of how Spark tends to suggest that you handle wide dependencies and for narrow dependencies like i mentioned it's not really a huge deal if one node fails because you can go ahead and parallelize all that computation over the other nodes but generally speaking for handling wide dependencies, you want to be checkpointing.
- ![[Screenshot 2024-11-13 at 5.44.06 AM.png]]
	- Conclusion
		- Description
			- By better utilizing memory, and refraining from writing all state to disk, Spark is able to provide huge performance gains compared to MapReduce. While it comes at the cost of using more RAM, and having slightly worse fault tolerance than MapReduce, it seems that developers have realized that the increase in speed is well worth these tradeoffs, and MapReduce is seldom used in practice for large scale computation anymore.
		- Okay, so in conclusion basically Spark is able to provide huge performance gains over MapReduce by declining to materialize intermediate state every single time there's an operation and by doing so there are some slight trade-offs in the sense that Spark is going to be more expensive by making you use more RAM because you're storing all these RDDs in memory and in addition there is going to be slightly worse fault tolerance. Recall that MapReduce was kind of actually built to be super fault tolerant because Google was constantly preempting those jobs. That being said, it's pretty clear I guess in industry that Spark is much more widely used these days and I would imagine that's probably just because of the huge speed increases that it leads to. So as a result guys, I hope you understand now a little bit more about these dataflow engines and kind of the motivation behind using Spark over something like MapReduce. In practice, a lot of these batch jobs are literally gigantic and have tons of data and you have to chain together a ton of functions and so MapReduce becomes pretty infeasible. All of those disk writes just don't make sense. Okay, so i hope you guys enjoyed this video. I'll be uh hopefully doing another one tomorrow