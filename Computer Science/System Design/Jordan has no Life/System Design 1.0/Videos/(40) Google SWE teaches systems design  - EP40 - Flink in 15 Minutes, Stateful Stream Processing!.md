---
Source:
  - https://www.youtube.com/watch?v=YgxiRs4y4q0
---
- ![[Screenshot 2024-11-13 at 4.32.42 PM.png]]
	- Introduction
		- With that in mind, there seems like no better time to talk about systems design and computer architecture and so as a result I will be talking about [[Apache Flink]], A stream processing framework. I wish there were someone out there to process my stream but I don't think it's going to be that way so let's get into it. 
	- Flink (Background)
		- Description
			- Flink is what is known as a stream processing framework. It is not a message broker, but instead rather software run on a cluster of nodes used as consumers of some message broker.
			- Flink is useful as it allows distributed processing of a message queue (in real time as opposed to micro batching), while persisting state about the seen messages amongst the nodes in a cluster in order to make meaningful aggregations or joins on incoming data
		- Okay, so Flink. What is it? Well, basically Flink is known as a stream processing framework. So up until this point just because I hadn't really done my research on this topic, I kind of assumed that Flink itself was like a message broker and that somehow it was you know kind of enhancing that experience but as it turns out, Flink is not a message broker but rather a software that you run on a cluster of nodes in order to provide stateful computations on your actual stream processing. Unlike something like Spark streaming, it is giving you the ability to process these messages in real time as opposed to just using micro batching which lets a bunch of these messages accumulate over you know a short period of time and then processes them together. Um like I mentioned, kind of the whole reason that you would use something like Flink as opposed to just using a typical consumer is the fact that in many stream processing applications you have to maintain some sort of State. This is in order to perform things like joins or aggregations and we'll go ahead and see some other examples in the coming slides. 
	- Use Case Visualized
		- Descriptions
			- Transactions, Logs, IOT, Clicks, ...
			- (Real Time) Events, (Database, File system, KV-store)
				- Event-driven applications, Streaming Pipeline, Stream & Batch Analytics
					- Resources | Storage
						- K8s, Yarn, Mesos, ...
						- HDFS, S3, NFS, ....
			- Application, Event Log, (Database, File System, Kv-Store)
		- So just to quickly visualize the use case of what you might be using Flink for. As you can see, you can kind of ingest events from either something like a message broker or maybe even change data from a database like a file system or a key value store. Then as you can see here in the middle, we have these few different um clusters each running flank where all of those clusters are um allowing you to kind of go ahead and parallelize the computation of actually um processing each message and each of them has its own little like key value store on the Node that it uses in order to keep track of the messages it's seen in order to kind of better provide more functionality. Then from actually going ahead and processing these messages, you can then send the messages to a sync which is basically just the outgoing place where messages are going to be sent which can either be um to an application, the application can obviously be some sort of consumer or it can be um to uh another stream, so another event log or to another database.
	- State in Stream Processing
		- Description
			- Why is it so important to keep state if each event should be processed independently?
				- Grouping events by time
				- Holding a local copy of a database table for stream enrichment
					- Table updated over time by ingesting changes from a stream
				- Tinder use case example
					- Every like placed into a stream, Flink processes likes and stores them in state, if a like comes in with the opposite directional like already in state, we have a match
					- Flink forwards match data into a second stream
		- Okay, so let's talk about State and stream processing cuz I mentioned this is kind of the bread and butter of Flink and the reason why it's important. So why do we need to actually keep State? Well for starters a big point of um stream processing is to be able to group events by time. So while Flink does let you have a bunch of kind of flexibility in terms of which time you're actually going to use for a given event so that includes um the actual time it gets to uh the node that it's going to be processed on, the time that it is received by the consumer of the stream, the time that it was actually sent to the log broker, so you have all these options basically for how to group events but the point is in order to group events by time, generally speaking you want to be going ahead and windowing certain events and that way messages um from similar time periods will be sent to the same partitions to be processed together and you kind of need some sort of local state in order to do that. Additionally, another common case that we see is something known as stream enrichment where you're basically taking a bunch of stream messages and adding some corresponding information from an existing database table in order to go ahead and provide more information for those messages. So for example, you know imagine we had a bunch of web browser activity coming in and we wanted to go ahead and use that user ID coming in with the web browser activity to go ahead and list some demographics about the user before we send that to a secondary stream. Um, you can actually keep this table local to Flink and then update it over time using the change data capture from that table. Um another case and this is one that I saw in uh just one of the videos that I watched for learning about this topic was just like a Tinder use case. So imagine as opposed to just storing every single like or dislike in a database, what we actually did is place every single like in a stream for asynchronous processing because we don't have to return a result instantly. So every single like basically goes into a stream and then we use a partitioning schema such that all messages or rather all likes um consisting of the same two users are certain to go to the same node in that Flint cluster and as a result of that, if Flint uh processes a like message and we see that um the corresponding like from you know the other user to uh the first user is already there, then we know we have a match. Otherwise, we don't have a match, we have to store that like and then hope that um the secondary like comes in. If it doesn't, we'll probably end up expiring those messages after a certain time period. Who knows maybe a week, maybe a month or something like that. 
	- Fault Tolerance
		- Description
			- Each node has its own built up local state that it computes based on the messages that it uniquely sees (messages are hashed such that they only go to one node in the cluster, it is the developer's job to make sure they are partitioned correctly).
			- So each node has its own local state, and we want to make sure that if one of the nodes crashes, we can recover its local state as well as process any messages that it had yet to process on a new node. It does so by occasionally storing checkpoints of node state to durable storage such as HDFS or S3!
		- Okay, let's talk about fault tolerance because obviously if you're storing any state in either memory or disk and it's just going to be local to one node, we run the risk of that node crashing, losing this state and then everything is going to be useless because then we've lost pretty much a significant chunk of our stream processing information and this kind of represents that whole concept of having like a wide dependency. The state on one local node might represent uh messages that were partially processed by other nodes and then we would have to go ahead and reprocess every single message ever. So it's important that um we're able to kind of go ahead and checkpoint all of the local state on each node occasionally so that we know in the event of a crash we can go ahead and pull up a new node and have it running the same thing as that crashed node. So every single time that we have a checkpoint basically, the data uh consisting of the local state on all of the nodes is going to be sent to some high availability persistent distributed storage like HDFS or S3 and I'll talk about how checkpointing works in a little bit more because that's kind of interesting to see how it goes.
- ![[Screenshot 2024-11-13 at 4.55.34 PM.png]]
	- Flink Architecture
		- Description
			- Obviously, in order to increase performance, Flink tries to parallelize computation as much as possible
				- Single Job Manager node
					- Responsible for parallelizing a given job into work for each Task Manager using dataflow graph
					- Responsible for initiating checkpoints, and storing where they are located in distributed file system
					- Can achieve high availability on job manager by using a coordination service for keeping track of which nodes are up and which are not, as well as status of which checkpoints have worked
		- So just before I do talk about checkpointing, let's actually talk about the architecture of Flink. So typically, Flink does the following. We have this kind of single job manager node in the cluster which goes ahead and takes some Flint code and establishes this data flow graph which we can see on the right here. So even though originally the data flow graph is basically just saying, take some messages from the stream, go ahead and do something called [[KeyBy]]. So you're basically telling um flank how to go ahead and partition those messages and then after that send them to a sync which is basically, you know, maybe let's put them in another Stream. So the job manager would go ahead and see something like this and say, okay, well I know that I have a couple of partitions of that incoming stream so I have to partition those to a couple of consumer nodes, then after that, based on this whole [[KeyBy]] function, we know that all messages with the same key are going to the same node so that uh you know that's basically just how partitioning works. You want to be handling the same messages or similar messages so that the state is most relevant, so then basically the single job manager node is going to go ahead and map out this entire data flow execution so that it can be parallelized amongst all the nodes in the cluster and it'll then go ahead and tell each node in the cluster known as a task node what to do. Um, in addition to just telling the task nodes what to do though, the single job manager node is also responsible for uh you know sending and receiving heartbeats from those task manager nodes and also sending and receiving um checkpoint data to all of these task manager nodes and storing the metadata regarding you know which is the most recent checkpoint, where can we find it in our distributed storage. Um now you might say to yourself, okay, well obviously the single job manager node is going to be a single source of failure and you would be correct. So there's something similar to Hadoop if you remember this uh called high availability mode where basically any configuration that the single job manager node would have been storing just goes instead to an external coordination service, something like [[zookeeper]] or [[etcd]] and then you know because that uses consensus, we know that it's reliable and then we can always have a backup or a standby job manager node that in the event the single job manager node goes down, the backup just goes ahead and reads from zookeeper and then takes over its functionality. 
	- Log Based Message Brokers
		- Description
			- Recall: Log Based Message Brokers are partitioned and persistent message queues that keep track of the offset of the last message that a consumer node has seen. Messages are delivered in order to a single consumer
			- Flink uses log based message brokers such as Kafka as its data source.
		- Okay, so just as a quick kind of recall slide, uh let's remember log based message Brokers because this is more or less um what flank is you know very kind of biased that you should be pulling your messages from as opposed to an in-memory non-persistent one and we'll see why in a second. But just as a reminder, log based message Brokers are just any type of distributed message queue that um are going to be partitioned over many nodes and we have them on disk which means that messages can be replayed, they're not instantly going to be deleted unless we need to compact the queue to save space at some point down the line and basically what they do is because these messages are persistent, they will actually have one consumer reading from each partition and they will keep track of the offset that the consumer has last read and this way we can ensure that every message is going to be processed at least once because you know if a consumer were to go down, we can say oh well you know I see that was it offset 4, so if we have another node coming back to eventually read from this partition of the queue, we should start from offset 4 and process all messages after that. So keep in mind um this you know something like this is like Kafka which is um another Apache project where um it's another log based message broker and that's kind of what Flink generally wants you to use. You have a bunch of ingestion options but uh it works very well with Kafka and we'll see why. 
	- Checkpointing
		- Description
			- Job Manager occasionally sends "checkpoint barrier" message to all nodes in cluster
				- Checkpoint barrier delivered in order to all consumers, since it is treated like any other message in the log based queue (which delivers messages in order)
				- Upon processing checkpoint barrier consumers note their current offset in the log based message broker, and persist this along with their local state to distributed storage
					- Using Chandy Lamport distributed snapshots (consistent with causality)
			- If a node crashes, restore all nodes to a checkpointed state, and start processing messages from corresponding offsets
				- May result in certain messages being processed more than once!
			- Snapshots created in the background (non-blocking), considered successful if all nodes report successfully storing local snapshot to job manager
		- Okay, so this kind of all ties in back to checkpointing. So just to remind you, checkpointing is basically where we are going ahead and gathering the local state amongst every single node in the cluster and we're going to go ahead and store that to some sort of distributed file system or maybe an object store like S3. So basically, here's what happens. The job manager does the following. Every once in a while, it will send something known as a checkpoint barrier to all the nodes in the cluster and the checkpoint barrier is basically going to be processed in the same way that any message might be normally. So what that means is that you know once that checkpoint barrier is in the system, it's going to to stay in the proper order, right. It's not going to like jump the gun on other messages, it's not going to fall behind, it's going to be processed in order in the same way that you know a message from a log based message broker might be. So here's what this eventually does is that basically eventually that checkpoint barrier is going to be delivered to all of the nodes in the cluster because it's kind of being sent there and that's the entire point of the job manager and once it does get delivered there, the checkpoint barrier is going to do the following. It's going to say to any node in the cluster, okay give me your local state and also keep track of the last offset of the message that you've kind of processed. So now the job manager is going to be getting back from all of those nodes in the cluster, (a) the last message that it's consumed from uh the message broker and, (B) uh its local state and it's going to persist both of those things to that distributed file system and this is kind of what's going to be comprising of our checkpoint. So basically now if a node crashes, we can restore all these nodes to their previous state from a checkpoint and also start processing messages again from those actual corresponding offsets. We would do this for all of these nodes. So that way we can make sure that every message is going to be processed and even if um the checkpoint didn't encapsulate every single change that had ever been seen by virtue of going back to the corresponding offset for that checkpoint, we can reconstruct the state. It's almost like using um a write ahead log plus checkpoint. It's very similar. So the issue here is that it may result in certain messages being processed more than once. There are a couple ways of dealing with this one of which is just using some sort of idempotent operation. Maybe you have a database that um tells you which kind of message IDs it's already seen and that way you can stop any external services from you know say you have an email client tied into your stream, uh you know stop a duplicate email from being sent. Another option which is slower but does technically work is you using two-phase commit so you do have some options there to ensure exactly once message semantics but generally speaking just as kind of a base thing, messages are potentially able to be processed more than once. Additionally, it's important to note that snapshots are actually created in the background. This is completely asynchronous so creating them doesn't actually stop messages from being processed and they are considered to be successful by that job manager once it receives um a success message from all the nodes reporting that they were able to go ahead and checkpoint their state.
	- Changing Cluster Size
		- Description
			- If we want to change the size of our cluster, we have to redistribute state over the nodes. Similarly to consistent hashing, we want to ensure that as little state as possible is redistributed over the network while ensuring that each node processes a similar amount. To do this, Flink has made an adjustment such that checkpoints are just lists of keys and values (as opposed to individual objects representing the state on each node), so that this way they can be easily re-partitioned using some sort of range schema.
		- Okay, another quick aside that I want to make is actually changing the cluster size in Flank because we know we have all these nodes with their local state and what happens if we either remove a node from the cluster or add a new one? Well, we want to basically rebalance that local state but in the same way that partitioning has worked pretty much throughout the entirety of you know this series and me talking about it, you obviously want to move the minimum amount of key value pairs over the network. It's kind of important to use something like consistent hashing here but basically in order to do this, what Flink does is instead of storing each checkpoint as um just one object representing the state of every single node in the cluster, it breaks down each node state into basically just a list of key value pairs and that way it's really easy to go ahead and repartition them if need be where you can basically load in um part of that checkpoint and that way a new node is going to be able to get its own partition of I guess existing state and then you can start reprocessing messages from there. 
- ![[Screenshot 2024-11-13 at 5.00.23 PM.png]]
	- Conclusion
		- Description
			- Unlike a Spark Streaming, which processes messages in microbatches, Flink is a fully real time stream processing engine that provides the ability to make stateful computations with high availability. In comparison to other stream processing engines like [[Apache Storm]] or [[Samza]], Flink makes life easier for the developer by automatically calculating the dataflow graph, and parallelizing work for you.
		- Okay, in conclusion, um unlike Spark streaming where basically using micro batches, Flink is a real-time stateful stream processing engine. It's able to make sure to use basically Chandy Lamport checkpointing in order to go ahead and ensure that um you're not going to be losing State and in doing so, generally speaking, this should be deterministic processing. Obviously if you're using non-deterministic methods in order to create computations like time, then you're going to have a problem. Um but that being said, Flink is a super useful stream processing engine that does a lot for you. It's kind of got that similar philosophy to Spark where basically you go ahead and write very little code and on the back-end or behind the scenes, it goes ahead and does all that parallelization for you, so you don't have to worry about any race conditions or even partitioning for that matter. Flink just does it all under the hood. So as a result, this is super useful and it is used in a ton of companies in order to process Big Data. Okay, have a good one guys and I'll see you in the next one