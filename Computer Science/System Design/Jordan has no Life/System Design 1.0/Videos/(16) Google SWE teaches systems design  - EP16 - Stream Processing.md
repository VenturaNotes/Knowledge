---
Source:
  - https://www.youtube.com/watch?v=XFAx53P9NWE
---
- ![[Screenshot 2024-09-28 at 5.32.26 PM.png]]
	- hey guys uh today i am back with another video to talk about [[streaming]] um i apologize in advance because this video is gonna be pretty freaking long. um additionally the one other really nice thing about this is that the second we're done covering streaming we've basically covered all like the fundamental / theoretical topics so now from here on out we can either basically look at specific technologies or like even more in-depth algorithms so we're gonna start going from you know systems design noobs to experts after this video. um either way let's get into it like i said it's gonna be a long one 
	- [[Stream Processing]]
	- Background
		- Description
			- Often, we will have lots of data coming in that we want to process asynchronously in the background. Streaming works on an unbounded set of data, as opposed to just a limited set of files/records.
			- Messages are created by a [[producer node]], and eventually handled by at least one [[consumer node]]
			- While these messages can be sent directly from producer to consumer, it is common practice to use a [[message broker]] to buffer these messages in a centralized place.
		- All right well stream processing what is it um as far as the background goes um like batch processing there are often times where we have data and we want to perform some sort of computation on it in the background. however unlike batch processing which takes a bounded set of input files, streaming works on an unbounded set of data. so we go ahead and do that, you know on messages as they come in basically and then we process them asynchronously.
		- Messages are going to be created by some node in our system called a producer that could be a server a client or anything and then eventually it's going to be handled hopefully by at least one consumer node.
		- Then finally even though these messages can be sent directly from producer to consumer via protocols like [[UDP multicast]] or something like that, generally speaking we tend to use an intermediary such as a message broker in order to buffer these messages in some sort of centralized place or queue.
	- Message Brokers
		- Description
			- A type of database that handles streaming, both producers and consumers can connect to it, typically puts messages in a queue if there are many of them.
			- Some will keep the messages durably, others will delete them after they are successfully consumed.
			- Two delivery patterns: fan out (send to all consumers) vs. load balancing (one message per consumer).
			- Producer $\to$ \[m5, m4, m3, m2, m1]
				- Consumer 1
				- Consumer 2
				- Consumer 3
		- so let's talk about those message brokers because that's kind of the most important concept of this video. message brokers are a special type of database for streaming. all they basically do is allow producers and consumers to connect to them. the producers can push messages to them and the consumers can go ahead and automatically pull messages from them. some keep those messages durably which means that even after they've been sent to a consumer they still stay in the message broker and others don't, they just go ahead and delete them. so there are basically two delivery patterns for a message broker.
		- one is fan out which means every message that's in the broker is going to be sent to all consumers that are subscribed to the broker and there's also load balancing which is basically saying in order to maximize throughput and performance, each one of the consumers is going to only handle one message in order to process as many messages as we can at a time.
	- In Order Messages?
		- Description
			- Even though a queue holds the messages in the order they were received, if a consumer crashes/takes a while to handle a message, the [[broker]] may have sent the next message to a different consumer which processed it first!
			- Messages can reliably be delivered in order - but at a performance penalty which we will discuss later.
		- okay what about sending messages in order? even though the messages are represented by a queue which inherently should hold those messages in a first in first out order, if a message takes really long to process or if a consumer fails while handling a message, the message broker generally is not going to wait for an acknowledgement from the consumer that it has properly handled a message and will instead send out the subsequent messages to other consumers. hence unless we take additional precautions on our end which i'll discuss later in the video generally speaking we can assume that messages are not going to be delivered or processed in order.
	- Types of Message Brokers
		- Description
			- In memory
				- Can be done on something like a Redis instance, no persistence
				- Messages that are acknowledged by a consumer are deleted
			- Log based (on disk)
		- okay so there are two main types of message brokers and i'll discuss them both now. the first is in memory so for example if we have something like a redis instance which is basically just a database that allows you to write to memory as opposed to disk, we can just go ahead and make a queue in there and use our messages and you know establish some sort of long polling with a bunch of consumers and then the second that a consumer acknowledges that it's processed a message that'll be deleted. 
		- The other type are log based message brokers and i'll discuss those now but the main point is that the messages are held on disk in some sort of log 
- ![[Screenshot 2024-09-28 at 5.48.26 PM.png]]
	- Log Based message brokers
		- Description
			- Overview
				- Messages sent to an append only log on disk
				- Log can be both partitioned and replicated to improve performance and fault tolerance (partitioning it may mess up order of message delivery)
				- One consumer per partition, keeps track of which messages on the log it has already seen to avoid processing a message twice
					- A single hard to process message stops the rest of the partition from proceeding forward!
		- So what is a log based message broker well basically every single message goes to an append only log on disk which as we know means the writes are going to be pretty fast because we can just you know do them sequentially. additionally that log can actually be partitioned and replicated in order to ensure fault tolerance. obviously we don't want to be losing any messages if our message broker server goes down and in addition the fact that it's partitioned means that we can have multiple consumers reading from each partition and that way we can improve parallelism and performance. generally speaking we do one consumer per partition and the reason to do this is that ensures that all the messages are going to be handled in the proper order per partition so that's important to note which is that you know obviously if we're load balancing our messages between partition via some sort of like consistent hashing method um then uh it's not necessarily all going to be in order but if you know we like smartly distribute the messages per petition uh we might be able to handle those in an order that we want.
		- Another thing to note is the issue with having one consumer per partition is that one long uh message that you know it takes a while to process is gonna slow all the other messages um in that partition down from being processed because they have to wait for the slow one.
	- Log Based message brokers continued
		- Description
			- Adding a message: deciding which partition to send the message to via the use of some function or load balancing method (like consistent hashing) and send it there!
			- Reading a message: consumer is alerted of message (via something like [[long polling]] or [[WebSockets]]), handles it, and on
		- okay so actually representing this visually as you can see we have these two partitions in a log based message broker and as you can see we have a consumer reading each of them and the producer based on some sort of hash function can push a message to either of these partition.
		- so on a right for example let's say we're going to go and write to the top partition and we'll say write "m5". all we're going to really do is use our hash function to decide which partition the message is going to be sent to and then go ahead and append it to that write only log 
		- okay in terms of reading a message as you can see each consumer is keeping track of which index that it is currently read in the log. what does this do? well it shows us which messages have already been handled so if that consumer fails we have the fault tolerance aspect of saying okay we're not going to accidentally reprocess this message so that's really useful but as you can see now that consumer 2 is reading "m7" it can go ahead and say okay now we're on index 2 which means the next message we should be reading is "m8". The second that the message broker receives an acknowledgement by the consumer that it successfully read a message, it's going to increment that index locally thus keeping track of how much the consumers have read
	- Log Based vs. In Memory Message Brokers
		- Description
			- In Memory:
				- Good for when messages take a long time to process and their order does not matter
			- Log Based:
				- Good for when keeping messages around after processing them is useful so that you can potentially relay messages
				- More fault tolerant as they will not lose existing messages on crash
				- Good for ordering message processing within a log partition
		- okay so what's the ultimate comparison between log based message brokers versus in-memory message brokers. well in-memory message brokers are really good in terms of their performance. messages if they take a long time to process obviously you want them to be written to and read from memory because that's just faster writes than disk but in addition when their order doesn't matter and we just want to achieve maximum throughput because say they you know take a while to process, that would in a log-based message broker be a real bottleneck because it means that the partition can't move forward until that message is processed. so if you just want to have a bunch of consumers and achieve really fast throughput in some arbitrary ordering of the messages in memory is probably the way to go for you otherwise if you want to have some durability of your messages maybe you want to be able to replay them later or take a look at them for debugging purposes or even just order them in some sense or another then log based is probably the way to go 
	- Common Uses of Streams
		- Description
			- Logging and metrics
			- Change Data Capture
			- Event Sourcing
		- okay so what are some common usages of streams because right now i've kind of just defined them as some arbitrary things but uh let's give some examples in order to help you know show everyone what they're actually useful for. One way they're hugely important is logging in metrics. When you're logging in an application, we don't really need to use the logs generally speaking instantly or the metrics instantly but it's good to be able to look back on them do some processing with them in order to make you know metrics for say like a certain time window 
- ![[Screenshot 2024-10-02 at 12.21.33 AM.png]]
	- Logging and Metrics
		- Description
			- Often want to be able to aggregate certain events into time windows, such as application logs or certain metrics:
				- Challenging because an event can arrive to a stream any amount of time after the window closes, do we add it to the time window or just leave it be?
			- Putting events in fixed time intervals:
				- [[Tumbling windows]] (non overlapping intervals of fixed length, e.g. every minute starting at 0 seconds)
				- [[Hopping windows]] (overlapping intervals of fixed length, e.g., all 5 minute intervals starting at 0 seconds, can aggregate tumbling window metrics to create hopping windows)
				- [[Sliding windows]] of fixed duration but no set start point can be created by using an in memory buffer of all the events in the window and then removing them once they are outside of the window
		- so logging in metrics. we want to be able to aggregate certain events into time windows such as logs or you know maybe like the load on our servers or something like that and this is challenging for a couple of reasons. well first of all, are we talking about the time that the message was received by the message broker, the time that um the message was sent by the server and it's kind of hard because like i've mentioned in uh previous videos most server clocks are out of sync and as a result of that it's kind of hard to get over the discrepancy of which time to actually use.
			- The processing time, so the time that the event actually happened or the receiving time which is the time that the message broker actually got that event.
			- additionally what if say we want to find all the events that happened from you know one o'clock to two o'clock p.m today and then we know that some event that happened from one o'clock to two o'clock p.m actually only shows up in the message broker at 3 pm. Are we going to add it to our window in retrospect or should we just discard it and accept that we have incomplete data. So you know these are all considerations to make. 
		- In terms of how we might actually put events in a fixed time interval, well i'll talk about three different types of windows.
			- First they're tumbling windows so there's going to be non-overlapping intervals of fixed length so say um every single minute starting at the zeroth second um so like 12:01 uh 12:01 to 12:02, 12:02 to 12:03, 12:03 to 12:04, and they're non-overlapping and they're all one minute long so that's pretty easy to do in terms of you know you just actually go ahead and aggregate all that data and calculate it together.
			- however then there's this thing called hopping windows. Hopping windows are similar but imagine it being like 12 o'clock to 12:05, 12:01 to 12:06, 12:02 to 12:07, so there's overlap between them. So what you should actually be doing there is calculating the tumbling window for every single minute and then aggregating those to create a five minute hopping window.
			- And then finally there's also a concept of a sliding window which doesn't necessarily have a discrete or rather you know one set start point and end point but it's just a sliding window of a fixed duration but it could start it literally any time. All we can really do there is use an in-memory buffer of all the events in the sliding window and then probably have some secondary processor thread that removes events as soon as they're outside of that time window and adds them as soon as they're in the time window.
	- [[Change Data Capture]]
		- Description
			- Writes to a database are then sent to a stream, where they can be consumed by other derived data (such as caches, search indexes, data warehouses) in order to keep them up to date as well.
			- Assuming these logs are persistent, they can be compacted by only holding the most recent value of these keys, useful in the event that new derived databases are added to our system.
		- Okay now we also have this concept of change data capture. So this is a little bit different than uh logging and metrics but basically change data capture is taking the write ahead lock from any single database so imagine we have like a single leader replication so all of our writes are going to one single database. Change data capture is saying take our write ahead log and don't only send it to the other replicas but go ahead and stream it to other sources of data that we also have to keep up like our caches, our search indexes, our data warehouses and as a result we can actually process those write ahead logs and update our other sources of data to make sure that our derived data is up to date. Assuming these logs are persistent, they can also be compacted from you know getting too big assuming we're using like a log base message broker by just only keeping the most recent value of any key that's kind of similar to something we did with uh indexes as well in our first video.
	- [[Event Sourcing]]
		- Description
			- Similar to change data capture, but all events of the user are put in a streamed, append-only log, so that you can derive all sources of data from it.
			- Is more future-proof as it means that you do not have to be stuck with some specific data schema, but can derive many different types of data from it. This comes at the cost that the log cannot be compacted, because everything in it is just an "event", not a database write.
			- Examples include: Jordan clicks the watch button, Jordan is the 40th watcher of Kate Upton's post, Jordan is the 41st watcher of Kate Upton's post, Jordan is the 42nd watcher of Kate Upton's post.
		- And then finally we have event sourcing. Event sourcing at a surface level is really similar to change data capture but the subtle difference is this. Instead of streaming changes made to a database and kind of all of that stream is represented as like hey what key did we change, you know what row, what we're actually going to be doing is streaming the user events. So a [[user event]] might be something like you know Jordan clicks a watch button, Jordan is the 40th watcher of some video. So we're not putting it in database terms but we're putting it in terms of the actual like front-end of the application itself and then what this does is it allows us to derive a ton of different data in basically any way we want. We basically just have to write application code that says here's how you should handle this event for this database and what it does is it doesn't leave us to having to deal with one schema of data but rather we can develop many different schemas of data just because we have all of these events and we can derive a ton of different views of the data from it.
	- [[Stream Joins]]
		- Description
			- Like with batches, there are data associations with streams and we want to reduce the number of network calls made to an actual database. In order to do so, we often have to keep some amount of state local to the stream, representing some other data source. However, we now run the risk that processing certain stream events becomes nondeterministic
				- [[Stream-stream joins]]
				- [[Stream-table joins]]
				- [[Table-table joins]]
		- okay now let's quickly talk about stream joins. If you watch the batch processing episode there are batch processing joins and this all basically comes from the fact that there are associations within the data. So more or less like with batches, we want to be able to do a ton of joins here and there are three types that i'll talk about. So stream-stream joins, stream-table joins and also table-table joins.
- ![[Screenshot 2024-10-02 at 12.33.58 AM.png]]
	- [[Stream-Stream Joins]]
		- Description
			- Joining two different types of events in a stream (such as a search in the search bar, as well as zero or more clicks that occur from a search). Can keep a local index of both types of events on the message broker, and when a new event comes in, check the other index in order to see if there is a join to be made. If events are only valid for a certain window of time, they can be removed from the index after that amount of time
		- so the first type stream-stream joins are basically when we're joining two different types of events in a stream. So the example from DDIA is this and hopefully this will make sense. Imagine i'm Google. Every time you type in the search bar, I want to see for that search which advertisements you've clicked and you might click zero or more advertisements. So i can't just say every single time there's a click, log the search that it came from because then that doesn't show me the searches where there were no clicks. So basically all we can really do is have all of our search events in a stream, have all of our click events in a stream, and then you know try and join them based on this concept of a session ID that probably relates to some sort of, you know, search ID. So what you'll actually do is in each stream, you'll keep an index of all of those searches or clicks for both the search and the click stream and then you'll cross-reference the other streams index every time a new event comes in to try and find an association between the two. This local index is something that we'll see a lot in both the other types of joins for the stream and it all comes down to having to involve some sort of state in our own stream so that we can um go ahead and reduce the number of calls made to a database or another stream.
	- Stream-Table Joins
		- Description
			- Enriching one stream event with data from a database table, possible in order to send to another stream. Keep a local copy of the table in the stream broker in order to avoid having to make network calls to the database for each join. Subscribe to the change data capture of the database table in order to make changes to the local copy of the table in accordance with the actual table
		- okay now we have this concept of stream-table joins where instead of joining two streams together, we have one stream and we're probably trying to enrich an event with some data from a database. So we're going to make a join on you know say a few columns of a database. Obviously it would be very problematic if every single time a stream event came in, we had to make a very expensive network call to a database. So instead what we do is we keep a local copy of the database in the stream so we keep some state there and then every single time that database is updated we use change data capture to keep the local copy of our database and the stream updated. Then we can actually perform the join without having to do any network calls.
	- Table-Table Joins
		- Description
			- Occasionally we might even have to keep multiple local copies of a table up to date, in order to perform joins on them. In order to do this, we have to subscribe to multiple sources of change data capture. Effectively, the result of the joined streams maintains a cache of the actual SQL query of the two tables
		- okay finally we have table-table joins. This is basically saying um you know imagine that we have two tables where um every single time where there's a change to them, we frequently want to um go ahead and take the joint of them and keep that join in like a kind of cached way where you know say we have like a tweets and followers table and we want to see for each tweet which followers to send it out to. So every time there's a new tweet, we want to find all the followers for it. So a table-table join can be done by basically having two subscribers for the change data capture of each table and when that change data comes in, you go ahead and perform the join on local copies of a stream. I know I probably didn't explain that in the best way but hopefully it makes sense 
	- Fault Tolerance
		- Description
			- Want to ensure that each message is processed exactly once (no less, no more):
				- Every message processed at least once
					- Occasionally checkpoint stream state to disk to restart from most recent state on crash
					- Can also run microbatches, where you let a few stream events accumulate and then run a batch workload on them
				- Messages not processed more than once
					- Can use idempotence, which is some way of keeping track of seen messages (usually via a unique message ID) to ensure that they are not processed multiple times
					- Could also use atomic transactions (see two phase commit), which are slower
		- okay and then finally i know this keeps going but we're almost done. Uh fault tolerance. So we want to ensure and this is pretty important that each message is processed exactly once and that means not zero times, not two times, not three times, only once. So in order ensure that each message is processed at least once, what can we do? Well we can occasionally checkpoint the stream state to disk. So this is especially relevant for in memory streams um so that you know if something were to crash, we still have those messages that are durable and they still get processed at least once. Additionally, if we have a bunch of messages in memory, we can run them as micro batches so that basically means like you run i don't know say 10 stream events as a batch job and if you recall from the batch job video, batch jobs are really easy to restart if they go wrong and that way we can ensure that every single message is going to be processed at least once. In order to make sure however that messages are not processed more than once, we have a couple of options, we could use atomic transactions. So if you remember two-phase commit, it basically involves a coordinator node making sure that in this case, it would be that the message brokers both sends the message to the consumer and the consumer also processes it at the same time so that's what two-phase commit would do here. However that's really expensive and it also isn't very fault tolerant because of a coordinator node, so a lot of people prefer to use instead is this concept of idempotence, which means that you know we only allow an operation that occurs to have an effect one time, so you know if it happens more than once, there will be no additional effect and a way that this can be done for you know a lot of streaming applications is you use some sort of item potency key, which is basically just a unique user id, and then you have some sort of database or system that checks if the user id has been seen before and if it has, you don't process the message again and if it hasn't you can go ahead and process it.
- ![[Screenshot 2024-10-02 at 12.41.35 AM.png]]
	- Streaming Conclusion
		- Description
			- Streaming is becoming incredibly important as companies take in increasing amounts of data which can be processed in the background as opposed to instantly - we will see it pop up frequently in systems design questions
			- Recall log based streams optimize for persistence and ordering, whereas in memory streams optimize for speed and are better suited for tasks that take a long time to execute
			- Often times, there is a need to enrich or cross reference stream data with other data which we have shown how to do via joins.
			- Finally, concepts such as event sourcing and change data capture offer promising options for keeping derived data up to date in an eventually consistent way.
		- okay in conclusion, streaming is becoming pretty important because companies have tons of data that they need to process in the background and oftentimes it's not necessarily like a batch job where it makes sense to process it for a discrete time interval but rather it's just better if you process it as it comes in. Um log base streams versus uh in memory based streams have their advantages and disadvantages where the log base streams you know optimize for durability and ordering while the in-memory streams just optimize for throughput basically and try and uh get all of those messages out of the memory. Oftentimes there is a need to do some sort of joining or enriching of stream data and as you can see we've covered three methods of stream joins and that's how you would go ahead and do that. And then finally event sourcing and change data capture offer interesting ways to kind of keep up your derived data whether that's with a search index or a cache or some other data type. okay overall yeah guys last video of just like the pretty basic concepts to cover. From here on out, we're going to be going pretty in depth and personally i greatly look forward to that so uh, I'll see you next time.