---
Source:
  - https://www.youtube.com/watch?v=wJcG1Rzr8i4
Reviewed: false
---
- ![[Screenshot 2024-11-12 at 7.47.03 PM.png]]
	- Introduction
		- Today's video is about load balancing which is a topic that is near and dear to my heart. The reason for this is that i've had some trouble balancing my load in the past, you know, gave it all too much to one person and i should have just been spreading it out you know. So obviously [[load balancing]] has its real life applications too but uh let's go ahead and talk about it and uh we'll figure out how to best do it. 
	- Load Balancers (Background)
		- Description
			- As systems scale, there reaches a point where you can no longer use more powerful hardware (vertical scaling) and have to use more nodes (horizontal scaling). We have first seen this with databases via partitioning for increasing storage, as well as replication for increasing read (and sometimes even write) throughput. However, databases are not the only thing being horizontally scaled, application servers can be too!
			- Any time that there are a variety of nodes in a cluster that can serve the same requests, using something called a load balancer can help to ensure that none of the servers become overwhelmed. 
		- Alrighty, let's get started, so load balancing. What is it? Well basically in any massively distributed system, there gets to be a point where you can't just build better hardware and use that instead because you just have so many users and that's known as vertical scaling. Instead you have to use an alternative method of scaling out your application which is basically just adding more nodes to your system. That's called horizontal scaling. We've seen this so far with databases in terms of both partitioning to store more data as well as replication in order to kind of increase the throughput of your data by adding a bunch of copies of it. That being said not only are databases something that you can horizontally scale but also your actual application servers. If you have a ton of users, they're just going to be hammering one application server and it's not going to be able to fulfill every request plus in something known as the microservices architecture where basically individual teams in your company are building out their own services, you tend to just have a bunch of different application servers anyway kind of running their own functions and you need something to go ahead and properly route requests. Anyways the point is that load balancing is something that is going to take all of your incoming requests and make sure that amongst your horizontally scaled application servers or even databases, they're not getting overwhelmed by them. So let's go ahead and talk about how those actually work. 
	- Load balancers
		- Description
			- Load balancers are a type of reverse proxy, that can sit between layers of an application 
		- Basically as you can see here in the following diagram, you can have load balancers sitting between a client device such as a mobile phone or a laptop and your application servers and also between your application servers and your database. 
	- Load Balancing Overview
		- Description
			- Locally keep state of the health of each server via heartbeats
				- Number of open connections
				- Load
				- Time of last heartbeat
				- Done locally as opposed to in a coordination service to speed up routing
			- Route traffic to each server based on some sort of routing policies
		- Okay, so how does load balancing actually work? Well it's something like this. Basically every single you know unit of time, load balancers exchange heartbeats with your application servers or maybe even your databases to just get an idea of how many open connections they have, if they're experiencing low or high load, the time of the last heartbeat you know they're keeping track of this in some sort of table on disk probably and then also this is done locally as opposed to something like an external coordination service because if this was done in an external coordination service, then you would have to go ahead and query that every single time and if you're hitting the load balancer every single request that you're making to your servers, that would be a huge performance hit. Additionally then you're going to kind of take all these metrics and route traffic to each server based on some sort of routing policy which i'll discuss now. 
	- Routing Policies
		- Description
			- Least connections
				- Traffic routed to server with fewest active open TCP connections
			- Least response time
				- Traffic routed to server with lowest average response time
			- (Weighted) round robin
				- Create ordering of the servers, send each subsequent request in order to next server
				- Can be weighted by compute capacity of the servers
			- Hashing
				- Based on the contents of the request
		- So here's some possible uh basically possible ways of going ahead and deciding which server you're going to send each request to. One of them is just least connections, you just go ahead and look at which servers have a bunch of open TCP connections and you send them to ones that don't. Least response time, the load balancer can actually just go ahead and keep track of the average amount of time it's weighted for responses from each server and then use that as kind of a heuristic in order to decide which server is actually going to be fastest or has the least load currently on it. There's round robin which is basically deciding and ordering of the application servers and then making sure that each request goes to every node in that order once before restarting where you send those requests to and then there's a variation of that called weighted round robin which is basically just taking into consideration the actual computing capacity of each server so you know if one server is doubly you know more computationally powerful than another, it'll probably see two requests um while you know the the half is powerful server sees one in one of those round robin iterations and then there's also hashing which is what we've discussed quite a bit in the past with um partitioning and that's basically where you take the contents of the request and you go ahead and send it to a given application server. So i'm going to expand on that because i was kind of purposely vague there. 
- ![[Screenshot 2024-11-12 at 8.02.54 PM.png]]
	- Layer 4 vs. Layer 7 Load Balancing
		- Description
			- Layer 4:
				- Requests are routed based on the details exposed by the network transport layer
					- Source IP address, destination IP address, ports in the header
					- Computationally faster
			- Layer 7:
				- Requests are routed based on header contents
					- More flexibility in routing, can hash on more relevant information
		- So what can you actually hash on? Well there's this concept of layer 4 versus layer 7 load balancing where in layer 4 requests are routed based on kind of the low level network transport layer so that gives you things like the IP address of both the source and destination and also ports in the header. Since you have less information to potentially consider or unpack, this is going to be computationally faster. It requires less resources. Layer 7 on the other hand actually lets you look in the contents of the packet that you're sending which means that requests are going to be routed based on those. This provides you with more flexibility and allows you to do kind of like smarter routing which i'll discuss in a second. You can hash on more relevant information. 
	- Consistent Hashing
		- Description
			- Recall: Consistent hashing is an algorithm that distributes items into buckets in a way such that adding/removing buckets only shifts around the hash of a small portion of the items
			- This is extremely useful for loading balancing, as routing the same/similar requests to the same nodes allows nodes to locally cache certain results, thus reducing the amount of network calls/computation needed
		- So this kind of comes into, well, what are we doing as far as hashing goes? Are we just using a modulus where we take you know the hash of our request ID for example and then do the modulus of the number of servers that there are? Well no you probably shouldn't do that because we have consistent hashing which i've talked about in a previous video. So recall that consistent hashing is basically just an algorithm that goes ahead and distributes a bunch of items into a variety of buckets and the reason it's so useful is that when buckets are added or removed, consistent hashing ensures that the minimum number of items can be distributed from each bucket or moved in a manner that um you know there's very little rebalancing. So why is this useful for load balancing? Well it's not inherently the same thing as partitioning, right? Because in partitioning if you rebalance all the keys, that's a ton of strain on the network. There's no extra strain on the network to actually send requests to different server nodes, however, if you are utilizing local cache on each server node, then being able to put similar requests on the same server is super useful because you might be able to cache certain results such as images or database calls or even the result of certain computations and as a result of that, you can greatly speed things up. So that's why it's important to use consistent hashing in this case as opposed to just any sort of normal hashing algorithm with a modulus. 
	- Availability
		- Description
			- Up until this point, we have acted like there is only one load balancer, which would act as a single point of failure - if the load balancer went down, we would not be able to serve any requests
			- Can either run two machines with some shared network attached storage for configuration, both acting as load balancers (active-active), or have one machine running as a load balancer with a second that occasionally exchanges heartbeats with it and takes over if it is down (active-passive).
		- Okay, let's also talk about availability. So if you remember my diagram from earlier in the video, I kind of just acted as if there was only one load balancer and that's pretty naive if you do that because it acts as a single point of failure and in any systems design interview, if you see that there's just you know one of something, it's probably a single point of failure and you should probably talk about that. So, obviously we need to figure out some sort of way of keeping things in check. As i mentioned earlier, coordination services are a little bit less feasible in something like a load balancer simply just because it requires too many network calls. I suppose you could use a coordination service just between multiple load balancers in order to keep track of which ones are alive and which ones aren't through heartbeats and then you know promote a backup to be the primary. However generally speaking it seems like there are two ways of people doing high availability load balancing and they're known as active-active and active-passive. Active-active load balancing is where you basically have multiple load balancers running at once all routing requests at the same time and they kind of have to share the same configuration state probably through something like network attached storage. That basically means though however that if both these guys are running at close to full capacity, if one of them goes down, the other load balancer might be pretty overloaded. We also have this concept of active-passive where basically we have one machine running as the only active load balancer and then a second one just sitting there sharing the same configuration state and basically just waiting, sending heartbeats between the other load balancer and it making sure that the second load balancer if it goes down that the passive one can take over. And you know, I imagine that there would be some sort of methodology to kind of ensure against something like split brain. Maybe by using like an epoch number or a fencing token so they can you know communicate with one another to an extent. 
	- Conclusion
		- Description
			- Load balancers are a relatively essential part of applications at scale, and in reality they often perform even more functionality than just load balancing - doing things like encryption/decryption, and acting as a reverse proxy.
				- Pros:
					- make sure that your various servers do not become overwhelmed by incoming requests
					- Maximize the local cache performance via consistent hashing
				- Cons:
					- Can become a performance bottleneck
					- Adds more complexity to the system
		- Okay, so in conclusion, um you know obviously there are pros and cons to load balancing but the truth of the matter is you're going to use a load balancer any single time you have a good amount of application servers. It's kind of a necessity and as a result, they you know are pretty much in every single huge application at scale and oftentimes they even perform more than just load balancing. Sometimes they act as a reverse proxy which basically means they're the only public-facing part of your application that users can access over the internet and then everything else is kind of in private behind a firewall and all of your servers can only talk to one another and then the load balancer is kind of like the gateway to all of that. Additionally, sometimes they do things like encryption/decryption of you know requests to make sure the data can't just be you know stolen or something or snooped and then in terms of just the general pros and cons of load balancing, obviously the biggest pros are that it makes sure that your servers don't get overwhelmed as well as maximizing local cache performance via that consistent hashing. You just want to be able to send the same request to the same servers in order to cache as much as possible where the biggest cons are basically if your load balancer is slow for some reason and every request is going through it, that can become a huge performance bottleneck. And then additionally, obviously any single time you add a component to your system, it's going to add additional complexity that you have to think about such as you know what happens when this component fails. But ultimately, I hope this video was helpful guys. I certainly could have used a load balancer in my life. Now you guys know how to use them too