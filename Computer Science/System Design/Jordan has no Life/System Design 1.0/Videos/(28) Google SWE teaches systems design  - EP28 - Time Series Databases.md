---
Source:
  - https://www.youtube.com/watch?v=_pZ7VNO1UDs
---
- ![[Screenshot 2024-11-10 at 11.16.48 PM.png]]
	- Introduction
		- All righty, I am back for another video. Today we're going to be talking about time series databases which shouldn't take too long. But um yeah just generally the points I'm going to make in this video, I'm not going to talk about any specific time series databases simply just because I feel like in an interview, no interviewer is ever just going to be like yeah so um tell me about how time scale DB works. I mean that would be like way too specific because it's not that popular. Um but I did abstract um you know a couple of designed decisions away from a few different time series databases so what I might say doesn't necessarily apply to every single time series database but they did seem to be good ideas that are in use in uh multiple different types of them and as a result that's kind of what I'm going to be covering in this video. So anyways, let's get into that. 
	- Time Series Databases (Background)
		- Description
			- In many applications, there is a pattern of "write-once, read many times" data inserts corresponding to a range of time - things like logs, sensors, or any other consistent data being ingested from a stream follows this rule.
			- As a result, many databases specifically tailored for this use case, such as [[InfluxDB]], [[TimescaleDB]], and [[Apache Druid]] have been created. They allow both great throughput for reads and writes on time series data
		- All righty, so time series databases. Well what are they? In most applications or most companies at one point or another you're going to have to probably store some sort of Time series data. This happens when you have logs from a bunch of servers, sensors or any other type of you know data that's coming in at a consistent type of stream um over a time interval. Uh as a result, a bunch of databases have actually been created that are specifically tailored towards this type of application. Um we'll go over kind of the use cases of you know what you might typically be doing when dealing with time series data in the first place, but um, overall, it's obviously good to use a specialized tool for certain types of data whenever you can and as a result, that's why these types of databases popped up. So they're really good for high read and write throughput for specifically ordered time series data and we'll talk about why they work and you know kind of the design decisions some of the creators have made in order to kind of follow through with that promise.
	- Time Series Operations
		- Description
			- In order to optimize for time series data, let's consider the access patterns used by applications that might be dealing with it:
				- Writes
					- Primarily inserts to a recent time interval, adjacent values likely similar
					- Generally using a compound index like a timestamp in conjunction with some data source ID
				- Reads
					- Generally from the same timestamp/data source combo over one column of data
					- All from relatively small time interval (probably the most recent one)
				- Deletes
					- Common scenario is to delete time data older than some date prior to the present
		- So what are some time series operations? Well, in order to optimize for the data, we should definitely consider the access patterns used in order to basically make sure that all of those access patterns are kind of being handled the best by our design. Okay so writes. Generally speaking the writes are going to go to a recent time interval. You know like you're not going to get sensor readings that are like 7 days late. You might get some that are a few minutes late, due to network delays, but generally speaking you're just writing things once not updating them and you're inserting them generally towards the end of a time interval and then additionally keep in mind that the adjacent values that you're inserting from row to row are probably going to be pretty similar. If it's something like a sensor reading for example uh, one, the time stamps are going to be pretty similar and two whatever values you're sharing probably didn't change that much in the you know the small unit of time so as a result um we have a bunch of similar values next to one another. Additionally um by using a compound index like timestamp in conjunction with some sort of data source ID, we can kind of Express you know all of the metrics that we're getting from a bunch of possible different data sources and still be able to cover all of those time intervals. Um in terms of you know reads and kind of the access patterns there, generally speaking, um, it's going to be from that same timestamp data source combination, so just a tuple of those, but just over one column of data. So maybe we have a sensor that makes you know four readings per timestamp. Generally speaking on graphs, we're probably only going to be using one of those columns of data. Additionally, they're all probably from a relatively small time interval, uh you know you're not looking at years of data generally speaking, it's probably only you know hours, days, or weeks. Then in terms of deletes, the the most common scenario is to you know take a bunch of your older time series data, say older than 6 months, and just start getting rid of that. You know it's not always the case that people will hold this analytics data for such a long time.
	- Optimizing Writes
		- Description
			- Should likely build a compound index first sorted on source ID, and then within that sorted on timestamp
				- This allows writes from the same data source over similar intervals of time to be on the same node
		- Okay, so how would we go about optimizing those writes?  Well the first thing is I kind of mentioned that um it's very important that things are sorted both on that timestamp value and also on the kind of source ID where the source might be either like you know the server producing the logs or the sensor producing a bunch of metrics but the point is um time series data from the same Source should be grouped together and it should probably be ordered by the timestamp itself. So we now kind of have this tuple that um provides like a very natural compound index for us and so this way, when we have this, we allow writes from the same data source over similar intervals of time to be on the same node, assuming we're doing, you know, say some type of sharding that way and you know if we're sharding over multiple nodes or even within one node, as long as we're keeping those writes together, they should be relatively quick. 
	- Optimizing Reads
		- Description
			- Storing data in a column oriented manner is much faster
				- Generally speaking all timestamps should be sequential and similar and should be able to be compressed a ton
				- The same applies for certain metrics which are similar in value to one another as time progresses
				- Additionally, having all column values in one file reduces disk I/O and allows faster aggregations of column data, generally we only need one or two columns at a time for graphing
		- Um in terms of optimizing reads. (a) storing data in a column oriented storage type is actually going to be great because like I said generally speaking you want to read probably one column of data at a time and that means that um in terms of performing aggregations on the data uh having column oriented storage makes that really easy because all the data is stored together in one file, it reduces a lot of disk I/O. Additionally since all those values are going to be so similar over you know the duration of one column, we can do a ton of encoding on it. So that might be run length or bit map encoding but the point is, you know, whatever Library a Time series database does use, it can greatly reduce the amount of storage that you need to be using by virtue of using this compression. 
- ![[Screenshot 2024-11-10 at 11.33.22 PM.png]]
	- Optimizing Reads Continued
		- Description
			- Even within one node, it is better to treat each (source, time interval) tuple as its own chunk table (abstract these away via a large table)
				- Since most writes are going to only a couple of these, we can achieve much better performance by caching their entire index in memory
				- Otherwise, we would have many more relevant index pages that would occasionally have to be swapped in and out of memory which incurs significant processing overhead
		- Um okay and then in terms of optimizing reads further, uh this is something that I saw that um was kind of unique to time series data and I haven't seen it before and so I'm going to go into a little bit of depth on it. So imagine that we have uh as you can see in the bottom right here we have this [[Hypertable]]. So basically what a few of these databases do is they represent all that time series data on one node as one huge table where that table represents both you know uh timestamps and the source ID and the point is it's kind of treated as if it were this one huge table which might insinuate that for that entire table, there's one continuous Index right? Well actually, instead what these databases have chosen to do are abstract away all of these things into the one huge table which they call a hypertable, but really the hypertable is made out of all these mini chunk tables where chunk tables are a combination of some sort of source and time interval tuple. So as you can see on the bottom here I have these four chunks and each one is just kind of a combination of a time interval plus a you know a sensor ID. And so as a result of that there are some huge benefits to this type of design schema. First of all since most writes are only going to actually access a couple of these at a time because like I said, we're um kind of modifying things that are only recent. We can achieve much better performance by caching the entire index of only the relevant chunks in memory. For example, if we had the entire hypertable and we didn't have you know smaller individual indexes, what we would have to end up doing is have this entire huge B-tree or all of these SSTable files and then we would have too much index information to potentially have in memory at once and we would constantly be swapping in page files from disk in and out to memory to you know access them and change them and as a result that would add a ton of overhead in terms of doing that disc to memory swap. So as a result, by creating all of these sets of smaller indexes, we can only keep the relevant small indexes in memory and that hugely speeds up performance. 
	- Optimizing Deletes
		- Description
			- By breaking the main table into many smaller chunk tables, we also greatly optimize on the performance of deletes
				- If using an LSM tree based architecture, each delete of data is treated the same way as a write, and requires adding a tombstone to index files
				- If instead we want to just delete a bunch of old data all at once, we can just delete the corresponding chunk tables/index, as opposed to actually writing all of the deletes to index files and waiting for compaction
				- The same applies to B-trees, where we can just delete the appropriate index as opposed to going through the B-tree many times and setting a bunch of pointers in it to null
		- Additionally, having uh this chunking design as opposed to just one huge table really helps by optimizing deletes. Like I said, it's a very common use case that you kind of just want to take a ton of old data and just wipe it out because you no longer need it anymore, it's kind of past that time threshold that it's relevant. So by breaking the main table into all these chunks, we can hugely improve the speed of that exact operation. Why? Well if think about like an LSM tree which we've spoken about a lot in the past. Each delete of data is its own write which means it has to go into that in-memory buffer and then you eventually add a tombstone to an SSTable file which exists there until it's compacted and then finally that key is deleted. But if we're writing a ton of deletes, that's going to be super inefficient. Instead, it would just be better if we actually just deleted the entire index and just dropped the file as a whole. So that's what we can do here here by using these chunks. You literally just delete the chunk which is great. And then the same thing applies to b-trees. If we were doing a ton of deletes, every single time we deleted um a key value pair we would have to go ahead and traverse through that b-tree table and actually get rid of the key-value or just you know set the pointer to it to null, whereas now we can just go ahead and delete that entire index. So this is something that um becomes much faster and it's actually a pretty common thing that happens in Time series databases. 
	- Conclusion
		- Description
			- While storing time series data is a relatively niche topic, it turns out that using a database specifically made for it can allow for huge gains in performance.
			- Ultimately, they do so by:
				- Creating separate indexes for each data source and time interval combination
				- Column oriented storage both for fast aggregations as well as compression to reduce space significantly
			- While not every time series database works identically, it seems like across them, these are the main features that allow for quick ingestion and processing of such types of data!
		- Okay, so I know this was a short video, but um in conclusion, even though storing time series data is not something that you know every application has to do all the time, if you are dealing with time series data, using a Time series specific database is a really good way to go. Um between creating separate indexes for each data source and time interval, you can take advantage of really good cache performance as well as the ability to quickly delete things and also you know just having a smaller index which makes things really easy to write to because you can just write to the cache and then Additionally the fact that all that data is so similar means that using column oriented storage can greatly reduce the amount of storage space that you're going to need as well as making it really easy to do aggregations over some sort of data. So even though not every time series database is identical, these are generally the main features of them which allow them to handle this type of data so well and so quickly. So as a result, you know if it comes up in an interview that you know you have some sort of logging type of data, then you should definitely consider a Time series database. Like I said, those chunks are making a huge difference and you know it's kind of unique because it's the first time uh that we really see partitioning within a single node other than kind of using that fix size partition schema that I've spoken about a bit in the past, this is actually a way of using adaptive size chunks on a single node and then obviously if you were to scale out your time series database in a distributed manner over multiple nodes, then you can probably imagine that one way or another those chunks would be put on different machines and you know hashed somehow