---
Source:
  - https://www.youtube.com/watch?v=4gLD2pKxo20
---
- ![[Screenshot 2024-09-24 at 2.40.19 PM.png]]
	- Finally talking about consensus algorithms. This is when things get juicy. So let's talk about the first and worst two phase commit
	- [[Two Phase Commit]]
		- Description
			- An algorithm that achieves consensus - also helps solve atomic commit!
			- [[Atomic commit]]: If we want distributed transactions that touch multiple partitions, we need each write to either succeed or fail. This is easy enough to do on a single machine, but when the transaction spans multiple nodes connected via a network, they need to all agree on whether the transaction is committed or aborted (consensus)
		- Two Phase commit is a pretty simple consensus algorithm that is going to help us solve atomic commit which is basically allowing us to do distributed transactions which basically go through on every node that involves them or they all get reverted on every node that involves them.
		- So to explain that in more detail on what atomic commit is. Let's say we want a transaction that has writes to multiple partitions. So we have a profile partition and let's talk about LinkedIn. Each of the job listings is going to be put into its own partition and I'm writing multiple job listings at once and so they're touching multiple partitions and I want those changes to either go through on all the partitions or be reverted on all the partitions. In order to do this, we need them all to succeed or all to fail and that's hard to coordinate because they are not just on a single machine, they're over a network
	- Why do we need atomic commit?
		- Description
			- To keep things from getting out of sync due to partially completed transactions:
				- Cross partition transactions
				- Global secondary indexes
				- Keeping other derived data consistent like data warehouses or caches
		- So why do we need atomic commit? I mentioned one case which was just like a partition database but in addition to that if you recall something like global secondary indexes where basically every single time you write to that secondary index, the write has to also kept in sync on another node which holds the relevant part of the secondary index. That's a case where atomic commit is important.
		- In addition, if you want to keep other derived data consistent, like a data warehouse or a cache, then additionally we need atomic commit. You might even think about a case where say I am going to send an email out every single time I have this write and you want it to be the case that an email is only sent when the write is successful. Otherwise, let's say an email gets sent to the, the write to the database is not successful. Well, we have to retry that write to the database and now we're going to be sending two emails which is annoying
	- Two Phase Commit
		- Description
			- Coordinator
			- Partition 1
			- Partition 2
			- (1) The database partitions have received the request to write, 2PC will now begin
			- (2) The coordinator sends out a prepare request to all nodes:
				- (a) Waits for a response from each node
				- (b) If not every partition responds with an "OK", send result of "abort" to all nodes
				- (c) If all partitions respond with "OK", send result of "commit" to all nodes
		- Let's talk about two phase commit. Basically there are there components. Well three components in this graph we're about to see. There's this concept of the coordinator node and there are going to be 2 database partitions. 2 database nodes. The coordinator node is not necessarily its own computer. It might just be software running on one of the database partitions. But effectively, the coordinator node acts as a single leader in this sense and so that's why I'm having it as its own area in the diagram
		- Let's imagine we want to make a write that has to go to both partitions. So after the database partitions have received the data that they need to write, this is where 2PC actually begins. The coordinator node is first going to send a prepare request to both of the partitions. The prepare request is going to be responded to so the partitions are going to send the result of that. Lets say they both say okay. What does it mean when you say okay. It means that each of the partitions is going to say "I am ready to actually commit this transaction as soon as the coordinator node tells me to do so". Once a partition says okay, I'm prepared to do this, it means that it is saying that no matter what happens, disk failures, the partition just loses power, network failures, eventually its going to commit that transaction
		- So what actually happens after all of these okays are received by the coordinator? If not every single partition responds with an okay, what the coordinator is going to do is send a result network call to every single one of the nodes which is going to say abort. So every single partition is going to abort the transaction. If every partition basically says okay, I'm prepared to commit this no matter what, I don't have any disk issues, uniqueness constraints, other types of errors. The coordinator node is going to take the amalgamation of all of those okays form the prepared and actually tell all the partitions to go ahead and commit that transaction.
		- So pretty simple but lets go and explain why this works
	- Why two phase commit works
		- Description
			- Each transaction has a unique ID from the single coordinator node
			- Each node executes its own local transaction per distributed transaction
				- This gives ACID properties like local atomicity and serializability
			- When a node responds OK to the prepare phase, it says that it will guaranteed be able to commit said transaction under any circumstances once it receives word to do so
			- Coordinator node has internal log of whether each transaction should be committed or aborted in the event that it fails before sending result
				- Known as the commit point, after receiving prepare responses, coordinator must retry forever to commit or abort transaction on all nodes until they all respond with OK
		- For starters, since we have only a single coordinator node, each transaction has a monotonically increasing unique transaction ID. So what that means is that you know we don't have to potentially mix two transactions up. We can just easily identify them. Each node is going to execute its own local transaction just like a transaction we've discussed in the past. What this means is that it has ACID properties. So we don't really have to worry about any concurrency bugs there and in addition to that, we know that either all of the writes on an individual node are going to succeed or fail themselves. Okay
		- Next, when a node responds okay to the prepared phase, this is known as the commit point. It's basically saying we are going to go ahead and commit this at some time in the future and we're ready to do so until we hear the word. Nothing that happens including other transactions is going to stop that node from being able to eventually commit this transaction. Okay
		- Furthermore, the coordinator node just for fault tolerance has an internal log of whether each transaction should be committed or aborted. So let's say the coordinator node receives word from all of the participant nodes. This is the database nodes. The coordinator node receives from all the participants that they're all good and prepared to commit this transaction before it actually goes and tells them all to commit or abort. It's going to put that in its write ahead so that if the coordinator node crashes, when it comes back to power, it can go ahead and make sure that that actually goes through. That adds a little bit of fault tolerance
		- The thing is though, there are a few issues with two phase commit
- ![[Screenshot 2024-09-24 at 3.00.48 PM.png]]
	- Issues with Two Phase Commit
		- Description
			- 2PC is not fault tolerant:
				- If a partition fails after the prepare phase, the coordinator must try forever to reach it
				- If the single coordinator node fails, everything breaks down
					- System is completely blocked, nodes cannot abort a transaction as other ones may have committed it, there is no way to know
					- "In doubt" nodes holding onto locks for relevant rows, prevents reading them (recall two phase locking)
		- Well first of all and kind of the biggest thing is that 2PC is not fault tolerant. If a partition fails after the prepare phase, the coordinator needs to try literally forever to reach it. Let's say the coordinator wants to hit partition one but partition one crashes before it can tell it to commit the transaction. The coordinator is just gonna keep sending messages over and over and over to partition 1 until it comes back up and can eventually commit that transaction.
		- While that's bad enough in the sense that you can't really proceed too much further there (could start doing some other transactions maybe). The bigger issue is that if the coordinator node fails, it's a single point of failure. There is no fault tolerance there. If that goes down, everything goes down. No writes can actually be made. The system is completely blocked and the nodes can't know if the coordinator node is down. It's unclear if it's actually down or the network connection is just buggy. In addition to that, they can't abort that transaction locally that they've already committed if half of the partitions have committed the transaction but the other half haven't because the coordinator node failed before it could actually tell them to do so. The issue there is that the ones that have already committed the transaction can't just roll it back because of the fact that they don't know the other nodes are going to roll back their transaction. There is no consensus between them to roll it back since the coordinator is down
		- A further big problem is that these in doubt nodes, ones that have already basically said I'm going to go ahead and commit this transaction but the coordinator node fails on them, they're going to be holding on to locks for these relevant rows that they're going to be touching and if you remember two-phase locking which is how these databases implement serializability or the traditional way that most of them do, what they actually do is claim an exclusive lock on these rows whenever they want to write and that means not only can other transactions not write to these rows now, it often means that other transactions can't even read from these rows so you're really slowing down the whole system
	- Heterogenous vs. Database Internal Transactions
		- Description
			- [[Heterogenous transactions]]: 2PC running on different types of software (using an API called XA) - since XA needs to work on many different types of storage systems, it cannot be very optimized for performance
			- [[Database internal transactions]]: 2PC running on the same type of software, can be significantly optimized to do things like run serializable snapshot isolation or detect deadlocks amongst nodes, still it is the case that all nodes need to respond for transactions to be committed or aborted
		- Just as a little bit of a digression or a tangent. I'm going to quickly touch upon heterogenous vs. database internal transactions. Even though we said so far that 2PC is not that feasible between having to coordinate with every node over the network and in addition to that you know just having no fault tolerance, it actually turns out that 2PC has been used in practice pretty well. The reason for that is that there's the difference between these heterogenous and database internal transactions. So heterogenous transactions is basically 2PC but running on a really basic library called XA or API rather that needs to work on basically every single type of storage system that is persistent. What this means is that we can't really make optimizations on the hardware because you don't exactly know, there's no protocol basically to talk about the locks on certain rows amongst different types of databases. However, amongst one type of database software, aka, a database internal transaction, 2PC can actually be really efficient in the sense that you can actually share more information from node to node since you can actually put it in the software itself for the database. As a result, instead of using something like two phase locking which would be shutting down all of the nodes for reads where a transaction might be grabbing the locks, you could use something like serializable snapshot isolation which is an optimistic concurrency control type and as a result, it won't be as blocking
		- So as you can see in times where you're not doing XA and instead using database internal 2PC transactions, that can actually be feasible.
	- Two Phase Commit Summary
		- Description
			- Used for distributed atomic transaction, by using a single coordinator node to:
				- Ask all nodes if they are able to commit a transaction
				- If so, inform all nodes that they should go ahead and commit it
			- This is nice, but 2PC lacks fault tolerance! If the coordinator node is down, the whole system cannot proceed - if a participant node is down, the transaction cannot be committed or aborted until the coordinator node can reach it.
			- Next, we will examine some consensus algorithms that actually display a degrees of fault tolerance!
		- To quickly summarize, 2PC is basically a protocol used for distributed atomic transactions and it's also a form of consensus where all you're doing is basically going to all the nodes from one coordinator node and saying, are you guys ready to commit this transaction and if they all respond and say yes we can do that, you go ahead and tell them to do so. If one of those nodes that is going to commit the transaction goes down, you have to keep reaching out to it indefinitely. If the coordinator node goes down, you're really screwed. As a result, it lacks fault tolerance. In subsequent videos though, we're actually going to be talking about much better consensus algorithms such as [[Paxos]] or [[Raft]] or [[Zab]]
		- All of these are able to use some kind of fault tolerance. If you remember from the leaderless replication video, has very similar logic to quorums which is you use a majority of nodes to determine consensus instead of having to reach out to every single node. That way you can determine what the truth is without having to have every single node running. 
		- I hope you enjoyed this video. 2PC isn't too complicated but these next consensus algorithms are significantly harder to understand and as a result of that, there will be some pretty long videos and a few days to compile all this information and post about it. Hope it helps. Have a good one