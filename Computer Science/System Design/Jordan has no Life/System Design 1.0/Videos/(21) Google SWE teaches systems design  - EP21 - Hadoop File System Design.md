---
Source:
  - https://www.youtube.com/watch?v=3-_652dKCIY
---
- ![[Screenshot 2024-10-04 at 12.43.21 PM.png]]
	- Intro
		- All right i'm back again this time in the morning because my roommate's not here. If you guys can tell my voice is a little messed up. I guess i was playing with the boys a little too late last night and for some reason my knees are a little scratched up too i don't really get that one but who knows. So anyways today we're going to talk about the architecture of [[HDFS]] and figure out why that works the way it does and how they're able to achieve high throughput both on reads and writes and then that'll allow us to segue hopefully pretty easily into [[HBase]] and see kind of the good reasons to use something like that. 
	- [[Hadoop File System Design]] Background
		- Description
			- Distributed file systems are a key component of many large scale distributed systems in modern day technology companies. HDFS is the most popular open source file system, and is modeled after the [[Google File System]] (GFS). Because HDFS can be run on commodity hardware, it is immensely popular
			- Not only is HDFS important for enabling large scale batch processing via MapReduce or dataflow engines like [[Spark]] or [[Tez]], but additionally HDFS acts as a building block upon which many open source databases have been created. HDFS is designed with computation in mind, opting to try and optimize for doing any compute close to the data.
		- Okay so HDFS and the design of it. Just to give a background, I've mentioned distributed file systems in the past but basically they're a really important component of a ton of large-scale distributed systems even though HDFS is probably the most popular one, it itself is based off the google file system which is a paper that came out well over a decade ago now and because of the fact that HDFS can be run on just normal desktop computers, it's really popular. Obviously there's a big wave to be able to just like spin up instances of things using things like [[EC2 clusters]] or just amazon web services in general. So even though HDFS is really useful for things like batch processing, we've talked about this with MapReduce, Spark and Tez. HDFS is really good for a database building block so that you can provide an extra layer to kind of interact with the data on HDFS and then ultimately run a ton of computations on it.
	- Overview
		- Description
			- HDFS is designed for the use case of being able to write a file once, and then read it many times over
			- Files are stored in chunks across nodes in the cluster (typically around 128 mb per chunk) to improve parallelism of writing and reading
			- Chunks are replicated to ensure data availability 
		- okay so just to give an overview as you can see on the right you're going gonna see a ton of terms that you don't know yet but by the end of this video you will. So generally speaking Hadoop is designed to basically be able to write a file once and then you know from then on you can append or truncate it but generally speaking just read it many times over. The way this is done is by storing them in chunks across a bunch of different data nodes and typically these are around 64 or 128 megabyte chunks and the reason you do that is to improve the parallelism of both reading and writing big files. Oftentimes these files are gigabytes or maybe even terabytes in size and as a result having to write them all sequentially would be terrible and then finally in order to ensure availability and no loss data, chunks are obviously going to be replicated.
	- NameNode
		- Description
			- NameNode stores all metadata regarding files, as well as keeping track of all of the chunks (and their version numbers) that comprise the file, and the data nodes on which they are located
			- Keeps all metadata in memory
				- All changes to file system metadata go to the EditLog (like a write ahead log)
					- Sequential writes are faster
				- State occasionally checkpointed to disk on a persistent FSImage file
					- This is when edit log writes are applied to FSImage file
				- On startup use the combination of FSImage and EditLog to load state into memory
		- Okay so the first component of HDFS that we have to talk about is probably the most important one and that's going to be called the NameNode. So the NameNode generally speaking is where all of the metadata regarding files is stored so that includes things like names but not only does it just hold the names of the files and perhaps even their subdirectories if it is a directory, but more importantly it has to keep track of all the chunks, so basically all of the data nodes where those chunks are located as well as their corresponding version numbers. Like I said you can append or truncate files and doing so would increment the version number of it. Okay so how does it do this, well it keeps all of that metadata in memory. All of the changes to file system metadata go to something called the edit log, so the edit log is effectively just a write ahead log for the NameNode because obviously if we had to you know like go ahead and change kind of disk state every single time or some persistent state of the entire state of the file system, those writes would not be sequential and they would take longer. So what we do is we put them to a write-ahead log, change the state in memory and then occasionally checkpoint that state on disk to something called an [[FSImage file]] and then if the name node ever crashes and has to reboot, the FSImage checkpoint file in conjunction with whatever edit log writes come after that checkpoint can be combined to create a new state for the name node. 
	- NameNode continued
		- Description
			- NameNode does not keep location of all chunks on disk, only in memory.
			- When it first boots, it goes into safe mode:
				- Receives a block report from each data node saying which chunks it holds
				- NameNode then uses this to construct local state in memory
				- If there are chunks that are not stored on enough replicas, the NameNode will automatically replicate them until the replication threshold is reached
					- This will also happen if NameNode assumes certain data nodes to be down due to a lack of heartbeats
		- Okay in terms of continuing to talk about the [[NameNode]], it actually only keeps the location of all the chunks only in memory. So when it first boots what happens is the NameNode goes into safe mode and it's going to receive something called a block report from each data node, where the data node is going to tell it which chunks are held on that data node. The name node is then going to compile all of this information, construct that local state and say here's where the chunks are located and say now it sees that only one replica is holding a given chunk and you know the user is say specified a replication factor of three for that chunk, it's going to say okay we don't have enough replicas for this chunk in particular, let's go ahead and add some additional replicas for it. So go ahead and replicate that chunk to two other nodes and that way we can reach the replication threshold. The same thing will occur if a name node assumes that a given data node is dead because it hasn't received any heartbeats from it for awhile.
- ![[Screenshot 2024-10-06 at 12.03.59 AM.png]]
	- Hadoop Replication
		- Description
			- "Rack Aware" Replication:
				- Replicate chunks in a way that reduces latency for clients as well as reducing the possibility of all replicas going down in the event of a rack/data center failure
					- For replication factor of 3: one replica in the same rack as the writer, two replicas on the same remote rack (does this to reduce network bandwidth)
		- okay now let's talk about replication. So replication in Hadoop is something called "rack aware" and this is really important because it allows for both maximizing availability and throughput so we'll talk about that in a second. Chunks are going to be replicated in a way that not only reduces latency for clients but also reduces the possibility of all the replica nodes going down because of the fact that they're put in a different rack or data center. So for example for the default replication factor of 3, Hadoop is going to put one replica in the same rack as the writer and then two replicas on the same remote random rack. The reason they put them in the same random rack is just to minimize network bandwidth. You don't have to go to two different racks and since we have kind of a synchronous replication here where we wait for all of these writes to complete, it's actually pretty important that all of these replicas complete their write as fast as possible. It's not eventually consistent so i'll touch upon that in a second 
	- Hadoop Replication Continued
		- Description
			- Replication Pipelining:
				- Data pipelined from one replica node to the next
				- Writes are only considered successful if all replicas in pipeline acknowledge it, otherwise client is expected to retry the write until successful
					- While in theory this leads to strong consistency, the reality is that it can lead to inconsistencies in the data nodes
		- so how does replication actually work? Well there's something called pipelining. Basically the replicas are arranged into some order and the data is pipelined from one replica to the next. On a write or an append or a truncate, writes are only considered successful from the client's point of view if all of the replicas in this pipeline actually acknowledge them. So even though in theory this should lead to strong consistency, the issue is that say the first replica receives a write, it's going to go ahead and commit that to itself so it's going to perform the write and then the second and third replicas don't ever actually acknowledge the write meaning that you know they didn't perform them themselves. Well the client is going to receive a failure for its write, however it's going to be the case that the write is still in one of the replicas so generally speaking when a client receives a failure on a write, it needs to just keep retrying until it receives a success message. So as you can see the first replica is going to send that write to the second one which sends the right to the third one which then sends the acknowledgement back and back again. So once this whole process is complete, the client sees its write as successful.
	- Hadoop Reads
		- Description
			- Client queries master to get list of data nodes carrying a specific chunk
			- Client figures out which data node is closest to it, and caches this result for subsequent reads of the chunk
			- Client performs the read from the proper data node
		- okay, in terms of reading in Hadoop, basically all that happens here is the client is going to query the master or when i say the master in this case i mean the name node, to get a list of data nodes carrying the chunk that it wants. It's going to figure out which data node is closest to it because like i said Hadoop is aware of the rack that the nodes are in and as a result of that it can say for a given client which one is probably going to have minimal network latency when communicating with it. So you choose the best data node to read from. You're going to cache this result on the client in the case that you want to read that file again because like i said write once read many times and then the client's going to just go ahead and perform that read.
	- Hadoop Writes
		- Description
			- In order to append to a file, first contact the NameNode to see the data nodes on which the chunks are located and pick a primary replica
				- If there is already a primary replica designated for the chunk (its lease is still valid), perform the write to the primary replica
				- Otherwise, we need to pick a primary replica, which must be one of the datanodes with an up to date (same version number as the NameNode) version of the file
				- If this does not exist, we have lost data
			- Once the primary replica is determined, all other replicas are considered secondary, and the write is performed on the primary replica, moving through the replication pipeline
			- Once all replicas acknowledge the write, the client receives a success message
		- okay in terms of doing writes, this is a little bit more complex but i'll have a visualization after i walk through this process. So to append to a file go ahead and reach out to the name node see the data nodes where the chunk is located and then you have to pick something called a primary replica. This is going to be the first replica in that replication pipeline. If there's already a primary and the lease for the primary is still valid because you know the lease basically says how long until there's no longer a primary, perform the write to the primary replica, let it go through the chain of replication, otherwise we need to pick a primary replica. How can we do this? Well we look at the data nodes that the chunk is located on and pick one of them with the most up-to-date version of that chunk. If it doesn't exist, we have a data loss problem and hopefully this never comes up. Once the primary replica is determined, all the other replicas are considered secondary. We kind of establish that path for the replication and then the client is going to make the write to the primary replica and you know hope to get a success result. 
- ![[Screenshot 2024-10-06 at 12.18.19 AM.png]]
	- Hadoop Writes Visualized
		- Description
			- (1) Client asks NameNode for data on `Jordan'snudes.png`
			- (2) Randomly picks replica with up to date version number as leader, rest are followers
			- (3) R1 is leader, client sends write three and let it propagate through replication pipeline
		- okay so to actually visualize this, let's say i'm trying to write `Jordan'snudes.png`. We've got three replicas. Replica one, two, and three. And the first thing we're going to do is I'm going to go ahead and ask the name node for what chunks are holding it and try and find out what the leader was. So i find out that the leader was replica 3 because as you can see it has version 23 of the file there but that's since expired.
		- So what are we going to do? We're going to randomly pick a replica with an up-to-date version number as the leader. So now the leader is going to be r1 and let's say we have a lease that expires in an hour for it because replica 1 also has version 23. It could have been one or three here. 
		- Now all that's going to happen is we're going to go ahead and contact replica one which is the leftmost one in the bottom there and send the write through it and let it propagate through the pipeline. 
	- Issues with Hadoop
		- Description
			- In the original GFS paper, and the architecture I have presented so far, there is only one NameNode. If it crashes, we are screwed!
			- How can we fix this? High availability HDFS.
		- okay, so what are some issues with Hadoop? well if you've been paying attention so far, you may have noticed that i've only mentioned one NameNode which is obviously a problem. What happens if the name node goes down? Well everything crashes. So in the original Hadoop implementation, there was kind of like this hacky way of solving things called a secondary NameNode which was basically just like a standby name node that went ahead and tried to take in all those changes, but there's actually a better way of solving this and it uses coordination services like i talked about in the last video. So this is known as high availability HDFS 
	- High Availability HDFS
		- Description
			- Recall that the NameNode keeps an EditLog of all metadata changes, which is really all of its persistent state. Instead of keeping this on the NameNode, we can use a coordination service (in this case ZooKeeper)! Known as quorum journal manager.
			- By creating a replicated log, we can have a backup NameNode which reads from the replicated log and keeps itself up to date.
		- what do we actually do? Well keep in mind that the NameNode basically the main persistence point of the NameNode that you can use to derive the state of the NameNode is the EditLog. So the EditLog is basically just going to keep track of all that file metadata changes such as renaming files, creating a new directory, anything along those lines and so instead of just keeping all of those changes locally to the NameNode, what we're going to do is go ahead and use something like a few zookeeper nodes to create a replicated log which represents the edit log. This in Hadoop terms is known as the [[quorum journal manager]]. So anyways after we have this replicated log, we can now have a second instance of a NameNode which you know we'll just call it the backup for now and all it's going to do is read that replicated log and keep its state up to date in the same way the NameNode does. So by using this coordination service here we're actually able to keep a secondary NameNode relatively up to date so in the event that the first one goes down, you know say we have the coordination service also has a distributed lock, the first one goes down, it will no longer be holding that distributed lock, and then the second one can grab the distributed lock to basically say i'm the leader now. I am going to be the NameNode. 
- ![[Screenshot 2024-10-06 at 12.20.08 AM.png]]
	- HDFS Conclusion
		- Description
			- By using a rack aware replication schema as well as rack aware reads, HDFS is able to provide extremely high availability and throughput for both file reads and writes. While the original implementation of HDFS was not at all fault tolerant, the design implementing ZooKeeper completely changes this!
			- Keep in mind that HDFS is not perfect, and we saw that there can be some data consistency issues as a result of the replication pipeline
			- On the whole though, using HDFS for storing data in conjunction with large scale compute is a great option, and we will see how certain databases leverage it.
	- Okay so in conclusion um HDFS can provide really high read and write throughput by using a rack-aware replication schema and reading as well. This is super useful and in addition to that while the original HDFS kind of design wasn't very fault tolerant, the fact that they've now added a coordination service to it is great for kind of that leader failover of the name node and allows high availability. Obviously HDFS isn't perfect, like i said it kind of aims for strong consistency but in reality you might have data inconsistencies and have to handle this in your application code. But on the whole, HDFS is really good for storing data in conjunction with large scale compute. We're going to see that plenty of databases are built on top of HDFS in order to kind of provide a better programming interface and allow for more complicated querying of it and that's what i'm going to be talking about in my next video. So i hope this was useful guys and welcome to all the new subscribers again and i'll see you soon