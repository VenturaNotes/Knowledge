---
Source:
  - https://www.youtube.com/watch?v=6GebEqt6Ynk
Reviewed: false
---
- ![[Screenshot 2024-11-15 at 2.37.24 AM.png]]
	- Introduction
		- Welcome back everyone and welcome to the channel if this is your first time. Uh you lazy in the comments section finally got me to do it and now I'm going to make an all-encompassing databases video I know. I've literally been promising this one since the first episode on my channel which uh is pretty embarrassing that I'm only finally getting around to it right now. Additionally, as you can see, I've got my women at Amazon shirt on, I'm definitely supporting the crowd, so if there are any women watching this video, I stand with you. Anyways, let's continue. Let's get into this video, um all of this information is completely redundant as far as I'm concerned, uh with the other stuff that I've made on my channel, but I've gone ahead and compiled it so hopefully that works and uh additionally, you know, I think I'm gonna go a little bit more technically in depth than any other video of this kind has ever gone on YouTube, so uh, you know, if you appreciate that, please follow me. My accounts are dwindling, and um I'm starting to get insecure.
	- Choosing a Database (Background)
		- Description
			- There are a lot of different options for databases, and each of them has a unique implementation! How can we decide which to choose for our systems design interview?
		- All right everyone, this is gonna be a kind of longer PowerPoint, so sit back, buckle up, and let's get into this. So we're gonna go ahead and choose a database. You're doing a systems design interview and your interviewer asks the famous question, what database are you going to use in this problem? If not more than one, oftentimes it is better to use more than one. So let's go ahead and talk about how we can think about this problem. Okay, like I said, there's your background. Let's start jumping into some databases
	- Review: Indexes
		- Description
			- A database index is used for the purpose of speeding up reads conditioned on the value of a specific key! Be careful to not overuse indexes, as they slow down database writes.
				- Two main types:
					- LSM Trees + SSTables
					- B-Trees
		- However, before we can really do any of this jumping into databases, first we have to review some information. I wouldn't be surprised if some of the people watching this video have not seen my earlier ones because, you know, just by virtue of being earlier on my channel, not everyone has seen them and so I am going to do a little bit of review. I'll make sure to put timestamps in the video if you actually know this stuff and then you can go ahead and skip through to the part where I'm actually talking about database evaluations. Just really quickly, a database index is super useful for speeding up reads on a database. It does slow down writes but the general gist is that you are sorting in a specific order by key and this makes it the case that when you're searching for a specific key in the database and the value of it, you can do so much faster than had you not used an index. So in today's databases that are on disk, there are two predominantly popular types of database indices, we are going to be talking about both. The first is the LSM tree and the SSTable combination and the next is the B-tree 
	- Review: LSM Trees and SSTables
		- Description
			- Parts
				- Writes first go to a balanced binary search tree in memory
				- Tree flushed to a sorted table on disk when it gets too big
				- Can binary search SSTables for the value of a key
				- If there are too many SSTables, they can be merged together (old values of keys will be discarded)
			- Pro: Fast writes to memory!
			- Con: May have to search many SSTables for value of key
		- So let's look at LSM trees and SSTables. The general point is this, we have our LSM tree which is basically a balanced binary search tree in memory. You can do that with something like a red black tree, it doesn't really matter for the purposes of this video like I said it's review. All this information is on my channel in more depth already so if you're confused about anything, just go watch the dedicated video. Anyways, the point is, we have this memory tree which basically has keys and their corresponding values. When that tree gets too big because there are too many keys in it, it gets flushed to an SSTable on disk. An SSTable is effectively just a sorted list of keys and their values on disk, and then finally, when we want to go ahead and read a key, we first check for it in the memtable which is that LSM tree and then if it's not in there, we go through the SStables in order from the most recent one to the least recent one. So we're never actually overwriting any keys. We simply just write new values for those keys and we keep creating new SSTables. However, the key here is if you know, we're running out of space, we can actually go ahead and compact our SSTables by merging them together. It's kind of like the merge sort algorithm where you're merging two sorted lists and then that way we can free up some space again. The final point is that because our SSTables are sorted, we can kind of keep a sparse index of the location of some keys in the SSTable and that allows us to do searching really fast in O of log(n) time via a binary search. So the general gist of things here are that the pro of this approach is that we have really fast writes because those right are going to an in-memory buffer in O(log(n)) time. Again, it's a balanced binary search tree so O(log(n)) and then the biggest con is that for reads, we potentially have to go through many SSTables to find the value of a key. We can use things like Bloom filters to speed this up, but ultimately it's just not as good as B-trees for reading and we'll talk about why that is.
	- Review: B-Trees
		- Description
			- Parts
				- A binary tree using pointers on disk
				- Writes iterate through the binary tree and either overwrite the existing key value or create a new page on disk and modify the parent pointer to the new page
			- Pro: Faster reads, know exactly where key is located!
			- Con: Slow writes to disk instead of memory
		- So B-trees on the other hand, as you can see from the right, is also a binary search tree. However, it's a binary search tree that is implemented completely on disk. Basically, we have pages on disk that have a range of keys and a bunch of pointers so that you can figure out for a specific value of a key, you basically iterate through a few of the pointers, and then go to the proper place on disk. This is really useful for actually being able to get to the right place to both read and write from. However, for writes, because rights in an LSM tree are going to memory, that is still going to be better for those. So B-trees are going to be worse for writes and faster for reads because we don't have to iterate through a bunch of SSTable files. We could just go directly to the location of the key by virtue of following our tree on disk.
- ![[Screenshot 2024-11-15 at 3.03.25 AM.png]]
	- Review: Replication
		- Description
			- Replication is the process of having multiple copies of data in order to make sure that if a database goes down, the data isn't lost!
			- Types
				- Single leader replication
					- All writes go to one database, reads come from any database
				- Multi leader replication
					- Writes can go to a small subset of leader databases reads can come from any database
				- Leaderless replication
					- Writes go to all databases, reads come from all databases
		- Okay, so we've touched upon indices, but now let's go ahead and get into replication. So just as a quick recap, replication is what you do when you go ahead and duplicate the data that you have on one database node to another database node so that in the event of some sort of Hardware or software failure, you can go ahead and make sure that that data is retained. So there are a few different types of replication that we're going to quickly cover. The first being single leader replication where you basically have one master node that goes ahead and writes to the others and then you can read from any of the nodes. You have multi-leader replication and leaderless replication where writes can go to multiple nodes but in multi-leader replication, you might have just a couple of leaders whereas in leaderless replication, those writes might be sent out to every single node in the cluster, and in turn, the reads might actually come from every single node in the cluster and you may use some sort of like majority voting process to decide what the accurate read is. 
	- Review: Replication Continued
		- Description
			- Single leader replication is useful to ensure that there are no data conflicts, all writes will go to one node
				- Single leader replication
					- Master
						- Slave A
						- Slave B
						- Slave C
			- Leaderless and multileader replication is useful for increasing write throughput beyond just one database node (at the cost of potential write conflicts)
		- Okay, so to just quickly sum this up in terms of the pros and cons of the single leader versus multiple leader approach, basically, the general gist is that with a single leader approach, you have one master and a bunch of slaves, so as a result, you never have to worry about write conflicts but on the flip side, you're going to have lower write throughput because all of your writes have to go through that one master node. You can try and mitigate this with things like partitioning or sharding, but at the end of the day, you know, if all the writes do have to go to one partition, you know, the throughput is going to be limited. On the exact contrary, we have leaderless, multi-leader replication, where you know, we have technically unlimited write throughput in terms of, we could be writing to different replicas every single time, however, at the same time, the more replicas that we write to, the more likely that there is to be a write conflict, and you know, there are kind of smart ways that we can try and get about avoiding this where we have certain writes going to the same replicas but at the same time, you know, it is good to kind of be able to know that your data is correct and you can avoid conflicts at all times.
	- Let's Get Started!
		- Okay, we've gone over some stuff, so let's actually go ahead and get started by talking about some legitimate database examples.
	- SQL Databases
		- Description
			- Key Features:
				- Relational/Normalized data - changes to one table may require changes to others
					- E.g. adding an author and their books to different tables on different nodes
					- May require two phase commit! (Expensive)
				- Have transactional (ACID guarantees)
					- Excessively slow if you don't need them (due to two phase locking)
				- Typically use B-trees
					- Better for reads than writes in theory
			- Conclusion: Use SQL when correctness is of more importance than speed
				- See banking applications, job scheduling
		- So the first one that I'm going to start with are SQL databases. So the reason I'm not breaking SQL down into things like MySQL and Postgres and things like that, is because the truth of the matter is the feature set is generally similar between the SQL databases but it's more so that there are many different types of NoSQL databases so I'm going to break those down further but just for the sake of getting started, let's go ahead and break down the important features of the SQL databases. So the first thing is just the actual data model itself. SQL is uh you know kind of biased towards using relational and normalized data. What that means is that you don't basically duplicate data in multiple places but rather you'll have rows and then you can use joins in order to kind of reference pieces of data with one another throughout the tables. What this means is that a lot of times when you're doing writes, you may have to write to multiple different tables and if those tables are on multiple different nodes, that can be a problem at scale. The reason being, if you want to write to two different tables and they're on two different computers for those to either both work or to not work, we would need something like a two-phase commit protocol and I've spoken about two phase commit plenty in the past but the point is, if you plan on guaranteeing that two writes are going to occur on two different computers, and you know if one fails then neither occurs, basically saying uh you want a transaction over multiple computers, that is very expensive. Distributed transactions are pretty unreasonable in practice and as a result, it's going to be hard to actually go ahead and implement that. Additionally, with SQL databases, we have asset guarantees meaning that we're actually going to be implementing transactions which basically says, every single transaction acts as if it were serializable, right? They can't go ahead and you know kind of read each other in the middle of a transaction, you don't have to worry about race conditions, you don't have to worry about a transaction partially succeeding or partially failing. It's either all going to succeed or all going to fail, and then finally, we're also using B-trees. So B-trees um like I mentioned versus LSM trees in particular are better for reading in theory but worse for writing and just to quickly go back on my point about transactions, um kind of the issue with transactions are that even though it's good for the correctness of the data, it also potentially slows down the entire database due to most of these SQL databases using an expensive two-phase locking scheme in order to ensure data correctness. Okay, so what is the actual conclusion about when we want to be using SQL databases. Well, you know based on everything I've kind of just said here, it's really good when correctness is more important than speed, right? You know if we're ever trying to deal with transactions where we're modifying multiple rows in a table at once, we want to make sure that all of those things happen or don't, you know something like a bank transaction, SQL is going to be super useful because we can't have it be the case where you know, I get a hundred dollars and then you never lose your hundred dollars. Now as the bank, I've just lost a hundred dollars so that doesn't work. Another good situation would be job scheduling, if we have like a status table and we're you know editing multiple rows of the status table at the same time to do updates, we want to make sure that those are relatively in sync and that all of those rows are either being changed or they're not. But yeah, I mean like I said, SQL databases are kind of the default in this situation and I don't think the breakdown between the specific types are too important because at the end of the day, they're kind of all based on the same Technologies but, you know, they may have some subtle differences in terms of feature sets.
- ![[Screenshot 2024-11-15 at 3.10.31 AM.png]]
	- MongoDB
		- Description
			- Key Features
				- Document data model (NoSQL)
					- Data is written in large nested documents, better data locality (if you choose to organize your data in a way that takes advantage of this) - but denormalized
				- B-Trees and Transactions supported
			- Conclusion: Rarely makes sense to use in a systems design interview since nothing is "special" about it, but good if you want SQL like guarantees on data with more flexibility via the document model
		- Okay, so let's move on to our first NoSQL database which is actually going to be MongoDB. So MongoDB is a document database which basically means that as opposed to storing data in rows, now you have these documents which are basically just lists of items, but then, within that document, you can have more nested documents. So now you can store a ton of data in these Collections and you know if you actually wanted to go ahead and store your data that way, it can provide you with really good data locality. So just to give an example, you know, with SQL for example, you could have a table of books and a table of authors and those might be stored on different computers, and you know, your author table might have an ID that helps you to fetch the relevant book rows, right? That being said, in a document data model, you know, you might have one author document but all of those book documents will actually be stored within the author document which is really nice because if you're only going to be updating those at the same time, it means that you can do that really easily, you have great data locality. At the same time though, you know, say you have multiple documents that rely on kind of the same piece of information. If one piece of information gets updated but the other doesn't, then those documents are out of sync and that's kind of the issue with denormalized data. Additionally, as far as MongoDB goes in terms of Technology under the hood, I believe they do use transactions and also B-trees. So think of it as terms of like similar performance to SQL, but you do get a little bit more flexibility in terms of the document data model. That being said, you know a lot of people are kind of familiar with relational databases and that data model, so that seems to be why SQL is kind of still the norm. But documents do have their own usage. So again, in terms of the systems design interview, I don't think MongoDB is like particularly groundbreaking, but, if you just want kind of like the SQL technology with kind of a more advanced way of actually storing your data, then MongoDB can be a little bit useful.
	- Apache Cassandra
		- Description
			- Key Features:
				- Wide column data store (NoSQL), has a shard key and a sort key
					- Allows for flexible schemas, ease of partitioning
				- Multileader/Leaderless replication (configurable)
					- Super fast writes, albeit uses last write wins for conflict resolution
					- May clobber existing writes if they were not the winner of LWW
				- Index based off of LSM tree and SSTables
					- Fast writes
			- Conclusion: Great for applications with high write volume, consistency is not as important, all writes and reads go to the same shard (no transactions)
				- See chat application for a good example of when to use
		- Okay, let's move on to Cassandra. I would say this is really the NoSQL database of choice that you should probably be listing in most of your systems design interviews and there are a few good reasons for that. For starters, it is a wide column data store which is essentially just an Excel spreadsheet more or less except you know other than the required sharding column and sorting column, you can basically put any other columns in there and that can change from row to row which is really nice in terms of having data flexibility. But additionally, there's also going to be multi-leader and leaderless replication and this is configurable. So what that means is, you know, you can choose to use something like Quorum writes but at the same time, you can also just choose to you know have like one node kind of be the leader and propagate that out elsewhere or have just a couple nodes be the leaders but it's not necessarily going to be a quorum read or write. So you don't always need the majority. Um, that's really good because compared to single leader replication, you can have much faster writes but at the same time, it also puts into question the correctness of these writes if you're going to have write conflicts. Cassandra in particular lets you use last write wins in order to resolve write conflicts which isn't always ideal because it means that certain rights can be clobbered if they have the lower timestamp. Keep in mind that timestamps and distributed systems are not reliable unless you're using something like a GPS clock and we'll talk about that later with Spanner, but as long as that's the case, it means that certain writes may be unfairly clobbered. And then the last useful part about Cassandra is that they are using an LSM tree index which means there should be faster writes albeit slower reads. Okay, so what's Cassandra really useful for? Um well it's really great when you want high write throughput and, you know, if you don't care as much about the consistency, it's okay if the occasional piece of data is overwritten or lost, then Cassandra is definitely going to be the way to go. So something where this could be really useful is something like a Facebook messenger chat where, you know, maybe your sharding key would be like the chat ID and then from there you, would have all of your messages ordered using timestamp as the sort key. 
	- Riak
		- Note: It's not a wide column store. Image incorrect.
		- Description
			- Key Features
				- Key value store(NoSQL), has a shard key and a sort key
					- Allows for flexible schemas, ease of partitioning
				- Multileader/Leaderless replication (configurable)
					- Super fast writes, supports CRDTs (conflict free replicated data types)
					- Allows for implementing things like counters and sets in a conflict free way, custom code to handle conflicts
				- Index based off of LSM tree and SSTables
					- Fast writes
			- Conclusion: Great for applications with high write volume, consistency is not as important, all writes and reads go to the same shard (no transactions)
				- See chat application for a good example of when to use
		- Okay, let's talk about Riak. I've got a bunch of information listed here but the general point of Riak is this. It's basically the same as Cassandra, but one weakness of Cassandra that I mentioned was the fact that conflict resolution was completely done using last write wins. On the other hand, Riak actually has something called CRDTs in order to help you with conflict resolution where basically CRDTs are these conflict-free replicated data types that act as ways of kind of aggregating certain conflicting writes and then allowing you to use more complex logic in order to actually resolve the conflicts. This is useful if you want to make sure that you're not just having writes clobbered and it can allow you to perform things like sets and counters in a multi-leader or leaderless replication setup that are eventually consistent, which is really great. So again, kind of the same use cases as Cassandra but it's just another nice point to touch upon that you can actually go ahead and you know kind of resolve those conflicts. One piece of information that I did actually fail to mention on this slide is that I've wrote that it's a wide column data store, it's actually a key value store. So that's an error on my part.
	- Apache Hbase
		- Description
			- Key Features:
				- Wide column data store (NoSQL), has a shard key and a sort key
					- Allows for flexible schemas, ease of partitioning
				- Single leader replication
					- Built on top of Hadoop, ensures data consistency and durability
					- Slower than leaderless replication
				- Index based off of LSM tree and SSTables
					- Fast writes
				- Column oriented storage
					- Column compression and increased data locality within columns of data
						- Errata: Apparently Hbase being wide-column store actually store column families together, not individual columns.
			- Conclusion: Great for applications that need fast column reads
				- Multiple thumbnails of a youtube video, sensor readings
		- Okay, next let's talk about Apache HBase. So what are the key features of HBase? Well, basically this is actually a wide column store, so same basically you know kind of outer interface as Cassandra, however, there are some pretty subtle differences that do make a difference in performance. So the first thing is that it is built on top of the HDFS or Hadoop distributed file system which means that you are using single leader replication. You write to a single node, and then from there, that data gets replicated over other nodes. So this is going to be slower than leaderless replication, but at the same time, it means we don't have to worry about write conflicts. Additionally, the indexes are based off of the LSM tree, so we should have relatively fast writes but the interesting thing about Apache HBase is that the actual storage itself instead of using a row wise storage model actually uses column wide storage which essentially means that instead of storing data within the same row together, you actually store data within the same column of the table together and as a result, you can achieve much better data locality when you're trying to read the entirety of just one column. So that's really useful. Basically, this is going to be really great for times where, you know, you're trying to get a very fast read on a column of data, so for example, you know, let's say we're like TikTok or something, or you know, even YouTube here where when you actually press on a video, it shows you a few images of that video, that would be a great place to use Apache HBase because you can actually store that raw image data in the table itself and use column compression to quickly pull all of those images from that column whereas you know if it were row storage, we would have to read multiple rows, you'd have less data locality, and the read would be slower 
- ![[Screenshot 2024-11-15 at 10.19.28 PM.png]]
	- Memcached and Redis
		- Description
			- Key Features:
				- Key value stores implemented in memory (Redis a bit more feature rich)
					- Uses a hashmap under the hood
			- Conclusion: Useful for data that needs to be written and retrieved extremely quickly, memory is expensive so good for small datasets
				- Good for caches, certain essential app features (see geo spatial index for Lyft)
		- Okay, let's next talk about Memcached and Redis. So these are technically not the best database solutions and the reason for that is that they are key value stores that are actually implemented in memory. The subtle difference here is that Redis I believe has a little bit more of a rich feature set. You can do things like um [[geosharding]], and you know sorted sets, and [[HyperLogLogs]], but at the end of the day, they're mostly just key value stores in memory and the point is is that instead of using anything like an SSTable LSM-tree combo or a B-tree combo, you're simply just using a hash map to store your data and so you don't need an index. The one thing to note about a hash map is that it's worse for range queries. So anyways, the point about memcache and redis is that it's really useful for data where it's going to be you know written and read very frequently, it's good for your most necessary data, and especially, you know, for small data sets, it's useful because, you know, storing stuff in memory is just a lot more expensive than storing things on disk. So, this is really great for things like caches or if you just have an essential part of your application that's getting written to and read from really often, it can also be good for that, so one example is actually the geospatial index in an app like Uber or Lyft where you see it's constantly being updated and read from with all the updated positions of drivers and Riders and so that's going to be very useful there.
	- Neo4j
		- Description
			- Key Features:
				- Graph database!
					- As opposed to just using a SQL database under the hood with relations to represent nodes and edges, actually has pointers form one address on disk to another for quicker lookups
					- The former is bad because reads become slower proportional to the size of the index (O log n to binary search), but using direct pointers is O(1)
			- Conclusion: Only useful for data naturally represented in graph formats
				- Map data, friends on social media
		- Okay, next we have Neo4j, so this is a less popular one but the reason for that is that it's actually a graph database. So to quickly explain the premise behind Neo4j, the point of a graph database is this. If we wanted to implement a graph with a SQL database, we could do it, but it would have to be using a many-to-many relationship, and the issue with the many-to-many relationship is that we would basically have a table with you know the IDS of two nodes and basically the fact that two nodes were in that many-to-many table together would indicate that there was an edge between them, so you could do that, however, that's going to be very slow. The reason for this being is, as that table gets bigger, we know that our index for that table is going to get bigger, and the index, keep in mind that the time complexity of reading from an index is proportional to log of the number of elements in that index. So, as the database table grows bigger, our graph database would get slower. If we implemented it using a SQL table. Instead, Neo4j is actually a native graft database which basically means that you have pointers to the actual location of the node corresponding to the other end of the edge on disk so you can more quickly traverse them in constant time as opposed to having to do logarithmic work every single time you want to traverse a node. So again, this is kind of a niche topic, I can't really think of too many systems design questions where they actually want you to use a graph database, but if it ever does come up, this is really useful for things like maybe map data and perhaps also something like modeling out friends on social media. 
	- TimeScaleDB/Apache Druid
		- Description
			- Key Features:
				- Time series database!
					- Use LSM trees for fast ingestion, but break table into many small indexes by both ingestion source and timestamp
					- Allows for placing the whole index in CPU cache for better performance, quick deletes of whole index when no longer relevant (as opposed to typical tombstone method)
			- Conclusion: Very niche but serve their purpose very well
				- Great for sensor data, metrics, logs, where you want to read by the ingestor and range of timestamp
		- Okay, another type of database that we're going to talk about are time series databases. So you see, I listed a couple examples at the top of this slide, but generally the point is this. Time series databases are really useful for when you're modeling a bunch of ingestion from many different sources of data, but also want to keep them in order relative to their timestamp, right? So let's say we have like five sensors and we want to model one day's worth of data for all of those sensors. So the reason these are useful, is they do use LSM trees and SSTables which means that you can have relatively fast ingestion because things go to that in-memory buffer first. However, they also take a little bit of a turn on them to kind of make them even more efficient. For starters, as opposed to having one huge LSM index for the entire table, what they do instead is split the table into many smaller indexes. (error indexes?). The reason for this being is that you're likely only going to need to be able to access a small chunk of data at a time in a Time series database and being able to make these indexes uh into a kind of small chunk indexes is going to allow you to put that entire index in the SSTable and as a result, really quickly read from it. And so you can use kind of the CPU cache much more efficiently by having all these smaller indexes. Additionally, this is also really efficient for deleting those indexes because a lot of times with time series databases, as the data gets too old, you want to throw it out. This is really inefficient with a typical LSM-index where you wouldn't actually delete the data directly but you would put in a tombstone into the LSM-tree where you're saying that a key is going to be deleted and then eventually when those SSTable files get merged together, that key is going to be thrown out. However, by actually just deleting the index itself, this is a much faster process than having to wait for table compaction to occur. So basically again, pretty Niche kind of type of database, but if you're ever dealing with an assistance design problem about, you know, ingesting a ton of metrics or logs or sets or data or anything like that, a time series database is definitely a way to go for at least part of that problem. 
	- New SQL
		- Description
			- VoltDb
				- SQL but completely in memory, single threaded execution for no locking
				- Expensive and only allows for small datasets
			- Spanner
				- SQL, uses GPS clocks in data center to avoid locking by using timestamps to determine order of writes
				- Very expensive
			- Cool discussions to have in an interview, but probably pretty impractical to ever actually suggest!
		- Okay, kind of honorable mentions here that um you know probably won't come up as much in interviews, unless you're having, you know, just like a fun discussion about databases. First, we have VoltDB. So New SQL is kind of the category that I'm talking about here and new SQL basically means they're SQL databases. However, they're implemented in interesting ways so as to get better performance than the traditional SQL database. So VaultDB basically goes ahead and takes a SQL database, however, it runs it all in memory and it also runs it using only a single thread of a CPU. So what this means is that there quite literally can't be any race conditions because there's only one thread being run. You know, there's no concurrent operations that are actually occurring and by running them all in memory, we can basically make all these operations happen so fast that using single threaded execution is actually feasible for performance. The issue with VoltDB is obviously that it's going to be expensive, it's running in memory, and also it's only going to be good for small data sets because it's running in memory, but it is kind of an interesting concept. Additionally, we have spanner which is actually a Google product and the premise of spanner is that you're also using SQL. However, in order to kind of avoid having to do a ton of locking to figure out what write came after, you know, what write, what it does instead is it puts GPS clocks in the actual data center itself in order to assign each write a relatively accurate timestamp and then use those timestamps to figure out, you know, kind of what write came after what, and once you can order all of these writes, it means that all of the databases are able to go in a consistent state, and we can basically achieve strong consistency by virtue of linearizability. Spanner is also very expensive because we were no longer using commodity hardware, we're actually putting you know clocks in the data center which is not an easy thing to do. So again, you know, cool discussions to have but also not really something you need to know as much. A last kind of final honorable mention that I forgot to even make a slide about was just kind of data warehouses in general. Those are generally for SQL formats and probably after you would do some sort of batch work to properly format the data, but yeah, if it ever comes down to an analytic question, something like a data warehouse could be very useful. 

