---
Source:
  - https://www.youtube.com/watch?v=Dt1trw66IAc
---
- ![[Screenshot 2024-11-13 at 8.17.47 PM.png]]
	- [[Data Serialization]] Background
		- Description
			- In most distributed systems, we need a way of turning in-memory data into byte sequences for transport. This could be either to files on disk, or over the network to other computers. However, as we will see, some formats for storing data in a transferable format are not only more efficient, but more maintainable for long term development and data schema evolution, than others
				- Example shows "Needs to be converted to a sequence of bytes"
		- Okay, let's talk about uh serialization. Alrighty, so what do i mean by data serialization. Well basically for starters on computers most data is actually stored in memory and we're going to be doing that through the use of objects or classes or something like that, but a lot of the times we have to go ahead and transport this data somehow and generally speaking we have to do this as a byte stream. That could be either writing this data to a disk for example or sending it over the network to another computer in a distributed system. Either way, the point is we have to actually be able to convert data from this in-memory representation which might contain things like hashes or pointers and go ahead and put that into a format readable by other computers. There are a lot of ways to go about doing this. There are a lot of ways that we can represent this data and each obviously has their pros and cons and we're going to discuss a bunch of those in this episode. So as you can see for example, if i wanted to convert the class below which is a person and you know has some key features that are extremely important to a sequence of bytes, I would have to go ahead and convert that pointer over somehow and be able to kind of serialize that maybe as json or something else but we'll discuss all these options in a second. 
	- Naive Approach to serialization
		- Description
			- Language Specific Serialization Frameworks:
				- Pickle for Python
				- Marshal for Ruby
				- Serializable for Java
			- While these libraries are convenient, they lock you into using a single language, and additionally, they often will deserialize the data into arbitrary classes which can lead to security vulnerabilities! Additionally, they do not care about versioning data, and re relatively bad performance wise.
		- Okay, so what's the naive approach to serialization. Well as it would turn out, most programming languages actually have libraries built in in order to serialize objects. There's Pickle, Marshal, Serializable, but all of these are pretty bad in the sense that it pretty much limits you to one language which is not huge. They're not very focused on performance which is another issue with them and additionally they offer kind of some security vulnerability in the sense that um if you know someone with malicious intent can get you to kind of de-pickle or de-marshal some arbitrary byte stream that they wrote, it might allow them to instantiate arbitrary classes and do things that you don't want them doing. So as a result, generally speaking, these language specific serialization frameworks while easy to use are probably not the move. 
	- Standardized Encodings
		- Description
			- Extremely popular, formats like JSON/XML:
				- Useful because everyone is familiar with them, hence why they are great for communication between different organizations
				- Issues determining between numbers and strings (XML) and integers and floats (JSON)
				- No support for binary strings (only unicode ones)
				- Not particularly compact, thus introducing extra network load and leaving room for performance improvements (field names need to be sent unless there is a schema involved)
					- Binary encoding can improve these issues but still suffer from above problem regarding field names
		- Okay, what about standardized encodings like json or xml. Well basically as you can see, there's a lot of red that i wrote here but the truth in the matter is, these are extremely popular and the reason for that is that they're extremely popular. You know it's kind of a network effect which is that if you can get two different companies to go ahead and agree on some sort of common format for data exchange, there's a ton of value in that and so the fact that json, xml are so widely used in the past just encourages the you know further use of them and especially for cross-organizational data transfer, json and xml are both extremely popular. However, they definitely have their issues. For starters with xml in particular you can't really differentiate between numbers and strings. For json, you can't really differentiate between integers and floats and with floats in particular you can't really specify the precision of floats. Additionally, you can only deal with unicode strings as opposed to binary ones consisting of just zeros and ones and then finally they're not particularly compact which means there's more network overhead to have to actually go ahead and send some xml from one computer to another. Same goes for json main reason for this being that we have all these you know strings representing keys or fields that we have to send over and that's kind of annoying. You can improve the size of your encoding a little bit by binary encoding however even still even with this slight optimization you still have to go ahead and include all of those field names and so you're not actually doing that much better. So instead, what do a lot of companies want to go ahead and use when they're doing this kind of internal data transfer only as in yeah you're transferring data from computer to computer but it's within your company so you can kind of deal with you know getting computers to agree on the format that you're going to use.
	- Thrift and Protocol Buffers
		- Description
			- Binary encoding libraries that use a schema to greatly reduce the size of serialized messages
				- Once a schema is provided, these libraries can generate classes representing those datatypes in most popular languages
				- Since each field has a numbered tag we can greatly decrease the amount of space required for encoding, as we do not need a full string to represent the field name
					- Thrift
					- Protocol Buffers
		- Well, this is where something like thrift or protocol buffers comes in. Thrift and protocol buffers are two libraries created by facebook and google respectively. I think thrift has since been open source to the apache license but basically all they do is they create these very compact binary encodings that they're able to do so because before actually going and creating the binary encodings, what you do in thrift and protocol buffers is actually specify a schema of the type of message that you're going to go ahead and encode. So once this schema is provided, you can actually use code generation tools in your IDE to represent classes for all of these types of messages and basically the way that they work is that with these two in particular, each of these fields within a message actually has a number and that number is going to allow you to very compactly kind of express which field it is that you're sending in a json message as opposed to having to write out the entire name of a string. 
- ![[Screenshot 2024-11-13 at 8.29.54 PM.png]]
	- Schema Evolution
		- Description
			- Important to ensure that the schemas that we set are both forwards and backwards compatible. Rolling updates, as well as slow to update clients are always a possibility!
			- Backwards compatibility: Newer code can read data written by older code
			- Forwards compatibility: Older code can read data written by newer code
		- Another thing that these are really useful for is for documenting schema evolution. So basically, if you're going to have some sort of set schema for your data, it is probably the case that at one point or another throughout the life cycle of your application development, you're going to actually have to change that data. Additionally, even things like rolling upgrades in your servers mean that it's very likely that servers are going to be running different versions of software at the same time and as a result, we want to be able to achieve both backwards compatibility and forwards compatibility where backwards compatibility basically just means that newer code can read the messages encoded by older code. Forwards compatibility is basically just saying that older versions of the code or older versions of the software on the server can read messages written by the newer code. It doesn't necessarily mean that they have to be handled in the same way but it does mean that you at least don't want things crashing. 
	- Schema Evolution
		- Description
			- Cannot change existing field tags as it would make all existing encodings invalid
				- But you can add new fields with unique tag numbers (cannot be set as required)
					- Ensures forwards compatibility, as old code can ignore new fields
					- Ensures backwards compatibility, as new code still knows how to read all fields contained in old serializations (hence new field cannot be required, old code won't contain it)
			- Depending on situation, may be able to change data type of field
		- So how does schema evolution work in both protocol buffers and thrift. Well for starters, everything is kind of based around those field tags right. Field tags are what we use in order to identify kind of which data corresponds to which field. So as it turns out, you actually can't change existing field tags but you definitely can add new ones and so in addition to being able to kind of add these new fields with unique tag numbers, we know that this both ensures forward compatibility because the old code can basically just ignore any new fields included in messages and it also ensures backwards compatibility because the new code since it has still those existing field tags from before, it can actually go ahead and read all these fields contained in the old versions of the messages. That being said obviously any um kind of new field tags that we're adding have to be optional because otherwise those old messages aren't going to contain them and it would crash newer versions of the code and then additionally depending on the certain um you know situation that you're dealing with, you can sometimes actually change the data type of the code. You can refer to the documentation for these services um for which data types you can change to what. 
	- [[Avro]]
		- Description
			- Similar to [[ProtoBuf]] and [[Thrift]], but created to better suit use cases for Hadoop (dumping lots of possible unstructured data files)
			- Again declare a schema, but no filed tags this time
				- Use this schema itself to decode the data, must be in the same order as the fields
				- However, the schema can still evolve!
		- But that being said, protocol buffers and thrift are certainly not perfect. By virtue of having to evolve your schema by adding only new tags and not actually kind of being able to add and remove fields as you go, we lose a lot of flexibility because that doesn't really reflect how actual database schemas evolve, right? Like it's the fact that a lot of the times columns will just be removed and so if you want to go ahead and automate kind of this schema generation for a given existing format of data it's hard to do that because the schema generation versus the actual data formatting is not going to be one-to-one if you just have to basically add a field even though you know in reality some columns may have been removed from that database. So i'll explain that more in a little bit but basically the solution to this is apache Avro. So similar to protocol buffers and thrift in the sense that you know we're using this to kind of encode these different types of messages using a schema, however it's better suited for cases where you have big data and data where the scheme can definitely change over time. So again, we're declaring this schema but this time there are no field tags. So instead, we're using the schema to decode the data but it's got to be in the same order as the fields and additionally we can still have schema evolutions. So how do we actually have schema evolutions?
	- Writer vs. Reader Schema
		- Description
			- Fields matched up by field name in writer and reader's schema
			- If fields present in writer schema and not in reader's schema they are ignored
			- If fields present in reader schema and not in writer's schema, we use the default value for them
			- This means that fields can only be added or removed if they have a default value!
			- Examples
				- Writer's schema for Person record
				- Reader's schema for Person record
		- Well basically, there's this concept of the writer and the reader schema. So basically, every message that's encoded is encoded with a writer schema and then eventually that message is going to reach some end node where it is going to be decoded and the end node contains some sort of reader schema. Now it is the case that the writer schema and the reader schema can actually be different and Avro kind of has some cool logic to actually go ahead and kind of rectify the differences between them and it's mainly through the use of the actual field names themselves. So as you can see in the image below basically even though the ordering of the field names is not exactly the same, if you contain both the writer schema and the reader schema you know when trying to make a read or you know deserializing a message, you can go ahead and say okay, well now i see where the username field should be now i see how kind of interest maps to interest and favorite number maps to favorite number and so even though the order is changed, you can go ahead and rectify that and kind of figure out which fields of the message plug into which fields. So basically, what we now do is if these schemas are different, if there are fields present in the writer schema and not in the reader schema means that we no longer care about those, they can just be ignored. If there are fields present in the reader schema and not in the writer schema, then we have to go ahead and use the default value provided for those fields. This is something that you can obviously do in Avro and it's probably something that you should be doing in order to go ahead and make sure that your schema can evolve over time. Hence, obviously fields can only be added or removed if they have a default value because otherwise you're going to be in trouble and it's not going to know what to kind of make of a given field if it's not included in the document. 
- ![[Screenshot 2024-11-13 at 8.48.38 PM.png]]
	- Optimizing Network Bandwidth
		- Description
			- So far, we have implied that in Avro, every single record needs to also include the writer's schema, which would use a ton of unnecessary network I/O.
			- Remember that Avro is most useful for Hadoop:
				- If we have a large file with many records encoded the same way, only need to send the writer's schema once
				- If we have records written many different ways, include a version number with each record that corresponds to one writer's schema
		- Okay, so how do we actually optimize network bandwidth because i just kind of mentioned that you know now with Avro in order to kind of decode a message, you need both the writer schema and the reader schema which is problematic if we're sending all of these messages over the network, we don't want to have to also include the writer schema. That can take up a ton of space. It'll slow everything down. It's not really acceptable, but do keep in mind that a lot of the time, what you're doing with Avro is dealing with Hadoop files and what that means is that you probably have one big file and all of these records encoded in the same way. So generally speaking, you can actually just go ahead and send that writer schema over one time and then use that for all of those different records. That being said, if you have a bunch of records that are written in all these different ways, what you can actually do is after encoding them, you can include a version number with each record that corresponds to the writer schema and then that way the decoding uh you know the decoding node can go ahead and pull the correct writer schema using the version number every single time it's decoding a message and that way you only have to you know send over all of the versions of the writer's schema once. 
	- Why Field Tags are Hard
		- Description
			- If we want to export all rows from a database to something like HDFS, we want to be able to fully automate this process. With Thrift or ProtoBuf, we would have to have somebody manually convert the database schema to a serialization schema to follow the rules of only increasing tag numbers (the database is not aware of this). With Avro, we can more easily automate the schema generation process because we can just create it from the existing columns of the database.
			- Very useful for ETL processes
		- Okay, so why are field tags bad because i kind of touched upon this before and this is kind of where Avro optimizes over both thrift and protocol buffers. Basically, the point is a lot of the time you want to be able to automate the task of having all of these different things in a database and keep in mind that database schema can be evolving over time and then we want to be able to automatically create that serialization schema from the updated database configuration. We want it to all be a very seamless process because if we're ever going to do like an ETL job which basically means pull everything from the database and plop it into something like a distributed file system, it's very important to kind of be able to automatically generate the serialization. If we had to do it manually by hand every time, that would be really annoying. So basically Avro allows us to more easily automate this process because like i said before, it doesn't rely on having to kind of just append new fields with a unique field tag but instead you basically just list out whatever you have in there and then the Avro logic will kind of figure out the differences between the reader and writer schema. Obviously if you do think about it and you kind of are logical, you probably can automate the thrift or protocol buffers process as well. However, it just seems a little bit easier to do it with Avro. 
	- Schema Evolution in Databases
		- Description
			- Data outlives code:
				- Database holds data written by multiple different schemas
				- It is up to the serialization framework to be able to rectify differences in data written at different times
					- Make sure that processes that are reading data encoded in an older format do not crash
					- Make sure that processes with an older version of the schema do not lose columns if performing a read-modify-update cycle
		- Okay and then let's also just quickly touch upon how schema evolution is really useful in databases. We have this concept of data outliving code which means that in a database, we have records that are potentially written from multiple different versions of a writer schema and we have servers that are reading rows from the database using potentially many different you know serialization schemas on read. So we have all these different schema mismatches and as a result of that being able to use this framework for evolving data, make sure that you have this forwards and backwards compatibility in order to make sure that processes don't crash from data that was eventually written to the database in different formats. You also just have to make sure to keep in mind that any time you're performing a read modify update cycle with an older version of the reader schema on the basically the node that's going to perform the update, that you basically go ahead and you know don't just like ignore all of the data that isn't in your reader schema because eventually when you make that update, you want to make sure that data is still included even if it wasn't important to you, you know in the actual code itself. So that's one thing to keep a note of.
	- Conclusion
		- Description
			- Ultimately, these text serialization libraries are extremely useful for providing some format to data that may otherwise be unstructured. Not only do they further compress byte sequences that need to be transferred, keeping a record/database of the schemas over time acts as an effective method of documenting the representation of the data through the progression of the application. Finally, in statically typed applications, the ability to generate code / classes to interact with is extremely useful
		- Okay, so in conclusion, basically these text serialization libraries are super commonplace, especially just within services that only stay within one organization, right? Once you're going to cross organizational services, you probably want to be using something like json because of the super high support but within one organization, protocol buffers, thrift, Avro can all save you a ton of network I/O, almost maybe even half as much. Especially if you're transporting terabytes of data over the network and so these have shown to be super useful not only for actually saving time via this kind of binary compression but in addition to that, the schemas act as effectively a documentation of all the different types of messages and the data evolution that you have over all of your services and that allows you to decode older messages or even just you know take a look and say like oh this is how we originally had things, that's how we changed it, and it's a really good form of documentation in that sense. So overall i know this is kind of like a topic that may not come up as much during systems design interviews, however, you know in practical considerations, being able to actually encode data or serialize it into a byte stream more efficiently is hugely important for saving time especially when working with big data in either batch or stream processing