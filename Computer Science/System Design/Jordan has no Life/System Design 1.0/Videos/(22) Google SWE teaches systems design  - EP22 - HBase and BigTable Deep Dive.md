---
Source:
  - https://www.youtube.com/watch?v=Pd3sIbWzlHw
Reviewed: false
---
- ![[Screenshot 2024-10-08 at 11.41.46 PM.png]]
	- Intro
		- all right um i'm back again uh welcome everyone. I should just say yesterday was the one month anniversary of this channel so i'm pretty you know overwhelmed and excited that we already have 260 people subscribed. So let's keep those numbers up but uh today we're going to talk about [[HBase]]. It's kind of wild actually because i was doing research for this topic in order to make a video about it and there's literally like probably 10 blog posts which are just blatant plagiarisms of one another and just copy the same like bullcrap information that go into absolutely zero depth. So i actually had to dive into a little bit here and then right as all that happens it occurs to me that Hbase is modeled after Bigtable which there's literally a ton of information on. So i'm an idiot but anyways uh let's get into this video.
	- [[HBase]] background
		- Description
			- HBase is a wide column storage database designed for either transactions or analytics processing, and it is built on top of HDFS. In this video, we will compare how it performs for certain operations with the other popular wide column data store, Cassandra
			- HDFS provides high latency writes and reads (since they all come directly from disk), and only allows for sequential writes as opposed to in the middle of a file - we will see how using an LSM tree based architecture HBase can mitigate this issue and provide low latency writes and reads on random accesses. 
		- all right so hbase. What is it um well the reason i'm talking about hbase in the order that i'm talking about it in is a couple things. First of all, it allowed me to touch on HDFS in the prior video but more importantly HBase is another wide column NoSQL storage database and the reason i want to talk about this now is the last database i talked about Cassandra was also a wide column NoSQL database. However, there are some pretty serious differences between the two even though on the surface level they look like they serve similar functionalities. So basically, we're going to compare those two but just to give an overview, even though Hadoop or the the file system provides pretty high latency writes and reads since they're all straight from disk and only allows basically sequential appends and truncates, we'll see how HBase uses LSM trees and SSTables in order to achieve better random read and write performance in addition to lower latency reads and writes performances.
	- Data Model
		- Description
			- Wide column format:
				- Each row has a single row key, and any columns that they want (or column families which consist of multiple values)
				- The row key can be comprised of multiple parts which allows the sorting of rows within a table
		- okay so in terms of the data model, HBase is a wide column database. So it's not identical to Cassandra but it looks pretty damn similar where each row has a single row key and then basically any column values that they want and there's this concept of like a column family as well which is kind of like you know combining uh you know two values to create a column. The row key also can and probably should be comprised of multiple parts so as opposed to having those clustering keys in Cassandra where that allows like an internal sort order, instead row keys just have you know maybe say multiple uh delineations inside of them and you have to be pretty clever when developing this row key to make sure that your data is sorted the way that you want it. 
	- Architectural Overview
		- Description
			- Master Server
			- Region Server
			- Replication
		- Okay so just as an image to kind of give you a sense of the architectural overview, we have this master server, a region server, and then you know kind of the replication that's going on in the background. There's also a zookeeper instance which i'll mention a little bit as well. But keep in mind that um you know if you're ever talking about HBase it's very much modeled after Google's Bigtable which basically does the same exact thing as HBase however it's built on the google file system or now actually the updated version of the google file system called [[Colossus]]. 
	- [[Master Server]]
		- Description
			- Many parallels to HDFS NameNode:
				- Stores all file metadata, as well as the locations of the chunks of files
					- Keep in mind that this is effectively the partitioning schema of HBase
					- Range based partitioning that can be split if too big, based on row key
				- For both reads and writes, client first reaches out to master server in order to figure out the location of the file that it will be reading to and writing from
				- Can be used in conjunction with ZooKeeper to replicate its write ahead log and thus support a backup master node, increases fault tolerance.
		- So what is the master server? Well if you remember the HDFS NameNode, it's actually very similar to that but it's kind of it's its own thing in HBase. So it's going to store all the file metadata and you know all the operations that occur on the metadata such as like renaming files or anything like that and also the corresponding chunks of where all the files are located and so this is basically how HBase does its partitioning. It's a range based partitioning on that original row key and if a given partition gets too big or it has too much load you can go ahead and split that up. The one thing to note here is that means that HBase is not good for usage patterns say with like time series data where you know the timestamp is the row key because then every single write is going to be going to one partition at a time and it's going to create hotspots. So in that case, you might want to use something like a hash of a key. Okay, in terms of reads and writes the client is going to reach out to that [[HMaster]] server in order to figure out the location of the file and then you know it's going to lead it to a region server which i'll touch upon next. And then presumably in order to ensure high availability, the master server like the master server and HDFS can use that zookeeper instance in order to keep a eventually consistent write ahead log and that way you can have a backup master server reading from zookeeper in order to eventually pull that write ahead log and stay consistent with the master server.
- ![[Screenshot 2024-10-09 at 1.25.41 AM.png]]
	- [[Region Server]]
		- Description
			- Run as a separate thread on HDFS datanode servers
				- Occasionally send heartbeats to ZooKeeper so master knows they are alive
				- Uses an LSM tree style in memory data structure where writes are first sent (in addition to a write ahead log for persistence)
				- Once the LSM tree becomes too large, writes are flushed to sorted HFiles
					- Basically the same as an SSTable
				- SSTables stored in column oriented format
					- Good for high read throughput and analytics over a range of the row key
		- okay what is a region server? Well a region server before you guys kind of get into the wrong mindset here. A region server tends to be run on HDFS datanode servers so as opposed to being its own dedicated server, usually it's just a second program that's being run on those data nodes in conjunction with whatever the program is running that actually you know makes a computer a data node. So what the region server does is it occasionally sends heartbeats to ZooKeeper. So basically says okay i'm alive, you can still send writes to me or reads to me. Additionally, on that actual region server, we're holding an in-memory LSM tree style data structure and that's where writes are going to first be set, so if you remember from literally my first video on this channel, we send writes to the LSM tree, this is a pretty efficient write structure because it means that they first go to memory and then reads, once you want them, first go to the LSM tree and if the LSM tree gets too big it gets flushed to SSTables and then if whatever the key it is that you want to read is not in the LSM tree, you look towards the SSTables and start sorting through those and there are a bunch of optimizations on that that you can do in order to kind of speed up that read process like bloom filters and you know other types of caching. So SSTables are actually going to be stored in column oriented format so that means that as opposed to just having you know the the key and then the entire value of the row, what you have are these sorted tables where it's all the values of the column in turn so that column oriented storage is something that i've mentioned in my data warehousing video but basically what it means is that you can achieve really high throughput over an entire table if you say you just want one column . 
	- [[Replication]]
		- Description
			- Recall: Region nodes, occasionally will write new HFiles (similar to SSTables).
			- Since the region node is (usually) running on an HDFS replication node, it uses the HDFS replication pipeline to synchronously replicate the HFile  to other nodes
		- Okay so now let's talk about replication because this is kind of the whole point of being built on HDFS in addition to kind of the analytical processing that we'll talk about next. So like i said region nodes generally speaking are going to be putting writes in this [[MemStore]] and then once that MemStore gets too big, they're going to flush them to an HFile which is just an SSTable and since the region node is generally speaking on or near an HDFS datanode, all it's going to do is once that HDFS file (so the HFile) is going to be propagated to that data node, the data node is going to use the replication pipeline of HDFS that we talked about in the previous video on this channel in order to go ahead and reach the required replication factor of that SSTable. So even though the region server is effectively going to be handling all writes and reads for a given like partition of data, it may technically be communicating with one or many of the replicas that is going to be holding that SSTable. For example if a given data node goes down the region server is probably going to have to be talking to a different data node.
	- Analytics Perspective
		- Description
			- Column oriented storage (all elements of a column family stored in one file) make for very high read throughput of one column
			- Being built on HDFS allows for good integration with both MapReduce and dataflow engines ([[Spark]], [[Tez]]) for high throughput analytics
				- Use batch processes to perform joins! Joins are not supported in HBase
		- um oh and furthermore uh we should just keep in mind the fact that technically that is strong consistency because um everything has to kind of succeed in HDFS for the replication to be considered successful and the write to be considered successful in the first place but okay in terms of the analytics perspective what good does it do us storing all of these SSTables on HDFS as opposed to just you know kind of having them on any normal database? Well for starters the fact that they are column oriented storage means that we can have really high read throughput when trying to read over all the values on one partition in a column and additionally being built on HDFS means really good integration with MapReduce and dataflow engines like spark and Tez. One thing to note is that since HBase is not a SQL database, it doesn't have structured data, you can't actually perform native joins. What you have to do is instead use something like a batch process to perform joins and i talked about those in the batch process video. Basically you are trying to find data associations perhaps even using two sets of mappers and then combining those together into one set of reducers. 
	- Conclusion
		- Description
			- While HBase looks very similar to Cassandra on a surface level, they serve very different use cases!
			- Cassandra:
				- Real time transactions processing database, super fast writes
			- Hbase:
				- Great as a data lake for running huge batch/streaming processes on, also capable of fast reads and writes
				- If you want to support more structured data with advanced queries, opt for a SQL based data warehouse
		- okay so in conclusion even though HBase looks really similar to Cassandra on a surface level, they're very very different in terms of what you want to be doing with them. Cassandra's really nice for real-time transactions processing. So that means that basically anything user facing where you just want like a really quick write so you can get them a really quick success message and then have that in the database to you know have that eventual consistency between all of these Cassandra instances, that's good, that's kind of like this leaderless replication strategy. It's very um based off dynamo. HBase on the other hand is a master, you know master oriented database in terms of the replication strategy and so even though they both use LSM trees, HBase isn't going to be able to handle those writes as fast. However in terms of reads and analytics processing, you can achieve very high read throughput over a column which is super useful if you want to run a huge batch job or a stream job. This is great um for something called being a [[data lake]] which is basically saying i'm going to dump a ton of unstructured data in kind of like a database format into one place and then eventually run analytics later. It's just that the advantage of doing this with HBase over HDFS itself is that you can get nice read and write performance for both random reads and writes in the event that you do have to do some general like you know transaction processing. So it's basically a data lake that allows you to do some transaction processing but keep in mind that data lakes also are not exactly the same as data warehouses which are specifically for structured data with a ton of data associations. If you recall, data warehouses use that whole like stars and snowflake schema where you have a huge fact table which is very structured and then a ton of dimension and sub-dimension tables which reference the fact table. (Never mind, actually the fact table is going to reference those dimension tables.) But the point is if you just want to be able to run SQL queries here, HBase is not the thing but if you want to be able to dump in a ton of data and then eventually run you know a ton of expensive batch jobs on it, then HBase is really nice for you. So yeah all right guys i think uh next i'm going to start moving into key value stores so i can talk about things like [[Riak]], [[Redis]], [[Memcached]], and just caching in general all right have a good one.