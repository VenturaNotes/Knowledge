---
Source:
  - https://www.youtube.com/watch?v=JIAAlb3AtjA
---
- ![[Screenshot 2024-10-09 at 3.56.40 AM.png]]
	- Intro
		- all right, it's time for another video. I want to thank everyone else who's new again for subscribing because it seems like there's a lot of you today and that is sweet. Um just to give a quick little rant before i start today's video. Um i hope that all this stuff has been useful just as kind of like a primer for those of you who are first starting to look at this channel. I see a ton of systems design channels out there and that's great and i'm trying to make myself unique for most of those by you know starting to do deeper dives and actually providing something of value to you guys because the reason i started this channel was I read designing data intensive applications like probably a lot of you have or will do at some point if you're watching this and then you're going to wonder well what do i do from here. How does this actually translate to a systems design interview and how do i actually hone my skills in a way that you know reading that book was worthwhile, and that's where i hope this channel comes in. I mean i literally see a million computer science videos on youtube and they're all so useless like it's you know tips and tricks to put on your resume and this is how i got my internship and like those channels are literally all more redundant than a replicated database and at the end of the day like i'm just trying to provide something of value to you guys and you know hopefully help out the community as a whole. So without further ado, I'm going to do a pretty introductory video on caching which you could probably get from other channels but for the sake of completeness on my channels, I want to do it and then I am going to do a deep dive into some of those in-memory services that you can use in order to kind of implement caching on a distributed scale. All right. 
	- [[Caching]] Background
		- Description
			- In all technology (not just distributed systems), caching is a way of providing faster reads to the end client. It tends to do so by using some faster form of data storage (in distributed systems this generally means storing data in RAM, not disk).
			- Caching accomplishes three main objectives:
				- Storing precomputed values (either previous calls to a database or aggregations)
				- Fewer network calls to the database (which is probably located physically further away)
				- Fewer load on the database, as it already is under plenty
		- Okay so introduction to caching. Well what is caching? Caching is a super useful thing that pretty much any super large at scale system is going to need where you are basically going to provide lower latency reads and oftentimes even writes to the end user. This is generally done either by improving data locality which means you hold the data closer to the user or you're using some faster form of data storage and in distributed systems what that generally means is you're storing data in RAM as opposed to disk. Caching accomplishes three main things. The first one is there are going to be fewer network calls to the database which is probably going to be further away than where your cache systems are located. Additionally if you ever need to do huge computations or you know if you're making a duplicated request to a server, you can store a bunch of pre-computed values. We've spoken about this in analytics processing with something like data cubes and then finally it reduces the read load on the database heavily and possibly even the write load by virtue of intercepting a bunch of these operations and handling them on their own. 
	- Tradeoffs of Caching
		- Description
			- Pros:
				- Faster reads
					- However if we attempt to find our data in the cache and it is rarely there, cache slows things down
						- This can happen because of thrashing (value that we want in cache keeps getting replaced right before we want to read it since cache size is too small)
						- Can also happen because of poor cache eviction policies
				- Potentially faster writes
			- Cons:
				- Increased complexity on each write
		- Okay so what are the main tradeoffs of caching? Well there are a ton of pros here and you know obviously they're cons but if there weren't so many pros, not everyone would do it but here are the pros. The pros are hugely faster reads. A read from a cache can be well over in order of magnitude which means 10 times more than a read from a database. However if we attempt to find our data in the cache and it's not there, which is called a cache miss, then having that cache is going to slow things down so there are kind of a couple of scenarios where this can happen. One of them is called thrashing. Thrashing is when you know let's say we have a small cache and me and my friend are both trying to load our facebook profiles but when i load my profile and it's originally not in the cache, now we have Jordan's profile on the cache then he loads his, so now his is in the cache. It's actually replaced mine because it was such a small piece of cache and then i try and load mine again. Now mine is no longer in the cache, we have a bunch of cache misses and we've actually slowed things down quite a bit. This can also happen because of poor cache eviction policies. I guess thrashing is technically an example of a poor cache eviction policy but you know generally there are other poor cache eviction policies as well that aren't just you know because the size of the cache was too small. Additionally caches can potentially improve the speed of writes and I'll touch upon that in a bit. The biggest con is the increased complexity of each write or generally speaking just you know kind of the schema that you're going to use to keep the data and the cache valid and up to date. This is kind of a design by design type of thing and obviously there are trade-offs for each design but you really have to think about what users need for your application and then make the right decision here. There's not one objectively correct answer for any approach.
	- Cache Eviction Policies
		- Description
			- First in first out
			- Last in first out
			- Least recently used
				- Out of these this is the most practical one for an interview answer, discard the piece of data that was accessed longest ago
				- If it comes up, this is implemented with a doubly linked list and a hashmap
			- Least frequently used
			- Random replacement
			- Sliding window
				- Can work very well but still being researched, probably not worth mentioning in this video
		- So what are some cache eviction policies. Well first of all, let's start by discussing what a cache eviction even is. Like i said, generally speaking we're storing our cache data in ram. RAM is more expensive and as a result of that most computers don't have a lot of RAM and so generally speaking, when you get a bunch of data in your cache, eventually you're going to have to evict some of it which means that there's newer data which would be better off going in the cache and you're going to have to get rid of some older data. So what I've done here is I've listed a bunch of cache eviction policies but the truth of the matter is most of these suck and the only one that you should really be using probably out of these is least recently used. You'll see i wrote sliding window at the bottom there which actually can work very well but it's kind of still a matter of research and it's not as widely implemented so for now let's ignore that and just say that least recently used is the most valid one and so you know if it comes up in a systems design interview or something and they're like how might you implement this. This is a leetcode problem and the answer is a doubly linked list and a hash map. Every single time you access something, that means it's been recently used so you put it at the head of the linked list and if you're going to delete something from there, you have to delete it from the doubly linked list and delete it from the hashmap.
	- Application Server and Global Cache
		- Description
			- Two possible options for implementing cache are either to use memory on our application servers as cache or instead to use standalone nodes as dedicated cache servers
			- Application Server Cache:
				- No extra network calls required from server to hit cache
			- Global Cache (Generally better):
				- Can scale independently of number of application servers and does not crash with application server
				- Can be accessed by any application server
				- Can be replicated and partitioned, we will see these in play with Redis and Memcached
		- okay so now we have a couple types of cache that we can actually think about because the question is you know wow all of our application servers are accessing the database and all of the results from the database are going through the application server. So wouldn't it make sense for us to just use the application server's ram as a cache and the answer to that is well kind of but not really. So even though the application server cache helps a lot because it means that you don't have to make any extra network calls hitting some dedicated cache server, the fact that each server has its own cache means a couple of things. Means that one, all of those cache results are kind of limited to that server. Another server can't really access them. Two is that if that application server dies, the cache goes with it and three is that if you want you know to add a bit more cache or you know add more servers without adding more cache, application servers are going to scale linearly with the application server cache. So generally speaking what most people choose to do is use a global cache and you know this might be something like Redis or Memcached but some service that actually allows you to use nodes as an independent cache and you know obviously we'll talk about those services in a future video but generally speaking global caches are good because they scale independently of the application server cache and they also don't die when an application server does and additionally they can be accessed by any application server and then furthermore because we have all of these nodes, they can be replicated and partitioned in any schema that we want and we'll see this in play eventually.
- ![[Screenshot 2024-10-10 at 1.05.37 PM.png]]
	- Keeping Cache Updated on Writes
		- Description
			- Write through
			- Write around
			- Write back
		- okay so in terms of keeping cache updated because we know that obviously we want to be able to get fast reads from the cache, however if we're writing data which most applications are generally speaking the data in the database is changing over time, you don't want a ton of stale data in your cache. So obviously you can just have you know cache data eventually expire after say a couple hours and then when you re-access it, it'll be repopulated with the newest stuff but generally speaking you don't want to just wait on some time for it to expire and you want to kind of take a more active approach in keeping the data up to date. So i'm going to look at three approaches of writing in a system that implements cache and we'll see kind of the trade-offs of each of those. So the first is actually going to be write through cache.
	- Write Through Cache
		- Description
			- Write both the cache and database at the same time, in parallel!
			- But what if only one update works? Depending on how much it matters, we many need a 2 phase commit protocol, which really slows things down.
		- So just to quickly explain this diagram. The computer is basically going to be like a client and you know let's assume there's just some application server that is attached to that. The red thing is the diagram for Redis, but for right now let's just call that a cache server and then that silver thing is the database. So the write through cache is basically doing the following. We're going to write both the cache and the database at the same time in parallel and obviously a write is going to be successful once you get successes from both of those. If you have, you know, thought about this a little bit, you can see that this is going to be pretty problematic in the sense that anytime you're writing two different things over the network, it is probably going to require some sort of distributed or atomic transaction unless you really don't care about the consistency that much. You know, if we have to do an atomic transaction, what that means is that we probably need two-phase commit because you know these aren't identical replicas of one another so we can't just use Raft here. What we do probably need is two-phase commit with a coordination manager and as a result that's gonna really slow down the writes.
- ![[Screenshot 2024-10-10 at 1.07.28 PM.png]]
	- Write Around Cache
		- Description
			- (1) Write the database, and also invalidate the current cache data for that key.
			- (2) Client requests data for the given key
			- (3) Cache requests data from the database for said key, and returns it to the client
		- Another option is right around cache. So what you do here and this is probably the most simple implementation of writing with a cache system is you just write to the database as expected and then the other thing you have to do is also invalidate the current cache data for that key. So you basically just go to the cache and say delete the data for this key. 
		- The client is then going to eventually request to read from the cache for whatever data it is that you updated. 
		- And then eventually what's going to happen is the cache is going to say wait i don't have that database. Give me the data and then report it back to the client.
- ![[Screenshot 2024-10-10 at 1.11.06 PM.png]]
	- Write Back Cache
		- Description
			- (1) Write data only to the cache
			- (2) Eventually, write data from cache to the database, possibly many writes at once so as to reduce network calls
			- While this approach has the fastest write speed for the client, it obviously does not work for data where we can not deal with eventual consistency such as password changes. In this scenario, such a write policy is unacceptable.
		- so that's obviously decently simple but the issue with that is it's going to result in a cache miss the first time. So now moving on to the third scheme, we have write back cache. So this one's pretty interesting, we actually write data only to the cache at first and then eventually the data is going to go from the cache to the database possibly even in bulk and that way we make fewer network calls to the database and make sure that everything is actually going to get there in one piece. So even though this approach has a fast write speed, it's definitely got its downsides. In the case where we can't deal with eventual consistency such as something like changing a password because obviously you need the most updated thing to be in the database which is our source of truth, write back is not going to work. Additionally since all of our data in the cache is going to be in memory, if we were to write some data to the cache and then that cache went ahead and failed, then we just lost our write so maybe that's not the hugest deal to have lost writes but in some systems it certainly is. So in that case write back cache is unacceptable.
- ![[Screenshot 2024-10-10 at 1.30.13 PM.png]]
	- Write Back Cache with Cache Consistency
		- Description
			- What if we did want to have fast writes using a write back cache, but also want other users to be able to see the changes held on the cache when they want to read the modified object?
			- We could use a distributed lock service (such as ZooKeeper), where a cache server needs to hold a lock in order to modify that object. When another server wants to read the object, the lock server informs the cache server that it must release its lock on the object, and write back the modifications to the database before doing so.
			- This obviously has the tradeoff that only one server can hold the lock at a time and therefore ruins the ability to have high write throughput from many servers.
		- What if we wanted to be able to have a slight modification of write-back cache where we actually achieved some sort of cache consistency? So not only would we use write-backs so that clients see fast writes on their end, but additionally when another server say wants to read that data that may have been modified, we make sure that they're actually reading the stuff that could still potentially be in that cache. Well the way we could do this is by using something like a distributed lock service so i've mentioned ZooKeeper and Etcd in the past and you know those are based on coordination services and consensus algorithms to work and basically what's going to happen here is every single time one server wants to read a piece of data that may have been modified by a cache, we know that any cache server that has modified a piece of data in order to do so must grab a lock for that object from the lock service and so when another server wants to read it, it'll basically say to the lock server, okay i'm trying to read this data, the lock server will say okay i'm going to have to kick the cache server off the lock and the cache server will then say okay but before i get off the lock, I'm going to go ahead and send my modifications back to the database so that other people can go ahead and read this. So that's kind of one way of dealing with that. If you've ever heard of the [[Frangipani file system]], this is kind of the approach they took where you could make cached writes on your local machine but if anyone else wanted to go ahead and read your writes from a remote machine, the local machine would have to give up its locks and send those changes back to the central server store. Obviously the downside of this is that since only one server can hold the lock at a time, it's going to greatly reduce both the write and read throughput of whatever object it is that you're trying to modify. 
	- Caching in Practice
		- Description
			- In actual systems, cache servers treated very similarly to databases!
				- Replication for fault tolerance
				- Partitioning for large datasets
				- Coordination services for cluster management
				- Secondary indexes (not just a hash table?)
			- In the following videos, we will see actual existing technologies that have built out distributed caching solutions using all of these principles!
		- Okay so in practice I've kind of talked about cache write now as this abstraction where it's just like a single memory server and the truth of the matter is that's obviously not acceptable. If the cache server goes down, we want it to be available or at least the data that it held to still be available. If a cache server isn't big enough to hold all the data that should be getting cached, it probably needs to be partitioned. Cache servers need to be able to coordinate with one another or communicate with one another to say who's up and who's not and additionally sometimes it's not even sufficient to just search by keys and values. Sometimes we want something like secondary indexes. So in practice, we have all of these you know third-party services that have been built on top of you know the idea of caching, but they add all of these concepts that we talked about in the past in this channel and so we're actually going to take a look at those next and that's things like Redis and Memcached which i think is a little bit more simple than Redis but we're still going to end up talking about them both. 
	- Caching Conclusion
		- Description
			- Caching - Improves read latency, and depending on design, possibly write latency
			- Generally better to decouple cache from application servers for ability to independently scale and communicate with all servers (at the cost of more network requests).
			- As for writes:
				- Write back for minimal write latency, but at the cost of eventual consistency
				- Write through for maximum consistency, but may have to use distributed transactions
				- Write around for simple implementation, but at the cost of an initial cache miss
		- So in conclusion caching is a super important part of any distributed system with a ton of users and users that kind of need low latency. It greatly improves read latency and depending on the design of your actual system, possibly even the write latency if you're using something like a write back cache. Generally speaking, you want to be decoupling your cache services from your application servers so that they can grow independently and scale on their own and also not fail if a server does but of course this comes at the cost of having to make additional RPC or network calls every single time you're updating or changing the cache and then in terms of writes in a system with cache being implemented, we want to use write-back cash for minimal write latency but of course that's at the cost of eventual consistency of you know the cache and the the database which you know may not be feasible for your use case. Um if you want maximum consistency between the cache and the database, then write through is your move. However it requires two phase commit or some sort of other atomic commit algorithm which is going to slow things down quite a bit and then write around cache while being the most simple comes at the cost of having an initial cache miss every single time you go ahead and make a write to the database so that the cache can actually repopulate itself with the correct data. Okay guys i hope this serves as a decent introduction or primer to caching. I'm going to go much more in depth in terms of actual services that you can use for caching and possibly even what existing companies have done to make caching work for them and keep everything consistent and coherent