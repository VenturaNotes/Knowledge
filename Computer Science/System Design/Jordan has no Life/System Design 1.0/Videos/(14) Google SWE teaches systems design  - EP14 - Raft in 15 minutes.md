---
Source:
  - https://www.youtube.com/watch?v=YOMgi7T823E
---
- ![[Screenshot 2024-09-27 at 3.21.50 PM.png]]
	- Will talk about [[Raft]] today
		- It is a consensus algorithm that came out in 2014
		- It's really popular. Getting a ton of usage even compared to older ones like [[Paxos]]
			- This is because it's easy to understand (made to be easy to understand)
		- Seems like Jordan is 21 at the making of this video
	- [[Raft]] background
		- Description
			- A consensus algorithm that displays fault tolerance! Used to build a replicated log, such that each node will eventually have the exact same log. As a result, this can be used to achieve total order broadcast. Many similarities to [[Multi-Paxos]].
		- Raft is really just a consensus algorithm that has some level of fault tolerance. This means we can use it to build a replicated log meaning every single position in the log is a decision that is made by the raft algorithm and as such, we can use this to create total order broadcast. Eventually every single node will have the same exact log which means we have this idea of eventual consistency. If you've heard of Paxos or Multi-Paxos to create a fully replicated log which is meaning you're doing Paxos but for multiple stages so every single entry of the log, there are a lot of similarities there but not identical
			- In terms of how it's actually used, imagine we have these 3 database replicas at the bottom of the screen here. Every single replica has a log associated with it. Imagine it's the write ahead log. This log is going to be running in a raft instance on the database and all it's going to have is the operation on the log and then something called a term number which will be explained later
			- As you can see, they're not all completely identical but eventually they will be
	- Review Terms
		- Description
			- [[Quorums|quorum]]: A majority of the nodes in the cluster - it is impossible for two different quorums of nodes to make conflicting decisions about anything as they must overlap in at least one node
			- [[Fencing tokens|fencing token]]: An increasing number used in situations like split brain in order to resolve a conflict (higher token number wins), in Raft we will call this the term number
			- [[Leader]]: A single node through which all writes are sent and propagated - if the leader fails, we must find a way to switch to a new leader
		- What's a quorum? A quorum is a majority of nodes in any cluster of nodes in this case it's a majority of nodes in a database cluster of replicas. If two different quorums of nodes have made some decision as in all of the nodes in that quorum agree on something, they cannot make conflicting decisions. Because that means that one of those nodes must overlap within the two quorums and so it's impossible to have a conflicting decision because one node can't decide two different things say the same entry of a log. 
		- Additionally, we discussed something called a fencing token in a previous video which is basically what the term number is in Raft. An increasing number used in situations such as split brain where there might be multiple leaders or even with a locking system to basically go ahead and say take the higher number, that one wins. If there is ever a conflict about two nodes believing that they have this one resource whether that's being the leader holding a lock, the higher fencing number wins
		- Finally, we have this concept of leader. In Raft, the leader takes all of the writes and even some of the reads if you want strong consistency but there are other ways to get strong consistency on reads by doing things such as quorum reads. A leader is going to go ahead and take in all the writes and propagate those to the other nodes which are known as followers
	- [[Raft]] Overview
		- Description
			- One leader sends all writes to the follower nodes
			- Once a leader receives responses from a majority of nodes accepting the write, it can tell them to commit said write to the above database layer
			- If a leader is presumed to be dead, a follower node will begin an election to become a new leader with a higher term number, and will only consider itself the new leader if it receives responses from a majority of nodes
		- Let's quickly go over the way Raft works on a very high level. Then we'll dive into how each of these pieces actually works. So basically there is one leader, at least at a time. There is one leader per term where each term is identified by a number and the leader is going to send all of the writes for that term to the follower nodes. Once a leader receives responses from a quorum of the nodes, so that would be a majority, it can go ahead and tell them to commit those writes to the above database layer. Once they're committed, the above database layer is going to put them in their key value store and then finally reads can actually go ahead and read that key in value. 
		- Finally, the biggest thing here is that with single leader replication, if a leader is presumed dead, you kind of just have to do a failover but that's not always an easy process to do and it's not always automatic. In this case, if a leader is presumed dead, a follower node is actually going to potentially go ahead and try and become the new leader. Once that happens, there is consensus reached amongst the follower nodes about who the new leader is and we don't have to worry about split brain. Raft actually solves that for us
	- [[Leader Election]]
		- Description
			- In Raft, each follower node periodically receives "heartbeats" from the leader to inform the follower that the leader is still up and running.
			- If a follower node does not receive a heartbeat within some timeout, it will assume the leader to be dead and declare itself to be a candidate and start a new leader election process, where the candidate votes for itself (increase internal term number by 1)
			- So that we don't have every follower starting new elections at the same time (because then no one can get a majority of votes), the heartbeat timeout is randomized over a reasonable range on each follower
		- Let's talk about leader election first. This is the big reason why Raft works which is that there is only one leader and there can only be one leader at a time per term. In Raft, every node is either a leader, a candidate or a follower.
			- Each follower node, these are nodes that will accept writes periodically receives heartbeats from the leader to say I'm still running, don't try and replace me. However, if a follower node does not receive a heartbeat within some amount of timeout where this timeout is typically the length of a few heartbeats up to the amount of time it might take for a new election process to ensue, it will assume the leader to be dead and declare itself to be the state of candidate. Once a node calls itself a candidate, it will start a new election process, vote for itself and tell all the other nodes that it's starting said election process. It is also increasing its internal turn number which we can call a local term number later by one. So it's saying this is a new term and we're going to have a new leader or maybe the same leader as before but under a new term number
				- Next, because this follower node who's now a candidate node is going to have to have a majority of votes to become a leader, we don't want every single follower starting new elections at the same time because if they do, they're all going to vote for themselves and then no node is actually going to be elected a leader. So we randomize the timeouts that each follower node has on it locally to determine when a leader is dead and what this does is it means that a follower node will typically you know one might go off and say oh I think the leader is dead and the others will still think that the leader is alive but know that a vote is going on. So that heartbeat timer is going to be randomized uniformly over some distribution where the distribution is the length probably at minimum of a few heartbeats but also still enough to make sure that an entire election process can ensue without some other node saying wait, I think I'm the candidate and now I want to try myself
- ![[Screenshot 2024-09-27 at 3.48.05 PM.png]]
	- Leader Election Continued
		- Description
			- At this point, the other nodes will be informed that an election is ongoing:
				- If the candidate term number is higher than the local term number, they once again become a follower (even the current leader) and change their local term number to the candidate term number
				- If the candidate log is more up to date than the local log and the node hasn't voted for any other candidate this term, the node will reply saying that it will vote for the candidate
				- Else, it replies saying it does not vote for the candidate
			- On receiving these vote responses, the candidate node will keep track of the set of nodes that have voted for it for its local term, and if the number of votes reaches a majority, will declare itself the leader!
		- Let's keep talking about leader election because now we have to talk about what happens when a follower node receives word that a new node is the candidate. There are a few things that we have to check. Firstly, if the candidate term number is higher than the local term number, the local or rather the follower node is going to receive the fact that there's an election needs to become a follower and that's the case even if that node was a leader and then additionally change their local term number to the candidate term number. Now we're saying, okay we're moving on to a new term, everyone has to update this fact. 
		- Additionally, we're going to go ahead and say if the candidate log is more up to date than the local log and I'll talk about how you can determine if a log is more up to date than another because there might be conflicts there. But if the candidate log is more up to date than the local log and the node aka the local node hasn't voted for any other candidate this term and it knows if it has because it's keeping track of what term it is and what nodes it's voted for in a given term, the node will reply saying okay you have my vote. 
		- If those conditions are not met (either the candidate log is not enough up to date or the nodes have already voted for someone else this term) it does not vote for the candidate and it responds to the candidate saying I'm not voting for you
		- Once the candidate receives all these responses, it tallies them up and basically it says okay, if I reach a majority of nodes in the cluster if I get a response saying that they voted for me, I'll declare my self the leader. Otherwise, we have this concept of an election timeout and if it doesn't hear back from all the other nodes, eventually it's going to say okay I give up on this election
	- Broadcasting Messages
		- Description
			- When the leader receives a message to broadcast, add it to the local log (don't commit it yet), and send it all the other nodes via some function which I will for now call ReplicateLog
			- Additionally, periodically call the ReplicateLog function for each node to act as a heart beat, keep logs in sync, and also alert other nodes to commit messages that should be committed to the database layer
		- So we now discussed how we have our leader, how is our leader actually going to pass these writes to the follower nodes? The leader is going to receive a message to broadcast (might receive from client itself or the client gives it to one of the other nodes and the other nodes passes it to the leader via a FIFO channel so we make sure all those messages are coming in the same order). Then it's going to add it to its local log but also make sure not to commit it. Even though the messages in the log, the database part can't actually see the message and won't make the change. It's going to send it to all the other nodes via a function which for now will be called ReplicateLog.
		- Additionally, ReplicateLog is going to be called periodically so that it acts as the heartbeat that a leader sends to follower nodes in the event another election needs to happen. Then finally, it also helps that ReplicateLog is periodically called so the leader can alert the other nodes if there are any new messages to actually commit which means (tell the database hey, this is a valid write)
	- Replicate Log Function
		- Description
			- For each node, leader keeps track of how many messages it has sent. It uses this to try and split its local log into a prefix and suffix, where the prefix is all the local messages that the remote node has already seen, and the suffix is all the local messages that need to be appended to the remote log. It then sends over the suffix messages. It also sends over the term number of the last message in the prefix.
			- Note: the leader may be wrong here about what the prefix and suffix are and have to readjust later, I'll explain soon!
		- So let's talk about this replicateLog function because this is pretty important because this is kind of why Raft works. For each node, the leader has its local copy of the log and for that local copy, the leader is now keeping track of every single follower and how many messages that it's sent to the follower and how many messages that the follower has acknowledged that they've received from the leader. Using this can try and determine which messages the follower has on hand which it'll call a prefix and then all of the new messages that the leader contains but they assume the follower doesn't contain is called the suffix. 
			- So what it's going to do is send all of the messages from the suffix so new messages that they assume the follower doesn't yet have to that follower and in addition it's going to send the term number of the last entry of the prefix. I'm going to explain why it does that in a second
			- Another thing to note is that the leader is not always right about what the prefix and the suffix are for each node. It might be the case that a certain follower node has less of the correct messages in its log than the leader originally thought. If that's the case, it's going to have to go back and make further changes
			- Right now, all that's important is that we're going ahead to send those suffix messages to be appended on the follower and we're also sending the term number of the last entry of the prefix. Why are we sending that term number of the last entry of the prefix?
	- Prefix Invariant
		- Description
			- [[Raft Invariant]]: If the logs on two replicas have the same term number at the same index in the log, they MUST be the same up to and including that index. 
			- Not Possible: If the first log had (D, 2) in index 0, the leader would not have been able to append (B, 3) to the follower node because the entries at index 0 would have different term numbers, and as a result the leader would have to change its guess for the prefix for the follower node and overwrite that entry of the follower. We know the leader is right here because the only reason that it was elected in the first place was that it was equal to or more updated than a majority of nodes which means that the leader itself must be up to date. (I know this is a lot, just focus on the invariant)
		- Well, because there is an invariant about every single replicated log in raft which basically says this: if the logs on two replicas have the same term number at the same index in the log, they must be the same up to and including that index. So looking here at these two database replicas, as you can see in index 2 assuming it's a 0 index log, we have the entry C for the operation (so like write C) and then the term number is 4. We know that if these guys have the same term number, it means that the leader has written to both of them and as a result of that, anything beforehand must've been correct so that the leader could've written to them in the first place. 
		- Something like the situation described above for "not possible" isn't possible where they both have C4 in that 2 entry but they differ in the zeroth entry with a A3 and D2. If you think about this invariant a little more, it's kind of a lot to think about but eventually becomes clear why it works. If you want to pause this video and read my note here, you definitely can but the general point is we know that the leader is correct and that's the reason it was elected in the first place. If the follower has a different prefix at a point, the leader needs to go ahead and write that prefix and not just make new appends to the log. It's gotta go ahead and change its logic
- ![[Screenshot 2024-09-28 at 1.05.13 PM.png]]
	- Replicate Log Function Continued
		- Description
			- Because of the invariant, if a follower node has the same term number at the end of its prefix as the leader, we can just go and copy over the rest of the suffix nodes because the follower was up to date (overwriting any log entries from a different term)! Respond to the leader saying that the new messages were acknowledged. Additionally, check for newly committed messages from the leader and commit those.
			- If the prefixes of the logs were not the same (or the follower has a higher term number than the leader), reject the write and report back
		- So basically, continuing to talk about the replicate log function, because the invariant just previously mentioned, we basically know that if the term number at the end of the prefixes are the same, we can just go ahead and copy over that suffix and put in those correct messages and then the log of the follower will be the same as the log of the leader
			- If the prefixes are not the same, then we know that the assumption that the leader made about what the follower already has in its log is bad and we're going to have to report that back to the leader. When the leader commits the suffixes to the followers which it only does if the prefixes are correct, then we'll go ahead and have the follower check for newly committed messages so that it can commit those to the database layer
	- Receiving Write Acknowledgements on the Leader
		- Description
			- If the leader receives word from a follower that a write was successful, it keeps track of this and waits for a quorum of  positive responses to commit the message and subsequently alert a follower
			- If it hears that a write was not successful, it must have been because the prefixes of the leader and follower node were not the same, and as the result it must try the write again with a smaller prefix (and overwrite more incorrect entires on the follower log). Also possible that the follower it sent the write to now has a higher term number in which case the leader gives up being a leader and becomes follower.
		- In terms of receiving writing acknowledgements on the leader, as we just mentioned the reader wants a quorum of write acknowledgements so if it receives a majority of acknowledgments for a successful write for a given log entry, it's going to go ahead and keep track of these. Once it reaches that quorum, it goes ahead, commits it locally and then sends that commit out to all of the followers. 
		- Additionally, if it hears the write wasn't successful, it must be because the prefix of the leader and the follower node were not the same or because the fact that they had a different term number between them. If the term number of the follower is higher than the leader, the leader now knows that it's no longer the leader and has to give up its leader status and transition to follower. If it's because the prefix of the follower was not up to date enough, then what the leader is going to try and do is try to replicate the log again but with a smaller prefix and see if it works this time. That might potentially be a loop that spans very far where you just keep adjusting the prefix to be smaller and smaller assuming that the follower knows less and less until you finally reach the amount of the log that they have in common and then you can go ahead and replicate what you have
	- Raft Conclusion
		- Description
			- Even though the original Raft paper is from 2014, it is already hugely popular, mainly due to how easy it is to understand!
			- Still takes a lot of network calls, and compared to something like 2 phase commit, is good for making replicated logs but not so great for cross partition atomic transactions. Nonetheless, it is far more fault tolerant as it can support a leader failure while also being able to only write to a quorum of nodes at a time.
			- In a future video, I will do a comparison of Raft with other consensus algorithms, while they are generally similar, there are some subtle differences that can affect real world performance!
		- In conclusion, even though the raft paper is from 2014, it's gained a ton of popularity because its ease of understandability. I just explained it to you guys in 15 minutes so clearly it's not that bad. It's hugely popular. Still takes a ton of network coordination to do it all. Need to reach out to every node or at least attempt to do so and get acknowledgements from a majority of them
		- Compared to two-phase commit though, the fact that it's so fault tolerant is really great. That being said, there are still times where 2PC is more useful than Raft and the main example of that are things like atomic commit across partitions. Obviously in Raft, if we want to make writes to multiple partitions, it's not enough for only a majority of them to say, okay yeah I got this. All of them have to acknowledge it and then maybe within the partitions, we replicate each partition, you can use something like Raft.
		- Nonetheless, it's really important to understand this algorithm as it gives a great idea of kind of a lot of the concepts we've talked about such as quorums, fencing tokens, and even two-phase commit in general how there's kind of this idea of a prepare phase and then eventually committing that write to the database layer. Ultimately, it's really important to know about Raft and not necessarily every step of it but generally what you might use it for and additionally in subsequent videos, how it differs from other consensus algorithms like Paxos or Zap and i'll get to those eventually too. Either way guys, this is probably the most theoretical video I've had so far but I hope it was easy enough to understand and have a great day