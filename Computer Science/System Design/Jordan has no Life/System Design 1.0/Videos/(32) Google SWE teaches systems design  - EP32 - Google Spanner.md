---
Source:
  - https://www.youtube.com/watch?v=y6mnZKls3iI
---
- ![[Screenshot 2024-11-11 at 10.05.16 PM.png]]
	- Introduction
		- Today's video will be about spanner which is an interesting concept. I don't know how overly relevant it'll be for systems design interviews but i think it would be actually a really cool thing to bring up especially as it pertains to you know clock synchronization and newSQL in general and how you can actually use something like this in perhaps a large enterprise application. 
	- Spanner (Background)
		- Description
			- Spanner is a database created by Google that falls under the category of NewSQL - it uses designs over the same familiar relational data model of SQL in order to greatly increase its performance
			- Spanner mainly relies on using synchronized clocks via their service known as TrueTime! While there are other similar databases to Spanner, such as CockroachDB (which basically is the same), its lack of ability to ensure that all servers have closely synchronized clocks makes the solution a bit less viable.
		- Okay so spanner what is it? Well basically spanner is a database created in google more or less in the last decade so probably i think 2012 is when the paper came out that falls under the category of NewSQL. So this is basically new architectures to traditional relational databases that try and make improvements that make them kind of more viable in a super scaled out environment because as we know, I've talked about SQL has many pitfalls and even though it's the right choice a lot of the time at super large scale it tends to fall apart because of all of these relational constraints and locking and things along those lines. So spanner mainly relies on actually using synchronized clocks. So the first thing you should you know think about when you hear that is well clocks are never synchronized. This is distributed systems. Well actually they kind of come up with an interesting approach to trying to fix that and once they do actually fix that, they can kind of go and make optimizations from there in order to make a quickly performing database. There are other companies known as i think like [[YugabyteDB]] and [[CockroachDB]] that kind of use similar ideas but i think don't have true time to the full extent that spanner does and maybe have you kind of synchronize your own clocks and so that makes them a little bit less reliable.
	- Spanner Guarantees
		- Description
			- ACID transactions
				- Atomicity
					- Achieved by two phase commit across shards
				- Serializability
					- Achieved by two phase locking on a single node (Spanner optimizes on this!)
			- Replicas are kept consistent via a consensus algorithm
			- Linearizable reads and writes
			- All while maintaining great performance!
		- Okay so what are the spanner guarantees before i go into actually how they make them work. Well for starters there are acid transactions which means that you get atomic operations so that's achieved to be a two-phase commit. You also get serializability and this is basically achieved by two-phase locking so if you recall that's kind of when you have shared and exclusive locks on every row um and then there are also a bunch of replicas in order to ensure availability that are kept consistent via paxos instead of raft but you know same thing for the most part. There's some slight differences between the two but that's probably for a future video and then linearizable reads and writes. What that means is that every single read and write in the spanner database system is totally ordered and trying to keep consistent with causality. So they can do this basically all while maintaining great performance at scale which is pretty surprising and the way they're able to do this is by optimizing on that whole two-phase locking. So i'm going to go into how they actually do that. 
	- The Issue with Two Phase Locking
		- Recall: In two phase locking, a transaction may hold the lock on an object in either shared or exclusive mode.
			- If T1 holds lock in exclusive mode, T2 cannot read it until T1 lets go
			- If T2 holds lock in shared mode, T1 cannot write it until T2 lets go
		- Spanner makes it so that reads do not require grabbing locks, huge performance gains! Previously, reads returning lots of data such as full database scans would require grabbing tons of locks, basically freezes writing to the database
		- So, in order to discuss how you might optimize on two-phase locking, I should first probably talk about the actual issues with it. Um so first of all in two-phase locking like i said a transaction can either hold the lock on an object in shared or exclusive mode. So reads occur in shared mode and writes occur in exclusive mode but recall that reads and writes actually block one another. So if one transaction has the lock in exclusive mode meaning it wants to write to that row, another transaction can't read it until the first one is done writing. Similarly, if one transaction is actually reading a row, a second transaction can't write to that row until the first lets go. So what spanner does is it actually kind of adjusts the whole system so that making reads doesn't require grabbing a ton of locks. This is really huge for doing things like backups or analytical queries or anything that requires scanning over a ton of rows that's going to take a few minutes because if you think about it, scanning over a huge database table in like a minute long query means that you're going to be having shared locks on pretty much every single row and when that happens, you can't write to any row and so obviously that's going to create a huge bottleneck in any type of database system. So being able to actually eliminate the need for locks on reading can seriously increase performance. So how do they go about actually doing this?
- ![[Screenshot 2024-11-11 at 10.10.22 PM.png]]
	- Consistent Snapshots
		- Description
			- A consistent snapshot is consistent with causality: if a snapshot contains a write, all writes that the writer had read in order to make its own write must be present
			- Dependency chain
		- Well like i said, they mainly want to be doing this on consistent snapshots and so a consistent snapshot is consistent with causality which means that um basically if it writes in there all the proceeding rights that have kind of you know it relies on must be present so imagine we have this timeline of a dependency chain here and you know i write w6 but in order to write w6 i on my screen was seeing w1, w2, w3, w4 and w5. Those better be present in the consistent snapshot because this is kind of the use case that spanner is optimizing for. 
	- Consistent Snapshots
		- Description
			- Not consistent! w3 is missing!
		- Obviously if w3 was missing, that would not be consistent 
	- Consistent snapshots
		- Description
			- In order to track causality, Spanner uses actual timestamps! Since they believe they can rely on their server's clocks, any read only transaction will only read writes with a lower timestamp than it.
			- A read only transaction that starts at 2:34 and finishes at 2:36 will not read w2:35 because that has a greater timestamp!
		- Additionally, so this is kind of where spanner comes in, as opposed to using something like some sort of version vector algorithm or some sort of timestamp that is trying to keep track of causality, spanner actually uses timestamps because obviously any write that occurs physically later than another write, you know it's going to want all the previous writes in that snapshot which is great and that's kind of exactly what linearizability is. Since they believe they can rely on their server's clocks, any read-only transaction is just going to take all the writes that were labeled with a lower timestamp. So you can see we're performing some sort of multi-version concurrency control here by labeling every single write with a timestamp. So imagine i had a super long transaction that takes two minutes and I label that with 2:34 even if it finishes at 2:36 and the 2:35 write that you see at the end of the timeline there was in the database, it won't be included in my snapshot which is good. 
- ![[Screenshot 2024-11-11 at 10.27.40 PM.png]]
	- Why not Lamport Timestamps?
		- Description
			- Recall: Lamport Timestamps are a way of creating a total ordering of writes in a distributed system consistent with causality. But do they work here?
			- Lamport Timestamps will leave out many writes that are "concurrent", but still did not happen at the same time.
				- It may be the case that we have two writes, W1 and W2, where the W1 < W2 (because they are concurrent and use an arbitrary node ordering to decide order) and W2 happened an hour earlier than W1 - hence Lamport Timestamps are not linearizable
			- Additionally, end users cannot communicate with one another externally!
				- E.g. Jordan sees a cooking video on his friend's TikTok for you page, immediately goes to follow the account. There is no way for TikTok to know analytically that Jordan had first been influenced by the for you page
		- Okay, so we actually kind of have distributed timestamps and you might be thinking to yourself, well if you know timestamps from physical clocks aren't reliable, why aren't we just using Lamport timestamps? So if you don't remember what those are you haven't watched my video on Lamport timestamps, just know that Lamport timestamps are a way of creating a total order of all the operations that happen on a database by basically propagating them between clients and databases and that way you can kind of keep track of which operations are concurrent and which operations are causally dependent on one another. That all being said Lamport timestamps do have some pretty big issues here. I mean the first thing and kind of the biggest is that they're not linearizable. You can have a bunch of writes that are concurrent according to Lamport timestamps but still didn't happen at the same time. So this is kind of problematic when you're doing full table queries because i might have according to some arbitrary ordering of nodes in you know like the Lamport ("lan port"?) ordering, I might have two writes where the database thinks they're concurrent and so as a result you know let's say we have w1 and w2 and the database says okay w1 occurred before w2 but the reality of the situation is just because of that random node ordering and the fact that Lamport timestamps think they're concurrent, uh it says w1 came first but the truth is w2 could have been written an hour earlier and we might have wanted that in our snapshot. So they're not linearizable in the sense that they're not consistent with physical time. Additionally, if there is any communication between you know end users on their own outside of the actual you know write and read path of the database itself, Lamport timestamps can't reflect that. For example, you know if I see a cooking video or more likely I don't know some baddie on TikTok on my friend's phone that i then say oh i'm going to go follow her. What TikTok now can't do is kind of see the causality or the decision making that led me to make that follow action. It can't see that i first saw this um interaction on my friend's device because my personal user generated Lamport timestamp doesn't reflect you know what i've been doing outside of reads and writes directly to the database. Um time can actually fix both of these issues.
	- Why not Centralized Timestamps?
		- Description
			- Could use a single server or cluster of servers to give each write to the system an ordering.
			- However, this is a huge bottleneck, as the ordering server could fail, and adds a ton of network latency for users not located geographically close to the servers
		- Another thing is why not just use a centralized timestamp. Every single write could just hit a server first. Um well obviously any time you say the word centralized, two things should be coming up in your head. The first is that, that's a single point of failure um assuming we're not using any sort of redundancy or like consensus algorithm to determine the timestamp, that's obviously going to be very problematic and then the second is that this is just going to be a huge bottleneck. Imagine if i'm in Australia and um you know i want to make a write to a spanner instance in Australia but the the whole ordering server is in America. That's going to hugely slow down my write.
	- TrueTime
		- Description
			- Does not return a single timestamp, but instead an uncertainty interval `[t_min, t_max]`, in which it takes into account multiple sources of error (such as network round trip time, quartz drift, GPS receiver accuracy. This range ensures a very high probability that the actual time is within it)
			- One write is considered to precede another if its maximum time is lower than the minimum time of the other.
		- Okay so what is [[TrueTime]] because i keep kind of floating this phrase around but now i'll actually discuss it. So TrueTime is Google's implementation of a timestamp where you basically are returning an uncertainty interval. So instead of saying like okay i think this event happened at x time, what Google instead does is perform a series of calculations based on a bunch of possible ranges of error and return a minimum and a maximum interval for that timestamp where they're basically just saying like okay we're pretty damn close to sure that whenever this event did happen, it happened within this bound. 
	- TrueTime
		- Description
			- Recall: Clocks in distributed system are not perfectly in sync, even if they are frequently updated using NTP!
		- Um okay, so in terms of um you know how clocks actually work in most distributed systems, recall that they're really not too in sync.
	- Spanner Writes
		- Description
			- Every single write to spanner has to wait a time period equal to the size of its uncertainty interval
				- So imagine we have W1 occurs at `[8, 10]` - W1 must wait two seconds before committing
				- This ensures that no matter where W1 actually was in the interval, it commits after time 10
				- If any transaction R1 reads W1, this must happen after time 10
				- So R1 must have interval value `[x, y such that y is greater than 10]`
				- Using the latest timestamps, R1 comes after W1
			- If we have to wait the uncertainty interval every write, we want to minimize the uncertainty interval
				- Google does this by putting atomic clocks in every data center and synchronizing with them frequently!
		- But in terms of spanner writes, this is kind of how we go ahead and make sure that TrueTime is actually working as it should be. So every single write to spanner has to wait a time period equal to the length of the uncertainty bound. So basically if I have $t_{min}$ and $t_{max}$, I subtract $t_{min}$ from $t_{max}$ (that's the length of the uncertainty interval) and every single time a write reaches the database, I have to actually go ahead and wait that length before it gets committed. Why do we do that? Well i'll explain it now. So imagine we have write one that occurs at \[8,10\], it means that we don't know exactly when um the write gets there but we know it's between time stamp eight and time stamp ten. So what spanner will actually do is it's gonna say okay the write is going to go ahead and wait two seconds before committing. The reason for this is it means that no matter when the write actually originally reached the server, we know it was between time 8 and time 10. So, if we wait two seconds, it means that the write is going to be committed after time 10 which means that any subsequent reads must also have um you know like after time 10 in its interval. So say r1 comes along and reads w1, we know that r1 is going to be after time 10, right? Because we know that w1 occurred after time 10 and so even if you know the time minimum of r1 is before 10, we know that the maximum has to be greater than 10 because we know that you know the read is going to be at some period where the bound must include times greater than 10. So now if we just use the later timestamp for both the write and the read, we see that r1 comes after w1. So that's a pretty clever trick that they use. However, it comes with the following problem. Every single time we're waiting the uncertainty interval on writes, that's greatly increasing the latency on writes. So kind of the challenge of spanner is to make sure that every single time you're making a write, we're minimizing the uncertainty interval as low as possible. How do we do this? Well, Google actually uses specialized hardware here. They put atomic clocks in every single data center and synchronize them very frequently. That's like every 30 seconds. So compared to traditional clocks which are synchronizing using network time protocol over the internet and possibly even synchronizing with some potentially unreliable servers, Google is using this kind of specialized design of the hardware in order to make sure that those clocks are as synchronized as humanly possible. In this way, they're able to kind of achieve this super high throughput and non-locking of reads. 
- ![[Screenshot 2024-11-11 at 10.37.09 PM.png]]
	- Conclusion
		- Description
			- Spanner is an extremely interesting example of using non-commodity hardware in order to achieve very high throughput using a relational data model and achieving linearizability. Although it probably doesn't make sense for most businesses, simply due to the lack of ability to run it on generic hardware, it is an interesting study on the lack of synchronized clocks in distributed systems and what could be accomplished if we could synchronize them
		- Okay, so in conclusion, although it's probably not that practical that you might talk about one of these in an actual systems design interview, if the the concept of you know distributed clocks comes up, it's very interesting to mention how you can use specialized hardware to kind of get a lower uncertainty bound on reads and writes and as a result of that, you can do some really interesting things. Even creating you know like a super scalable SQL system. I think this is a pretty interesting topic and it seems like most of the distributed systems courses tend to mention it because it's actually, i don't know pretty revolutionary in the sense that people haven't really just you know tried to optimize on time like this before