---
Source:
  - https://www.youtube.com/watch?v=I1wQsY-Nh_k
Reviewed: false
---
- ![[Screenshot 2024-11-19 at 10.40.45 PM.png]]
	- Introduction
		- hello everybody and welcome back. I am your host of the series. Google software engineer Jordan. Uh, I am actually known as the best backend developer in the world due to the sheer size and gravitational force of me and of course I'm well-versed in git, all commands with the exception of git pulling. If you were here for the last video, you know that we started to go into what database indexes are and in today's video, I'll be covering one specific type known as [[Hash index|Hash indexes]]. So without wasting any more of your time, let me go ahead and grab the iPad and we can start scribbling 
	- First
		- All right, welcome back everybody. Let's go ahead and get started. So to review from last video, we talked about indexes in general and why we might need them. So in that case, there's going to be two reasons. Here's number one. The first one is O(n) reads so O(n) reads basically means that uh effectively, it ties into reason number two, we have to go through all rows to find the rows that we want which means if we have some sort of table and here are all my rows in it, every single time if I want to find all rows for example where the name is Jordan or the ID is 10 or anything like that, I have to go through every single one and that is O(n) time complexity and so that is kind of the backdrop of this entire lesson made pretty simple. 
	- Second
		- So let's go over a different type of index effectively that is going to act as a precursor for the other indexes that we're going to go over and we're going to see the pros and cons of this one. So today's video is about hash indexes. Makes me think of hash browns because I'm fat but let's go into this further. Okay, so let's review a hash map because of course hash indexes are actually going and based off a hashmap. So the first thing to talk about when we talk about a hash map is a hash function so a hash function is effectively this. It is a black box, right? We have no clue what's going on in there but all we know is that out of all uh the elements that we could pass into it, you know, let's say I'm passing in Jordan, what it's going to spit out is a number. Obviously hash functions can spit out anything but we know that whenever we pass in Jordan, it's always going to spit out some number. Let's say that number is four, and so this hash function, it's always going to spit out the same number from this domain to the range where let's say the range that it could possibly spit out is 0 to 10 and so Jordan is always going to return a 4. Uh you know let's say we put in apple, that's going to return uh who knows a six or something like that and of course it is possible that sometimes I'll do another word and it's also going to return a 4. Right let's uh we'll do Shaq just to keep things consistent with last video, you know we get unlucky that also happens to return a four. 
	- Third
		- So basically, the premise of a hash map is that we have our hash function H and when we take in the the key and the value that we want, we basically take h of key and that's going to return some value V. We put it at basically the index V of the array. So in the previous example, let's imagine we have this array with uh indexes 0 through 9, right? It's size 10 and our hash function can return anywhere from 0 to 10. In the event of the hash map basically what I'd be doing is I'd be putting Jordan at index 4 right and so this allows us to have O(1) uh or constant time reads and writes. The reason for this being that our hash function is going to take constant time and basically then we can jump over to this place in memory which is really really great. So we jump right over here and that is O(1) to be able to access our element Jordan, check if it's in the hash map, etc etc etc. This is more of a hash set just since I haven't um you know added like a corresponding value to Jordan but don't worry about that for now, who cares. Um and then you know the other thing to consider is that sometimes like I said Shaq for example would also uh be hashed to the same place. So what do we do if two things are hashed to the same box, right? You know now we have Jordan and Shaq. Well there's a couple ways of actually uh dealing with this, one is where instead of just putting the key here, you would actually have a linked list, right. So I would have a node that contains Jordan and then a node containing Shaq that's called [[chaining]] and then the other alternative is probing, right? Where I would just look for the next available spot in the hash map, so for example, that would be this one here and then Shaq would be placed in this box at the array. Uh yes obviously that does ruin the kind of constant time complexity of it but uh if you do some math using amortized time complexity and uh something called a load factor, you can basically uh prove that it is still on average constant time and uh you know I'm not going to get into that for this video, I have a dedicated video to this, but you know that's generally how something like a hashmap would work.
	- Fourth
		- So let's continue to scroll down. So we have our hash map, right? and the general kind of gist of a hash index is just literally use a hashmap, right? So we know that the point of an index is we want to easily be able to scan for a certain key so in the case of you know uh Jordan for instance, I would basically put in the hashmap, Jordan, as the key and then the value corresponding to it would be the place on disk, who knows what address it would be or possibly even the actual row itself um but then that's what's in the hashmap, you know. You could do the same thing with Shaq and you know to keep the example uh from last time, you could do the same thing with Donald, right? Should all make sense for uh the most part. Whatever, let's assume those are addresses on disk, who knows, they're probably not. Uh but yes that's the point. We have our hashmap that maps keys and values of basically the index key that we care about and then the actual location on disk of the row and so this is going to be O(1) to actually find a row when we're searching by uh the name and that's in both reads and writes, right? Because we can just go right to the row and update it too which is perfect. However there are some pretty big issues with the hashmap. 
- ![[Screenshot 2024-11-19 at 11.43.49 PM.png]]
	- First
		- So the first one is that hashmaps are bad on disk. So the reason that they're bad on disk is, this is now going to be the third episode out of three that I've drawn this, we have our wheel and we have our pointer. In a hashmap, the values that you care about are stored all over that array, right? I might have one here, I might have one here, might have one in the middle because the entire point of the hashing function is to distribute elements evenly and so what that means is that on disk, we also have elements distributed equally because disk is effectively just that big array, right? You know 1,2,3 etc, etc. So the performance is actually pretty poor on hashmaps because we're jumping around all over this metallic uh disk and as a result, it just doesn't work very well. So what that means, is that hash indexes are always kept in memory but this has a couple of other issues. 
	- Second
		- One is that, RAM is expensive so there's less of it and because there's less it means that we actually have less leeway in terms of putting data in our database because all of our keys at the end of the day have to fit in Ram. So that's basically one issue, right? That's kind of the fundamental trade-off that we're making here. Obviously hash indexes are going to be super fast, right? All the operations are O(1), uh and it's also in memory which is even faster than disk. Random access memory means you can just jump around super quickly, that's the entire point. You're doing random accesses all over this array, so that's great. But there are still more downsides to having everything in memory because if we recall, RAM is not durable (the following section assumes that the index holds the actual rows as opposed to just their location on disk, otherwise just the index would be lost) and for our database, it would be pretty terrible if we could just lose data all the time, that wouldn't work very well for us at all, would it? So basically now what we need to do is figure out a way of actually keeping our data because if our computer were to shut down, our entire contents of our database would be lost and the way that we're going to do this is with this, a write ahead log. So I haven't really gone into this topic yet but it is pretty similar to something that I showed last episode where imagine a write ahead log as literally just a list every single time of all the writes and updates that you're making to the database, so for example, one thing could be you know change size of row you know with Jordan to 10 (If the index is just holding the locations of rows on disk, the write ahead log only needs to contain when we add new rows (e.g. row with name Jordan at 0x001)). Something like that, and so we write down all the changes that we're making in the database and the write ahead log is actually stored on disk. It's completely writing sequentially on disk right, so this is literally right next to this line which is next to this line and so since those lines are next to each other, these writes are relatively fast. We're still doing sequential writes which are not bad on a hard drive. So because we're using this write ahead log, now we have durability and so if the computer ever dies and we needed to actually recreate our hash index, we could do so by replaying all those changes in the write ahead log. First we'd replay this one, then we would replay this one, then we would replay this one, and so that way we could repopulate the hash index and so that is how our write ahead log provides durability. This is not something that's only used in the hash index, it's actually used in multiple different types of indexes but we will cover that in future videos. So that's how we would repopulate the hash index. So even though uh the hash index is itself kept in memory, having the write ahead log is going to slow things down because disk writes are definitely going to be slower than memory writes even though it is an O(1) time complexity write (To be extra clear here: first you write to the write ahead log, then you change the hash index) because we're writing sequentially. 
	- Third
		- Okay, so what's our last problem? We have this hash index, we know we have to write to disk first if we want durability. If we don't care, sure it's effectively just a hashmap in memory. Our last problem is that we can't do range queries. Why can't we do range queries? We'll think about how hashmaps work. Last time I introduced the problem of a range query as let's say we want all rows with names between A and B. Uh with names between A and B, so what the hashmap is really good for is we can easily check basically you know Alex, Abe, and Allan. We can easily check if the key Alex is in the hashmap, right? That's O(1), but for us to actually check all the rows with names between A and B, we would have to literally loop through every single possible name between A and B and check if that's in the hashmap which is not feasible. There are infinite names between A and B, you know, if we're not just like you know sticking to actual names that exist on the earth, so we would have to check literally every possible string and again there are infinite of those. So that doesn't work very well. That's why this isn't really feasible for range queries because the only way to do a range query with a hashmap is to either iterate through all the indexes which again is O(n) or to literally check all of the possibilities uh between A and B which in some some cases is endless, right? So O(n) is probably what we would stick with, we would just go through all the indexes in the array and check if it fits our predicate, um but again this is useless to us because we've already said O(n) reads are not good enough for us. So we're going to need a better solution in the future 
	- Fourth
		- Um, some of you may be thinking, use something like uh a tree set, right? Uh you remember binary search trees? They are very nice because uh you know you're able to actually perform range queries really easily because the kind of invariance of the binary search tree means that let's say, you know we want everything between uh whatever this node and this node is, you know I'm not explaining this well but whatever (Sorry, had to think about this more later to do it as efficiently as possible - the challenge is only fetching the rows that we need (database can be huge): Basically figure out the paths for the bounds of the range query. Then run a `dfs/bfs` from the root making sure to stay within those bounds). Point is, binary search trees work much better in terms of doing range queries. You can actually do them, however, unlike with a hash index, uh reads and writes are going to be O(log(n)) which is worse than our hash Index, right? Where everything was O(1). 
- Conclusion
	- So basically, in conclusion, hash indexes are really great if you want to do O(1) reads and O(1) writes, but you don't need range queries and additionally it comes with the even further constraint that you basically have to keep all of your keys in memory. So if you do have a sufficiently small data set and it's something that you're willing to tolerate where you're not looking for range queries but you're mostly just looking to get one row at a time, then the hash index could be for you. So keep this in mind guys um you know as we move on through the indexes, um, they're obviously going to be trade-offs between all three but these are kind of the main pros and cons of the hash index
		- Pros:
			- O(1) Reads and Writes
		- Cons:
			- Keys must fit in memory
			- No range queries