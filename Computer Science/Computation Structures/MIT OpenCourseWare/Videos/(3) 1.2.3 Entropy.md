---
Source:
  - https://www.youtube.com/watch?v=7ufoYYj15cU
Reviewed: false
---
- ![[Screenshot 2025-05-01 at 1.44.21 PM.png]]
	- Entropy
		- In the next section we’re going to start our discussion on how to actually engineer the bit encodings we’ll use to encode information, but first we’ll need a way to evaluate the efficacy of an encoding. The entropy, $H(X)$, of a discrete random variable $X$ is average amount of information received when learning the value of $X$:$$H(X) = E(I(X)) = \sum_ip_ilog_2\frac{1}{p_i}$$
		- Shannon followed Boltzmann’s lead in using $H$, the upper-case variant of the Greek letter $\eta$ (eta), for “entropy” since $E$ was already used for “expected value,” the mathematicians’ name for “average.” We compute the expected value in the usual way: we take the weighted sum, where the amount of information received when learning of a particular choice $i,log_2(1/p_i)$ is weighted by the probability of that choice actually happening.
		- Here’s an example. We have a random variable that can take on one of four values $\{A,B,C,D\}$. The probabilities of each choice are shown in the table, along with the associated information content.
		- Now we’ll compute the entropy using Equation (4):$$\begin{align}H(X) &= (1/3)(1.58) + (1/2)(1)+(1/12)(3.58) + (1/12)(3.58)\\&=1.626 \text{bits}\end{align}$$
		- This is telling us that a clever encoding scheme should, on the average, be able to do better than simply encoding each symbol using 2 bits to represent which of the four possible values is next. Food for thought! We’ll discuss this further in our discussion of variable-length encodings.
		- Description
			- In information theory, the entropy H(X) is the average amount of information contained in each piece of data received about the value of X
	- Meaning of Entropy
		- So, what is the entropy telling us? Suppose we have a sequence of data describing a sequence of values of the random variable $X$.
		- If, on the average, we use less than $H(X)$ bits transmit each piece of data in the sequence, we will not be sending enough information to resolve the uncertainty about the values. In other words, the entropy is a lower bound on the number of bits we need to transmit. Getting less than this number of bits wouldn’t be good if the goal was to unambiguously describe the sequence of values — we’d have failed at our job!
		- On the other hand, if we send, on the average, more than $H(X)$ bits to describe the sequence of values, we will not be making the most effective use of our resources, since the same information might have been able to be represented with fewer bits. This okay, but perhaps with some insights we could do better.
		- Finally, if we send on the average exactly $H(X)$ bits then we’d have the perfect encoding. Alas, perfection is, as always, a tough goal, so most of the time we’ll have to settle for getting close.
		- Description
			- Suppose we have a data sequence describing the values of the random variable X.
			- Average number of bits used to transmit choice
