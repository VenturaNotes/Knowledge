---
Source:
  - https://www.youtube.com/watch?v=qyBuzeUYs2M
Reviewed: false
---
- ![[Screenshot 2025-05-01 at 1.39.13 PM.png]]
	- Quantifying Information ([[Claude Shannon]], 1948)
		- Mathematicians like to model uncertainty about a particular circumstance by introducing the concept of a random variable. For our application, we’ll always be dealing with circumstances where there are a finite number N of distinct choices, so we’ll be using a discrete random variable X that can take on one of the N possible values from the set $\{x_1, x_2, \ldots, x_N\}$. The probability that X will take on the value $x_1$ is given by the probability $p_1$, the value $x_2$ by probability $p_2$, and so on. The smaller the probability, the more uncertain it is that X will take on that particular value.
		- Claude Shannon, in his seminal work on the theory of information, defined the information received when learning that X had taken on the value $x_i$ as $$I(x_i) = log_2 \frac{1}{p_i} \text{ bits}$$
		- Note that the uncertainty of a choice is inversely proportional its probability, so the term inside of the log is basically the uncertainty of that particular choice. We use the $log_2$ to measure the magnitude of the uncertainty in bits where a bit is a quantity that can take on the value 0 or 1. Think of the information content as the number of bits we would require to encode this choice.
		- Description
			- Given discrete random variable X
				- N possible values: $x_1, x_2, \ldots, x_N$
				- Associated probabilities: $p_1, p_2, \ldots, p_N$
			- Information received when learning that choice was $x_i$:
				- $I(x_i) = log_2(\frac{1}{p_i})$
					- $1/p_i$ is proportional to the uncertainty of choice $x_i$
					- Information is measured in bits (binary digits) = number of 0/1's required to encode choice(s)
	- Information Conveyed by Data
		- Suppose the data we receive doesn’t resolve all the uncertainty. For example, when earlier we received the data that the card was a Heart: some of uncertainty has been resolved since we know more about the card than we did before the receiving the data, but we don’t yet know the exact card, so some uncertainty still remains. We can slightly modify Equation (1) as follows $$I(\text{data}) = log_2\frac{1}{p_{\text{data}}}\text{ bits}$$
		- In our example, the probability of learning that a card chosen randomly from a 52-card deck is a Heart is 13/52 = 0.25, the number of Hearts over the total number of choices. So the information content is computed as $$I(\text{heart}) = log_2\frac{1}{p_{\text{heart}}}=log_2\frac{1}{0.25}=2\text{ bits}$$
		- This example is one we encounter often: we receive partial information about N equally-probable choices (each choice has probability 1/N) that narrows the number of choices down to M. The probability of receiving such information is M(1/N), so the information content is $$I(N \text{ choices} \to M \text{ choices}) = log_2 \frac{1}{M(1/N)}=log_2 \frac NM \text{ bits}$$
		- Description
			- Even when data doesn't resolve all the uncertainty
				- $I(\text{data}) = log_2\frac{1}{P_{\text{data}}}$ 
				- e.g., $I(heart) = log_2 \frac{1}{\frac{13}{52}} = 2 \text{ bits}$ 
			- Common case: Suppose you're faced with N equally probably choices, and you receive data that narrows it down to M choices. The probability that data would be sent is $M*(1/N)$ so the amount of information you have received is
				- $I(\text{data}) = log_2(\frac{1}{M*(1/N)})=log_2(\frac{N}{M}) \text{ bits}$ 
	- Example Information Content
		- Let’s look at some examples.
			- If we learn the result (heads or tails) of a flip of a fair coin, we go from 2 choices to a single choice. So, using our equation, the information received is $log_2(2/1) = 1$ bit. This makes sense: it would take us one bit to encode which of the two possibilities actually happened, say, “1” for heads and “0” for tails.
			- Reviewing the example from earlier, learning that a card drawn from a fresh deck is a Heart gives us $log_2(52/13) = 2$ bits of information. Again this makes sense: it would take us two bits to encode which of the four possible card suits had turned up.
			- - Finally consider what information we get from rolling two dice, one red and one green. Each die has six faces, so there are 36 possible combinations. Once we learn the exact outcome of the roll, we’ve received $log_2(36/1) = 5.17$ bits of information.
		- Hmm. What do those fractional bits mean? Our digital system only deals in whole bits! So to encode a single outcome, we’d need to use 6 bits. But suppose we wanted to record the outcome of 10 successive rolls. At 6 bits/roll we would need a total of 60 bits. What this formula is telling us is that we would need not 60 bits, but only 52 bits to unambiguously encode the results. Whether we can come up with an encoding that achieves this lower bound is an interesting question that we’ll take up later in this chapter.
		- Description
			- Examples:
				- Information in one coin flip:
					- N = 2
					- M = 1
					- Info content = $log_2(2/1) = 1 \text { bit}$
				- Card drawn from fresh deck is a heart:
					- N = 52
					- M = 13
					- Info content = $log_2(52/13) = 2 \text{ bits}$
				- Roll of 2 dice:
					- N = 36
					- M = 1
					- Info content = $log_2(36/1) = 5.17$
						- .17 bits???
	- Probability & Information Content
		- To wrap up, let’s return to our initial example. Here’s a table showing the different choices for the data received, along with the probability of that event and the computed information content.
		- The results line up nicely with our intuition: the more uncertainty is resolved by the data, the more information we have received. We can use Equation (2) to provide an exact answer to the questions at the end of the first slide. We get the most information when we learn that the card is the suicide King and the least information when we learn that the card is not the Ace of Spades.
		- Description
			- Information content
			- Shannon's definition for information content lines up nicely with my intuition: I get more information when the data resolves more uncertainty about the randomly selected card.