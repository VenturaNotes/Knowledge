---
Source:
  - zotero://open-pdf/library/items/FCTDHIDM?page=1&annotation=T7PMADWW
Length: "479"
Progress: "12"
tags:
  - status/incomplete
  - type/textbook
  - temp
---
## (1) Introduction
- “<mark style="background: #FFF3A3A6;">Computer science</mark> as an academic discipline <mark style="background: #FFF3A3A6;">began in the 1960’s</mark>.” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=GUACMP9Y))
- “Emphasis was on programming languages, [[Compilers|compilers]], [[Operating Systems|operating systems]], and the [[Mathematical Theory|mathematical theory]] that supported these areas.” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=RWFSNNBI))
- “Courses in theoretical computer science covered [[- Topics/Finite Automata|finite automata]], [[Regular Expressions|regular expressions]], [[Context-Free Languages|context-free language]], and [[Computability|computability]].” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=KXL4UPAZ))
- “[[Algorithms|algorithms]]” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=MI54IXP2))
- “One of the major changes is an increase in emphasis on [[Probability|probability]], [[Statistics|statistics]], and [[Numerical Methods|numerical methods]].” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=GYMTIZZD))
- “Modern data in diverse fields such as [[Information Processing|information processing]], [[Search|search]], and [[Machine Learning|machine learning]] is often advantageously represented as [[Vector|vectors]] with a large number of components.” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=JVIDNAL5))
- “Indeed, the two salient aspects of [[Vector|vectors]]: [[Geometric Vectors|geometric]] (length, dot products, orthogonality etc.) and [[Linear Algebraic Vectors|linear algebraic]] (independence, rank, singular values etc.) turn out to be relevant and useful.” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=LLLLDJYG)) ^0t90io
- “Chapter 3 focuses on [[Singular Value Decomposition (SVD)|singular value decomposition]] (SVD) a central tool to deal with [[Matrix|matrix]] data.” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=SMZ9MB87))
- “We give a from-first-principles description of the mathematics and algorithms for [[Singular Value Decomposition (SVD)|SVD]].” ([Blum et al., p. 9](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=7BJM9KLC))
- “Applications of singular value decomposition include [[Principal Component Analysis|principal component analysis]], a widely used technique which we touch upon, as well as modern applications to [[Mixture (probability)|statistical mixtures]] of [[Probability Densities|probability densities]], [[Discrete Optimization|discrete optimization]], etc., which are described in more detail.” ([Blum et al., p. 9-10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=9&annotation=LM8JNE6B))
- “Exploring large structures like the web or the space of configurations of a large system with [[Deterministic Methods|deterministic methods]] can be prohibitively expensive.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=URZGDMNI))
- “[[Random Walks|Random walks]] (also called Markov Chains) turn out often to be more efficient as well as illuminative.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=2UNUDJ7N))
- “The [[Stationary Distributions|stationary distributions]] of such walks are important for applications ranging from web search to the simulation of physical systems.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=BFZPQ4KG))
- “The underlying mathematical theory of such random walks, as well as connections to [[Electrical Networks|electrical networks]], forms the core of Chapter 4 on [[Random Walks|Markov chains]].” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=RQ26RQ7T))
- “One of the surprises of computer science over the last two decades is that some [[Domain-Independent Methods|domain-independent methods]] have been immensely successful in tackling problems from diverse areas. [[Machine Learning]] is a striking example.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=LH9XKAQS))
- “Chapter 5 describes the foundations of [[Machine Learning]], both algorithms for optimizing over given training examples, as well as the theory for understanding when such optimization can be expected to lead to good performance on new, unseen data.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=SELT4JCJ))
- “This includes important measures such as the [[Vapnik-Chervonenkis Dimension|Vapnik-Chervonenkis dimension]], important algorithms such as the[[ Perceptron Algorithm]], [[Stochastic Gradient Descent|stochastic gradient descent]], [[Boosting|boosting]], and [[Deep Learning|deep learning]], and important notions such as [[Regularization|regularization]] and [[Overfitting|overfitting]].” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=X7SVRQWE))
- “The field of algorithms has traditionally assumed that the input data to a problem is presented in [[Random Access Memory]], which the algorithm can repeatedly access. This is not feasible for problems involving enormous amounts of data. The [[Streaming Model]] and other models have been formulated to reflect this.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=3ZK4NMG9))
- “In this setting, [[Sampling|sampling]] plays a crucial role and, indeed, we have to sample on the fly. In Chapter 6 we study how to draw good samples efficiently and how to estimate statistical and linear algebra quantities, with such samples.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=XPE3ZKDN))
- “While Chapter 5 focuses on [[Supervised Learning|supervised learning]], where one learns from labeled training data, the problem of [[Unsupervised Learning|unsupervised learning]], or learning from unlabeled data, is equally important.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=CW9AWJGA))
- “A central topic in [[Unsupervised Learning]] is [[Clustering|clustering]], discussed in Chapter 7.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=BEDUT3PS))
- “[[Clustering]] refers to the problem of partitioning data into groups of similar objects.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=65ZMAB8N))
- “After describing some of the basic methods for [[Clustering]], such as the [[K-Means Algorithm|k-means algorithm]], Chapter 7 focuses on modern developments in understanding these, as well as newer algorithms and general frameworks for analyzing different kinds of clustering problems.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=ENZMTZ2C))
- “The simplest model is that of a [[Random Graph|random graph]] formulated by [[Erdos and Renyi Model|Erdös and Renyi]], which we study in detail in Chapter 8, proving that certain global phenomena, like a giant connected component, arise in such structures with only local choices.” ([Blum et al., p. 10](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=10&annotation=X7CEU82Y))
- “Chapter 9 focuses on [[Linear Algebra|linear-algebraic]] problems of making sense from data, in particular [[Topic Modeling|topic modeling]] and [[Non-Negative Matrix Factorization|non-negative matrix factorization]].” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=MAIBZBJF))
- “In addition to discussing well-known models, we also describe some current research on models and algorithms with provable guarantees on learning error and time.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=7W4UMNUH))
- “This is followed by [[Graphical Models|graphical models]] and [[Belief Propagation|belief propagation]].” (Blum et al., p. 11)
- “Chapter 10 discusses [[Ranking|ranking]] and [[Social Choice|social choice]] as well as problems of [[Sparse Representations|sparse representations]] such as [[Compressed Sensing|compressed sensing]].” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=6WDKN722))
- “Additionally, Chapter 10 includes a brief discussion of [[Linear Programming|linear programming]] and [[Semidefinite Programming|semidefinite programming]].” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=GN6BRMTI))
- “[[Wavelets|Wavelets]], which are an important method for representing signals across a wide range of applications, are discussed in Chapter 11 along with some of their fundamental mathematical properties. The appendix includes a range of background material.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=7PBWNACN))
- “lower case letters for [[Scalar Variables|scalar variables]] and [[Functions|functions]]” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=SZXYITMT))
- “bold face lower case for [[Vector|vectors]]” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=K8F7DM6G))
- “upper case letters for [[Matrix|matrices]]” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=9BKJ83AQ))
- “Lower case near the beginning of the alphabet tend to be [[Constants|constants]]” (Blum et al., p. 11)
- “in the middle of the alphabet, such as i, j, and k, are indices in [[Summations|summations]]” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=EC65GMLQ))
- “n and m for [[Integer Size|integer sizes]]” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=B5VGNTX3))
- “x, y and z for [[Variable|variables]].” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=ITJ3N7SX))
- “If A is a [[Matrix]] its elements are $a_{ij}$ and its rows are $a_i$.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=WAV5T6PQ))
- “If $a_i$ is a [[Vector|vector]] its coordinates are $a_{ij}$.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=Q44INVCQ))
- “If we have a set of points in some [[Vector Space|vector space]], and work with a [[Subspace|subspace]], we use n for the number of points, d for the dimension of the space, and k for the dimension of the subspace.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=UTR32C49))
- “The term “almost surely” means with [[Probability]] tending to one.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=GYUDHMVW))
- “We use $ln\space n$ for the natural [[Logarithm|logarithm]] and log n for the base two logarithm. If we want base ten, we will use log10 .” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=YKGM9H6K))
- “To simplify notation and to make it easier to read we use E2(1 − x) for (E(1 − x))2 and E(1 − x)2 for E ((1 − x)2) .” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=L5VBKSQR))
- “When we say “randomly select” some number of points from a given [[Probability Distribution|probability distribution]], independence is always assumed unless otherwise stated.” ([Blum et al., p. 11](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=11&annotation=LZNWAGG3))

## (2) High-Dimensional Space
“Generate n points at random in d-dimensions where each coordinate is a zero mean, unit variance Gaussian.” ([Blum et al., p. 12](zotero://select/library/items/UFNIKXB5)) ([pdf](zotero://open-pdf/library/items/FCTDHIDM?page=12&annotation=4HXXSGK5))
- n is used for the number of points. 
- each coordinate is a zero mean, unit variance gaussian? What does that mean
- Ok, so we are generating n points at random in d-dimensions where 
- each coordinate is a zero mean, unit variance Gaussian
	- Does it mean that each coordinate when made together is zero mean?
What is the best way to search?
- What is a zero mean coordinate?
- Break down the sentence
	- Generate n points at random 
	- in d-dimensions 
	- where each coordinate is a zero mean, unit variance Gaussian
		- I need to better understand what Gaussian means
[[(Video) What is a Gaussian Distribution.]]

### (2.1) Introduction
### (2.2) The Law of Large Numbers
### (2.3) The Geometry of High Dimensions
### (2.4) Properties of the Unit Ball
#### (2.4.1) Volume of the Unit Ball
#### (2.4.2) Volume Near the Equator
### (2.5) Generating Points Uniformly at Random from a Ball
### (2.6) Gaussians in High Dimension
### (2.7) Random Projection and Johnson-Lindenstrauss Lemma
### (2.8) Separating Gaussian to Data
### (2.9) Fitting a Spherical Gaussian to Data
### (2.10) Bibliographic Notes
### (2.11) Exercises
## (3) Best-Fit Subspaces and Singular Value Decomposition (SVD)
### (3.1) Introduction
### (3.2) Preliminaries
### (3.3) Singular Vectors
### (3.4) Singular Value Decomposition (SVD)
### (3.5) Best Rank-k Approximations
### (3.6) Left Singular Vectors
### (3.7) Power Method for Singular Value Decomposition
#### (3.7.1) A Faster Method
### (3.8) Singular Vectors and Eigenvectors
### (3.9) Applications of Singular Value Decomposition
#### (3.9.1) Centering Data
#### (3.9.2) Principal Component Analysis
#### (3.9.3) Clustering a Mixture of Spherical Gaussians
#### (3.9.4) Ranking Documents and Web Pages
#### (3.9.5) An Application of SVD to a Discrete Optimization Problem
### (3.10) Bibliographic Notes
### (3.11) Exercises
## (4) Random Walks and Markov Chains
### (4.1) Stationary Distribution
### (4.2) Markov Chain Monte Carlo
#### (4.2.1) Metropolis-Hasting Algorithm
#### (4.2.2) Gibbs Sampling
### (4.3) Areas and Volumes
### (4.4) Convergence of Random Walks on Undirected Graphs
#### (4.4.1) Using Normalized Conductance to Prove Convergence
### (4.5) Electrical Networks and Random Walks
### (4.6) Random Walks on Undirected Graphs with Unit Edge Weights
### (4.7) Random Walks in Euclidean Space
### (4.8) The Web as a Markov Chain
### (4.9) Bibliographic Notes
### (4.10) Exercises
## (5) Machine Learning
### (5.1) Introduction
### (5.2) The Perceptron algorithm
### (5.3) Kernel Functions
### (5.4) Generalizing to New Data
### (5.5) Overfitting and Uniform Convergence
### (5.6) Illustrative Examples and Occam's Razor
#### (5.6.1) Learning Disjunctions
#### (5.6.2) Occam's Razor
#### (5.6.3) Application: Learning Decision Trees
### (5.7) Regularization: Penalizing Complexity
### (5.8) Online Learning
#### (5.8.1) An Example: Learning Disjunctions
#### (5.8.2) The Halving Algorithm
#### (5.8.3) The Perceptron Algorithm
#### (5.8.4) Extensions: Inseparable Data and Hinge Loss
### (5.9) Online to Batch Conversion
### (5.10) Support-Vector Machines
### (5.11) VC-Dimension
#### (5.11.1) Definitions and Key Theorems
#### (5.11.2) Examples: VC-Dimension and Growth Function
#### (5.11.3) Proof of Main Theorems
#### (5.11.4) VC-Dimension of Combinations of Concepts
#### (5.11.5) Other Measures of Complexity
### (5.12) Strong and Weak Learning - Boosting
### (5.13) Stochastic Gradient Descent
### (5.14) Combining (Sleeping) Expert Advice
### (5.15) Deep Learning
#### (5.15.1) Generative Adversarial Networks (GANs)
### (5.16) Further Current Directions
#### (5.16.1) Semi-Supervised Learning
#### (5.16.2) Active Learning
#### (5.16.3) Multi-Task Learning
### (5.17) Bibliographic Notes
### (5.18) Exercises
## (6) Algorithms for Massive Data Problems: Streaming, Sketching, and Sampling
### (6.1) Introduction
### (6.2) Frequency Moments of Data Streams
#### (6.2.1) Number of Distinct Elements in a Data Stream
#### (6.2.2) Number of Occurrences of a Given Element
#### (6.2.3) Frequent Elements
#### (6.2.4) The Second Moment
### (6.3) Matrix Algorithms using Sampling
#### (6.3.1) Matrix Multiplication using Sampling
#### (6.3.2) Implementing Length Squared Sampling in Two Passes
#### (6.3.3) Sketch of a Large Matrix
### (6.4) Sketches of Documents
### (6.5) Bibliographic Notes
### (6.6) Exercises
## (7) Clustering
### (7.1) Introduction
#### (7.1.1) Preliminaries
#### (7.1.2) Two General Assumptions on the Form of Clusters
#### (7.1.3) Spectral Clustering
### (7.2) k-Means Clustering
#### (7.2.1) A Maximum-Likelihood Motivation
#### (7.2.2) Structural Properties of the k-Means Objective
#### (7.2.3) Lloyd's Algorithm
#### (7.2.4) Ward's Algorithm
#### (7.2.5) k-Means Clustering on the Line
### (7.3) k-Center Clustering
### (7.4) Finding Low-Error Clusterings
### (7.5) Spectral Clustering
#### (7.5.1) Why Project?
#### (7.5.2) The Algorithm
#### (7.5.3) Means Separated by Omega(1) Standard Deviations
#### (7.5.4) Laplacians
#### (7.5.5) Local spectral clustering
### (7.6) Approximation Stability
#### (7.6.1) The Conceptual Idea
#### (7.6.2) Making this Formal
#### (7.6.3) Algorithm and Analysis
### (7.7) High-Density Clusters
#### (7.7.1) Single Linkage
#### (7.7.2) Robust Linkage
### (7.8) Kernel Methods
### (7.9) Recursive Clustering based on Sparse Cuts
### (7.10) Dense Submatrices and Communities
### (7.11) Community Finding and Graph Partitioning
### (7.12) Spectral clustering applied to social networks
### (7.13) Bibliographic Notes
### (7.14) Exercises
## (8) Random Graphs
### (8.1) The G(n, p) Model
#### (8.1.1) Degree Distribution
#### (8.1.2) Existence of Triangles in G(n, d/n)
### (8.2) Phase Transitions
### (8.3) Giant Component
#### (8.3.1) Existence of a giant component
#### (8.3.2) No other large components
#### (8.3.3) The case of p < 1/n
### (8.4) Cycles and Full Connectivity
#### (8.4.1) Emergence of Cycles
#### (8.4.2) Full Connectivity
#### (8.4.3) Threshold for O(ln n) Diameter
### (8.5) Phase Transitions for Increasing Properties
### (8.6) Branching Processes
### (8.7) CNF-SAT
#### (8.7.1) SAT-solvers in practice
#### (8.7.2) Phase Transitions for CNF-SAT
### (8.8) Nonuniform Models of Random Graphs
#### (8.8.1) Giant Component in Graphs with Given Degree Distribution
### (8.9) Growth Models
#### (8.9.1) Growth Model Without Preferential Attachment
#### (8.9.2) Growth Model with Preferential Attachment
### (8.10) Small World Graphs
### (8.11) Bibliographic Notes
### (8.12) Exercises
## (9) Topic Models, Nonnegative Matrix Factorization, Hidden Markov Models, and Graphical Models
### (9.1) Topic Models
### (9.2) An Idealized Model
### (9.3) Nonnegative Matrix Factorization - NMF
### (9.4) NMF with Anchor Terms
### (9.5) Hard and Soft Clustering
### (9.6) The Latent Dirichlet Allocation Model for Topic Modeling
### (9.7) The Dominant Admixture Model
### (9.8) Formal Assumptions
### (9.9) Finding the Term-Topic Matrix
### (9.10) Hidden Markov Models
### (9.11) Graphical Models and Belief Propagation
### (9.12) Bayesian or Belief Networks
### (9.13) Markov Random Fields
### (9.14) Factor Graphs
### (9.15) Tree Algorithms
### (9.16) Message Passing in General Graphs
### (9.17) Graphs with a Single Cycle
### (9.18) Belief Update in Networks with a Single Loop
### (9.19) Maximum Weight Matching
### (9.20) Warning Propagation
### (9.21) Correlation Between Variables
### (9.22) Bibliographic Notes
### (9.23) Exercises
## (10) Other Topics
### (10.1) Ranking and Social Choice
#### (10.1.1) Randomization
#### (10.1.2) Examples
### (10.2) Compressed Sensing and Sparse Vectors
#### (10.2.1) Unique Reconstruction of a Sparse Vector
#### (10.2.2) Efficiently Finding the Unique Sparse Solution
### (10.3) Applications
#### (10.3.1) Biological
#### (10.3.2) Low Rank Matrices
### (10.4) An Uncertainty Principle
#### (10.4.1) Sparse Vector in Some Coordinate Basis
#### (10.4.2) A Representation Cannot be Sparse in Both Time and Frequency Domains
### (10.5) Gradient
### (10.6) Linear Programming
#### (10.6.1) The Ellipsoid Algorithm
### (10.7) Integer Optimization
### (10.8) Semi-Definite Programming
### (10.9) Bibliographic Notes
### (10.10) Exercises
## (11) Wavelets
### (11.1) Dilation
### (11.2) The Haar Wavelet
### (11.3) Wavelet Systems
### (11.4) Solving the Dilation Equation
### (11.5) Conditions on the Dilation Equation
### (11.6) Derivation of the Wavelets from the Scaling Function
### (11.7) Sufficient Conditions for the Wavelets to be Orthogonal
### (11.8) Expressing a Function in Terms of Wavelets
### (11.9) Designing a Wavelet System
### (11.10) Applications
### (11.11) Bibliographic Notes
### (11.12) Exercises
## (12) Appendix
### (12.1) Definitions and Notation
### (12.2) Asymptotic Notation
### (12.3) Useful Relations
### (12.4) Useful Inequalities
### (12.5) Probability
#### (12.5.1) Sample Space, Events, and Independence
#### (12.5.2) Linearity of Expectation
#### (12.5.3) Union Bound
#### (12.5.4) Indicator Variables
#### (12.5.5) Variance
#### (12.5.6) Variance of the Sum of Independent Random Variables
#### (12.5.7) Median
#### (12.5.8) The Central Limit Theorem
#### (12.5.9) Probability distributions
#### (12.5.10) Bayes Rule and Estimators
### (12.6) Bounds on Tail Probability
#### (12.6.1) Chernoff Bounds
#### (12.6.2) More General Tail Bounds
### (12.7) Applications of the Tail Bound
### (12.8) Eigenvalues and Eigenvectors
#### (12.8.1) Symmetric Matrices
#### (12.8.2) Relationship between SVD and Eigen Decomposition
#### (12.8.3) Extremal Properties of Eigenvalues
#### (12.8.4) Eigenvalues of the Sum of Two Symmetric Matrices
#### (12.8.5) Norms
#### (12.8.6) Important Norms and Their Properties
#### (12.8.7) Additional Linear Algebra
#### (12.8.8) Distance between subspaces
#### (12.8.9) Positive semidefinite matrix
### (12.9) Generating Functions
#### (12.9.1) Generating Functions for Sequences Defined by Recurrence Relationships
#### (12.9.2) The Exponential Generating Function and the Moment Generating Function
### (12.10) Miscellaneous
#### (12.10.1) Lagrange multipliers
#### (12.10.2) Finite Fields
#### (12.10.3) Application of Mean Value Theorem
#### (12.10.4) Sperner's Lemma
#### (12.10.5) Prüfer
### (12.11) Exercises
