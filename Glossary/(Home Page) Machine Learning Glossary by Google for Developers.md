---
Source:
  - https://developers.google.com/machine-learning/glossary#L2_loss
Length: "1"
tags:
  - type/website
  - status/incomplete
Reviewed: false
---
-  Researched
- Questioned
- Defined
- Created
	- A
		- [[ablation]]
		- [[A-B testing]]
		- [[accelerator chip]]
		- [[accuracy]]
		- [[action]]
		- [[activation function]]
		- [[active learning]]
		- [[AdaGrad]]
		- [[agent]]
		- [[agglomerative clustering]]
		- [[anomaly detection]]
		- [[AR]]
		- [[PR AUC]]
		- [[AUC]]
		- [[artificial general intelligence]]
		- [[artificial intelligence]]
		- [[attention]]
		- [[attribute]]
		- [[attribute sampling]]
		- [[augmented reality]]
		- [[autoencoder]]
		- [[automation bias]]
		- [[AutoML]]
		- [[auto-regressive model]]
		- [[auxiliary loss]]
		- [[average precision]]
		- [[axis-aligned condition]]
	- B
		- [[backpropagation]]
		- [[bagging]]
		- [[bag of words]]
		- [[baseline]]
		- [[batch]]
		- [[batch inference]]
		- [[batch normalization]]
		- [[batch size]]
		- [[Bayesian neural network]]
		- [[Bayesian optimization]]
		- [[Bellman equation]]
		- [[Bidirectional Encoder Representations from Transformers]]
		- [[bias]]
		- [[bias term]]
		- [[bidirectional]]
		- [[bidirectional language model]]
		- [[bigram]]
		- [[binary classification]]
		- [[binary condition]]
		- [[binning]]
		- [[Bilingual Evaluation Understudy]]
		- [[Boosting]]
		- [[bounding box]]
		- [[broadcasting]]
		- [[bucketing]]
	- C
		- [[calibration layer]]
		- [[candidate generation]]
		- [[candidate sampling]]
		- [[categorical data]]
		- [[causal language model]]
		- [[centroid]]
		- [[centroid-based clustering]]
		- [[chain-of-thought prompting]]
		- [[checkpoint]]
		- [[class]]
		- [[classification model]]
		- [[classification threshold]]
		- [[class-imbalanced dataset]]
		- [[clipping]]
		- [[Cloud TPU]]
		- [[clustering]]
		- [[co-adaptation]]
		- [[collaborative filtering]]
		- [[condition]]
		- [[configuration]]
		- [[confirmation bias]]
		- [[confusion matrix]]
		- [[constituency parsing]]
		- [[continuous feature]]
		- [[convenience sampling]]
		- [[convergence]]
		- [[convex function]]
		- [[convex optimization]]
		- [[convex set]]
		- [[convolution]]
		- [[convolutional filter]]
		- [[convolutional layer]]
		- [[convolutional neural network]]
		- [[convolutional operation]]
		- [[cost]]
		- [[co-training]]
		- [[counterfactual fairness]]
		- [[coverage bias]]
		- [[crash blossom]]
		- [[critic]]
		- [[cross-entropy]]
		- [[cross-validation]]
	- D
		- [[data analysis]]
		- [[data augmentation]]
		- [[DataFrame]]
		- [[data parallelism]]
		- [[dataset]]
		- [[Dataset API]]
		- [[decision boundary]]
		- [[decision forest]]
		- [[decision threshold]]
		- [[decision tree]]
		- [[decoder]]
		- [[deep model]]
		- [[deep neural network]]
		- [[Deep Q-Network]]
		- [[demographic parity]]
		- [[denoising]]
		- [[dense feature]]
		- [[dense layer]]
		- [[depth]]
		- [[depthwise separable convolutional neural network]]
		- [[derived label]]
		- [[device]]
		- [[differential privacy]]
		- [[dimension reduction]]
		- [[dimensions]]
		- [[direct prompting]]
		- [[discrete feature]]
		- [[discriminative model]]
		- [[discriminator]]
		- [[disparate impact]]
		- [[disparate treatment]]
		- [[distillation]]
		- [[divisive clustering]]
		- [[downsampling]]
		- [[DQN]]
		- [[dropout regularization]]
		- [[dynamic]]
		- [[dynamic model]]
	- E
		- [[eager execution]]
		- [[early stopping]]
		- [[earth mover's distance]]
		- [[edit distance]]
		- [[Einsum notation]]
		- [[embedding layer]]
		- [[embedding space]]
		- [[embedding vector]]
		- [[empirical risk minimization]]
		- [[encoder]]
		- [[ensemble]]
		- [[entropy]]
		- [[environment]]
		- [[episode]]
		- [[epoch]]
		- [[epsilon greedy policy]]
		- [[equality of opportunity]]
		- [[equalized odds]]
		- [[Estimator]]
		- [[example]]
		- [[experience replay]]
		- [[experimenter's bias]]
		- [[exploding gradient problem]]
	- F
		- [[F1]]
		- [[fairness constraint]]
		- [[fairness metric]]
		- [[false negative]]
		- [[false negative rate]]
		- [[false positive]]
		- [[false positive rate]]
		- [[feature]]
		- [[feature cross]]
		- [[feature engineering]]
		- [[feature extraction]]
		- [[feature importances]]
		- [[feature set]]
		- [[feature spec]]
		- [[feature vector]]
		- [[federated learning]]
		- [[feedback loop]]
		- [[feedforward neural network]]
		- [[few-shot learning]]
		- [[few-shot prompting]]
		- [[Fiddle]]
		- [[fine tuning]]
		- [[Flax]]
		- [[Flaxformer]]
		- [[forget gate]]
		- [[full softmax]]
		- [[fully connected layer]]
	- G
		- [[generative adversarial network]]
		- [[generalization]]
		- [[generalization curve]]
		- [[generalized linear model]]
		- [[generative AI]]
		- [[generative model]]
		- [[generator]]
		- [[gini impurity]]
		- [[Generative Pre-trained Transformer]]
		- [[gradient]]
		- [[gradient boosted decision trees]]
		- [[gradient boosting]]
		- [[gradient clipping]]
		- [[gradient descent]]
		- [[graph]]
		- [[graph execution]]
		- [[greedy policy]]
		- [[ground truth]]
		- [[group attribution bias]]
	- H
		- [[hallucination]]
		- [[hashing]]
		- [[heuristic]]
		- [[hidden layer]]
		- [[hierarchical clustering]]
		- [[hinge loss]]
		- [[holdout data]]
		- [[hosts|host]]
		- [[hyperparameter]]
		- [[hyperplane]]
	- I
		- [[independently and identically distributed]]
		- [[image recognition]]
		- [[imbalanced dataset]]
		- [[implicit bias]]
		- [[imputation]]
		- [[incompatibility of fairness metrics]]
		- [[in-context learning]]
		- [[individual fairness]]
		- [[inference]]
		- [[inference path]]
		- [[information gain]]
		- [[in-group bias]]
		- [[input generator]]
		- [[input layer]]
		- [[in-set condition]]
		- [[instance]]
		- [[instruction tuning]]
		- [[interpretability]]
		- [[inter-rater agreement]]
		- [[intersection over union]]
		- [[item matrix]]
		- [[items]]
		- [[iteration]]
	- J
		- [[JAX]]
	- K
		- [[Keras]]
		- [[Kernel Support Vector Machines]]
		- [[keypoints]]
		- [[k-fold cross validation]]
		- [[k-means]]
		- [[k-median]]
	- L
		- [[L0 regularization]]
		- [[L1 Loss]]
		- [[L1 regularization]]
		- [[L2 loss]]
		- [[L2 regularization]]
		- [[label]]
		- [[labeled example]]
		- [[label leakage]]
		- [[lambda]]
		- [[Language Model for Dialogue Applications]]
		- [[landmarks]]
		- [[language model]]
		- [[large language model]]
		- [[layer]]
		- [[Layers API]]
		- [[leaf]]
		- [[Learning Interpretability Tool]]
		- [[learning rate]]
		- [[least squares regression]]
		- [[linear]]
		- [[linear model]]
		- [[linear regression]]
		- [[logistic regression]]
		- [[logits]]
		- [[Log Loss]]
		- [[log-odds]]
		- [[Long Short-Term Memory]]
		- [[loss]]
		- [[loss aggregator]]
		- [[loss curve]]
		- [[loss function]]
		- [[loss surface]]
	- M
		- [[Machine Learning]]
		- [[majority class]]
		- [[Markov decision process]]
		- [[Markov property]]
		- [[masked language model]]
		- [[matplotlib]]
		- [[matrix factorization]]
		- [[Mean Absolute Error]]
		- [[Mean Squared Error]]
		- [[mesh]]
		- [[meta-learning]]
		- [[metric]]
		- [[Metrics API]]
		- [[mini-batch]]
		- [[mini-batch stochastic gradient descent]]
		- [[minimax loss]]
		- [[minority class]]
		- [[MNIST]]
		- [[modality]]
		- [[model]]
		- [[model capacity]]
		- [[model parallelism]]
		- [[model training]]
		- [[Momentum]]
		- [[multi-class classification]]
		- [[multi-class logistic regression]]
		- [[multi-head self-attention]]
		- [[multimodal model]]
		- [[multinomial classification]]
		- [[multinomial regression]]
		- [[multitask]]
	- N
		- [[NaN trap]]
		- [[natural language understanding]]
		- [[negative class]]
		- [[negative sampling]]
		- [[Neural Architecture Search]]
		- [[neural network]]
		- [[neuron]]
		- [[N-gram]]
		- [[NLU]]
		- [[node (decision tree)]]
		- [[node (neural network)]]
		- [[node (TensorFlow graph)]]
		- [[noise]]
		- [[non-binary condition]]
		- [[nonlinear]]
		- [[non-response bias]]
		- [[nonstationarity]]
		- [[normalization]]
		- [[novelty detection]]
		- [[numerical data]]
		- [[NumPy]]
	- O
		- [[objective]]
		- [[objective function]]
		- [[oblique condition]]
		- [[offline]]
		- [[offline inference]]
		- [[one-hot encoding]]
		- [[one-shot learning]]
		- [[one-shot prompting]]
		- [[one-vs.-all]]
		- [[online]]
		- [[online inference]]
		- [[operation]]
		- [[Optax]]
		- [[optimizer]]
		- [[out-group homogeneity bias]]
		- [[outlier detection]]
		- [[outlier|outliers]]
		- [[out-of-bag evaluation]]
		- [[output layer]]
		- [[overfitting]]
		- [[oversampling]]
	- P
		- [[packed data]]
		- [[pandas]]
		- [[parameter]]
		- [[parameter-efficient tuning]]
		- [[Parameter Server]]
		- [[parameter update]]
		- [[partial derivative]]
		- [[participation bias]]
		- [[partitioning strategy]]
		- [[Pax]]
		- [[perceptron]]
		- [[performance]]
		- [[permutation variable importances]]
		- [[perplexity]]
		- [[pipeline]]
		- [[pipelining]]
		- [[pjit]]
		- [[PLM]]
		- [[pmap]]
		- [[policy]]
		- [[pooling]]
		- [[positional encoding]]
		- [[positive class]]
		- [[post-processing]]
		- [[area under the PR curve]]
		- [[Praxis]]
		- [[precision]]
		- [[precision-recall curve]]
		- [[prediction]]
		- [[prediction bias]]
		- [[predictive ML]]
		- [[predictive parity]]
		- [[predictive rate parity]]
		- [[preprocessing]]
		- [[pre-trained model]]
		- [[pre-training]]
		- [[prior belief]]
		- [[probabilistic regression model]]
		- [[prompt]]
		- [[prompt-based learning]]
		- [[prompt design]]
		- [[prompt engineering]]
		- [[prompt tuning]]
		- [[proxy labels]]
		- [[proxy]]
		- [[pure function]]
	- Q
		- [[Q-function]]
		- [[Q-learning]]
		- [[quantile]]
		- [[quantile bucketing]]
		- [[quantization]]
		- [[queue]]
	- R
		- [[random forest]]
		- [[random policy]]
		- [[ranking]]
		- [[rank (ordinality)]]
		- [[rank (Tensor)]]
		- [[rater]]
		- [[recall]]
		- [[recommendation system]]
		- [[Rectified Linear Unit]]
		- [[recurrent neural network]]
		- [[regression model]]
		- [[regularization]]
		- [[regularization rate]]
		- [[reinforcement learning]]
		- [[Reinforcement Learning from Human Feedback]]
		- [[ReLU]]
		- [[replay buffer]]
		- [[replica]]
		- [[reporting bias]]
		- [[representation]]
		- [[re-ranking]]
		- [[retrieval-augmented generation]]
		- [[return]]
		- [[reward]]
		- [[ridge regularization]]
		- [[RNN]]
		- [[receiver operating characteristic curve]]
		- [[role prompting]]
		- [[root]]
		- [[root directory]]
		- [[Root Mean Squared Error]]
		- [[rotational invariance]]
		- [[R-squared]]
	- S
		- [[sampling bias]]
		- [[sampling with replacement]]
		- [[SavedModel]]
		- [[Saver]]
		- [[scalar]]
		- [[scaling]]
		- [[scikit-learn]]
		- [[scoring]]
		- [[selection bias]]
		- [[self-attention]]
		- [[self-supervised learning]]
		- [[self-training]]
		- [[semi-supervised learning]]
		- [[sensitive attribute]]
		- [[sentiment analysis]]
		- [[sequence model]]
		- [[sequence-to-sequence task]]
		- [[serving]]
		- [[shape (Tensor)]]
		- [[shard]]
		- [[shrinkage]]
		- [[sigmoid function]]
		- [[similarity measure]]
		- [[single program - multiple data]]
		- [[size invariance]]
		- [[sketching]]
		- [[softmax]]
		- [[sparse feature]]
		- [[sparse representation]]
		- [[sparse vector]]
		- [[sparsity]]
		- [[spatial pooling]]
		- [[split]]
		- [[splitter (machine learning)|splitter]]
		- [[squared hinge loss]]
		- [[squared loss]]
		- [[staged training]]
		- [[state]]
		- [[state-action value function]]
		- [[static]]
		- [[static inference]]
		- [[stationarity]]
		- [[step]]
		- [[step size]]
		- [[stochastic gradient descent]]
		- [[stride]]
		- [[structural risk minimization]]
		- [[subsampling]]
		- [[summary]]
		- [[supervised machined learning]]
		- [[synthetic feature]]
	- T
		- [[T5]]
		- [[T5X]]
		- [[tabular Q-learning]]
		- [[target]]
		- [[target network]]
		- [[task]]
		- [[temperature]]
		- [[temporal data]]
		- [[tensor]]
		- [[TensorBoard]]
		- [[TensorFlow]]
		- [[TensorFlow Playground]]
		- [[TensorFlow Serving]]
		- [[Tensor Processing Unit]]
		- [[Tensor rank]]
		- [[Tensor shape]]
		- [[Tensor size]]
		- [[TensorStore]]
		- [[termination condition]]
		- [[test]]
		- [[test loss]]
		- [[test set]]
		- [[text span]]
		- [[tf.Example]]
		- [[tf.keras]]
		- [[threshold]]
		- [[time series analysis]]
		- [[timestep]]
		- [[token]]
		- [[tower]]
		- [[TPU chip]]
		- [[TPU device]]
		- [[TPU master]]
		- [[TPU node]]
		- [[TPU Pod]]
		- [[TPU resource]]
		- [[TPU slice]]
		- [[TPU type]]
		- [[TPU worker]]
		- [[training]]
		- [[training loss]]
		- [[training-serving skew]]
		- [[training set]]
		- [[trajectory]]
		- [[transfer learning]]
		- [[Transformer]]
		- [[translational invariance]]
		- [[trigram]]
		- [[true negative]]
		- [[true positive]]
		- [[true positive rate]]
	- U
		- [[unawareness]]
		- [[underfitting]]
		- [[undersampling]]
		- [[unidirectional]]
		- [[unidirectional language model]]
		- [[unlabeled example]]
		- [[unsupervised machine learning]]
		- [[uplift modeling]]
		- [[upweighting]]
		- [[user matrix]]
	- V
		- [[validation]]
		- [[validation loss]]
		- [[validation set]]
		- [[value imputation]]
		- [[vanishing gradient problem]]
		- [[variable importances]]
		- [[variational autoencoder]]
	- W
		- [[Wasserstein loss]]
		- [[weight]]
		- [[Weighted Alternating Least Squares]]
		- [[weighted sum]]
		- [[wide model]]
		- [[width]]
		- [[wisdom of the crowd]]
		- [[word embedding]]
	- X
		- [[XLA]]
	- Z
		- [[zero-shot learning]]
		- [[zero-shot prompting]]
		- [[Z-score normalization]]