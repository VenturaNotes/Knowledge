---
Source:
  - https://www.youtube.com/watch?v=f5Z7sbkwEwc
---
- ![[Screenshot 2024-11-17 at 12.23.22 AM.png]]
	- Introduction
		- So today's video is going to be answering a very popular question in systems design interviews which is a [[url shortener]] or something similar which is like [[Pastebin]] and i'll kind of get into what those are later. Um but yeah, I've encountered these questions multiple times and I've pricked them just as many. So i think that being said, it's kind of important for me to go over them both for my sake and your own. Enjoy the video guys and let's get into it 
	- TinyURL
		- Okay, so before I actually get into the actual technical details of the problem, I should probably give a quick overview of what [[TinyURL]] is and what [[Pastebin]] is. So basically if you ever have a long url that you want to go ahead and send to a bunch of friends, you just want them to be able to type it into a browser itself without you know having to spend a ton of time writing the actual keys out, TinyURL basically allows you to copy in a longer link and then outputs a shorter link for you to use that when you go ahead and click on the shorter link, it'll redirect you to the longer link. It's also useful because the site will allow you to keep track of the tiny links that you've created for a given user and additionally potentially even see some statistics about that link if you want to be able to kind of get some sort of marketing insight. So you know, the number of clicks perhaps maybe even the demographics of the people clicking the link and so then you might ask yourself, well why am i bunching pastebin into this problem. 
	- Pastebin
		- For those of you that don't know Pastebin actually has a ton of similar functionality as TinyURL in the sense that it similarly is taking some sort of content and providing a shortened pastebin URL for it, but the main thing is that as opposed to just redirecting you to some existing website, all it's doing is going to be redirecting you to some sort of you know txt file containing a bunch of text that you may have copy pasted into it beforehand. So really the only difference between them is that with pastebin, you're going to be redirecting them to some text file and then with TinyURL, you're just going to be redirecting them to a website 
	- And okay, so for actually going ahead and looking at these problems, I'm going to go ahead and go through the five-step process that i went ahead and outlined in my episode 50 of my systems design concept series. So we're going to go ahead and go through all five of these steps and then once we do that, hopefully we can all have a solid grasp on how this problem should actually be answered or not necessarily how it should be answered but rather the possible trade-offs that you can make when trying to go ahead and approach a problem like this.
	- TinyURL & Pastebin
		- Description
			- [[Functional requirements]]:
				- (1) Generate expiring unique short URL from user provided URL
				- (2) Redirect users to the correct website upon clicking short URL
				- (Optional) Logging clicks per short URL
		- Okay, so, what are the actual functional requirements of our service? Well, for TinyURL specifically and i'll go through the functional requirements for both TinyURL and Pastebin, most importantly obviously given some sort of url from a user, we want to be able to go ahead and generate a shorter url of it that is unique and this is kind of the crux of the problem is making sure that we are able to generate these unique URLs with relatively low latency and high availability. Additionally, it's important to you know kind of encapsulate the logic in our service such that when users access or click one of our short urls, they're going to be redirected to the proper place. Additionally, we want to be able to have links expiring which isn't too complex but it is useful to kind of have that excess functionality so that we don't eventually run out of possible short links. Then, finally, even though this isn't typically a requirement in this problem, the last thing I'm going to touch upon is how to accomplish something like analytics where we would basically for a given TinyURL be able to see for example something like the number of clicks or the users that went ahead and clicked it.
	- Okay, similarly as far as Pastebin's functional requirements, very comparably to TinyURL, basically you want to be able to upload your data which is just going to be text to Pastebin and receive a Pastebin URL that is going to be shortened back to it which should again be unique where then you're going to be able to go ahead and access that txt file or that block of text and then finally we want that to be able to expire at some point as well and then additionally kind of similar to TinyURL again, to enable us to have some sort of analytics for the number of clicks and something like that.
	- TinyURL & Pastebin (Capacity Estimations)
		- Description
			- Initial Information
				- Imagine TinyURL has 500 million URLs shortenings generated per month
				- 100:1 reads to writes
				- 500 bytes of storage per URL
			- Calculations
				- 200 writes per second
				- 20k reads per second
				- `15tb` of disk over 5 years
				- 50cb of cache per month
		- Okay, so into capacity estimates we go. Um just a quick note, I've decided that i'm gonna just do one of these because I don't really feel like doing two copies of everything for TinyURL and Pastebin because they're just like a little bit different, but i'll just kind of describe where they they differ and i'll do everything for TinyURL and then like i said if you know Pastebin has any like unique characteristics for a given part of the problem, I'll mention that. But basically, for the capacity estimate part of the problem, I'm mostly stealing these numbers from Grokking because they're pretty arbitrary, right? Like i can more or less just guess any amount of scale, so i'm just going to use theirs because, you know, they're there. They're common. And I deviate from their calculations ever so slightly when it comes to the caching, but, anyways, let's get into those. So imagine that TinyURL has 500 million URL shortenings generated per month and in addition, a 100 to 1 read to write ratio meaning that, basically the majority of the time, people using TinyURL are actually just going to be looking to be redirected. They're not actually creating a shortened URL. So what this really means is that if we have 500 million URLs per month being generated, that means that if you divide that by 30 days, divide that by 24 days per month, and then divide that by 3,600 seconds per day, we can see that they're going to be 200 writes per second and then multiplying that by 100 because of that 100 read to write ratio, we can see that they're going to be 20,000 reads per second. So this should give you a sense that we're actually going to be working a pretty major scale here. 200 writes per second is no joke. It's not like we're just doing one write every single minute. So obviously we are going to have to be supporting relatively high write throughput even though it is going to be the case that reads are going to be our main kind of thing to optimize for here. Additionally, let's imagine that we estimate there are 500 bytes per URL. This was the Grokking the system design estimate. I personally probably would have estimated something a little bit less, maybe closer to 200 or 250 bytes because typically the rule of thumb is, it's two bytes per character of data and you know the URLs are pretty short, the URL shortenings are limited in size, so i wouldn't have thought it would be 500 but whatever 500 is fine. So assuming that's the case, let's say over five years, we want to be able to store 500 bytes times 500 million urls per month times 12 months per year times five years which is equal to 15 terabytes of storage which is actually not very much. You can store that on one machine so that should give some sense of you know how necessary partitioning might be in this problem. It probably isn't going to be overly necessary because all the URLs can be stored in one place, but we're still going to touch upon it later because, you know, why not. Maybe a given interviewer might give you a bigger scale for this problem. Additionally, if we want to cache and one thing that i see often frequently used in these systems design interview problems is that, you know, you assume that say 20% of the keys in a database are responsible for 80% of the traffic, so that means we want to be caching 20% of the keys which means that if there are 500 million URLs per month, we're going to be caching 100 million of those meaning that 100 million urls being in our cache times 500 bytes is going to be equal to 50 billion bytes or 50 gigabytes of cache per month. 50 gigabytes is more than enough to fit on memory of one server these days. You can typically have about 128 gigabytes of ram on a modern server, so that is something that's pretty feasible, but again, if we want to partition it for whatever reason, obviously we can.
		- Furthermore, the one kind of slight change here with Pastebin is that you have to consider the fact that unlike just URLs which are set in how big they can probably be, paste can kind of be an arbitrary size and so generally you want to think about, you know, like the the super high high capacity or high storage paste which might be gigabytes in size or maybe even the average case which is just a few lines of text which might be kilobytes in size or even just bytes in size and so it's kind of important to discuss with your interviewer what the average size of a paste might be so you can estimate kind of the type of scale that you might need out of your storage system for storing that text, but anyways i hope this section makes sense. Let's move on to the next one 
- Image
	- TinyURL & PasteBin (API Design)
		- Okay, uh this is pretty simple for the most part for the third component of the video. We're going to be talking about just the pretty simple api design for both TinyURL and Pastebin and there's really just two endpoints. This is a pretty like simplistic problem in terms of designing the functionality, so there's this create endpoint where you go ahead and take an original URL and then return that shortened TinyURL and then in both the TinyURL and the Pastebin case, there's this get endpoint which takes in basically the shortened URL and returns either a redirect or the actual text itself that was pasted earlier. Obviously you can add certain parameters to these URLs such as like an API key for rate limiting. Perhaps a user ID if you need one. An expiration time or even an alias for a link if you want to make it so that a user can choose their own link. So there's a lot of options here, but kind of the bread and butter of this is just a create and a get endpoint.
	- TinyURL & Pastebin (Database Schema)
		- Description
			- User Table
				- UserId: int
				- Email: string
				- PasswordHash: string
				- CreationTime: date
			- Link Table
				- ShortUrl: string
				- Expiration: date
				- UserId: int
				- `[For tinyurl]OriginalLink: string`
				- `[For pastebin]PasteObjectLink: string`
			- Click Table
				- ShortURL: int
				- TimeClicked: date
				- ClickerInfo: json
		- Okay, let's talk database tables. Um at most i can really only think of three possible tables that we might need and the first is going to be a users table which really isn't overly important. Basically just, you know, say you have a user ID, maybe an email, maybe a time of creation. Some sort of date, something along those lines. That's not overly important. Kind of the most important table here in this problem is going to be the actual mapping of short URLs to the content that they map to and generally speaking in the schema, it's going to probably be best to be using that short URL as a primary key or maybe some sort of partitioning key if we're going to be using Cassandra or something along those lines and i'll discuss database options a little bit later, but yeah, generally speaking, you should probably have some sort of short URL with maybe an expiration time, the content that it maps to, and then perhaps the user who created it. And then the third table i can think of if you're even going to do this because it's technically not part of the functional requirements, is some sort of analytics or clicks table which basically has you know a row per, basically shorten the link. But overall, if you're looking at this data, what you might be able to notice is that there's not a lot of relational needs going on. You're not going to be doing a lot of joining. We have a lot of single access patterns here where we're probably just going to be either writing to a single short url key at one time or we're going to be reading from a short url key at one time and as a result of this, it's going to give us some flexibility to explore databases that aren't necessarily relational and we'll talk about that in a little bit 
	- TinyURL & Pastebin (Architectural Design)
		- Description
			- 6 characters long: $36^6 = 2 \text{ billion}$
			- 8 characters long: $36^8 = 3 \text { trillion}$
Okay, so like i mentioned before the bread and butter of this problem is to basically go and find a way of taking some original url or i guess in the case of pastebin, some original text and figuring out a short url that's going to correspond to it in our databases so that we can reroute users when they're actually using that short url, so there are kind of two main ways to do this both of which have their advantages and disadvantages which i'm going to talk about, but before we even get into that, let's quickly talk about the actual size of our short urls because this is going to kind of determine how many possible long urls we can be holding in our database at once, so if you think about it let's imagine that our short url keys, so like `tinyurl.com/shorturlkey` can contain let's say "a" through "z", all lowercase and zero through 9. So that's going to be 26 letters, 10 um numbers, so 36 characters in total basically and then imagine we have, i don't know, say a length of 6. Then you could have 36 to the sixth total combinations if we want to do a length of eight, we could do 36 to the eighth combinations, and you know, do the math yourself, but just figure out the you know relative to the actual length that we want. How many possible short urls that are unique we can be storing. Okay so now let's talk about taking an original URL and basically generating some short URL from it. So the first thing may come to your mind is probably hashing and this is a good intuition because hashing basically goes ahead and takes one thing and maps it to another thing. We can use something like SHA or MD5 which are common hash algorithms in order to do this. However, the thing is keep in mind remember that we're mapping to a much shorter string than what the hash algorithm is going to output which typically is 32 bits and we're probably only going to be mapping to something like six or eight characters and so as a result of that, we are going to basically truncate the result of this hash function down and we may actually have collisions and once collisions happen, sure we can go ahead and basically use something like probing where we would go ahead and just i don't know maybe modify one of the characters slightly in order to create a string that is hopefully unique in the database. However, even then, that results in a worst case scenario where you have to probe potentially a few times and the request is going to take a little bit longer. Another thing to note is that if two users basically want to get unique tiny urls but are encoding the same original url they're going to get the same hash which is very problematic for certain popular sites that are going to have a bunch of tiny urls corresponding to them for example just something like google.com so as a result even though hashing does work it's very simplistic it doesn't really require very much back-end logic it can lead to collisions and especially if we were to choose a database like cassandra which has this kind of multi-leader replication what may end up happening is that two users are both going to basically receive wording that um their tiny url was successfully created get their tiny url back from that create endpoint and then down the line once anti-entropy happens and basically the the database states are kind of merged through the passing of those merkle trees which i've talked about in the past what's now going to happen is that one user is going to basically have their tiny url completely clobbered by another user because of the fact that there is a right conflict and cassandra uses last right wins so maybe even though cassandra is going to be really fast here in this case if we're using hashing perhaps it would be better to use a database that uses single leader replication because at least that way if we were to have hash collisions where basically two urls are going to the same tiny url we could at least use some sort of atomic operation over one partition in order to go ahead and recognize that there was a collision and then change the tiny url for one of the original urls you might say to yourself oh wait well couldn't we use cassandra if we're using quorums because technically quorums kind of make it so that you know we know if there's going to be a collision at any given point because we're basically reading from a majority of the replicas and the answer to that is still no because there is still a case where race conditions make it so that basically two quorums of nodes disagree with one another and i'll put a diagram up on the screen so that uh you guys can understand what i'm talking about without me having to kind of you know say the entire scenario out loud but basically my point is here that hashing even though it's super simplistic doesn't necessarily guarantee that there are no collisions instead what i've seen other people used in order to kind of accomplish this problem is this concept of a key generation service now the key generation service is basically saying this it is basically going ahead and putting every single possible key in the database so that once a user needs to basically grab a new tiny url you'll go ahead and just ask that key generation service for an unused key and once you have it you'll basically mark it as used and then this way we can basically ensure that there aren't going to be any type of collisions now the main issue with the key generation service is a couple of things first of all obviously you have to check the database every single time to basically ensure that a key hasn't been used yet so it may be a little bit slower in the average case not necessarily the worst case of hashing where you have to do a ton of probing but more importantly we open ourselves up to again a lot of race conditions here so the best possible thing would be to use some sort of atomic operation like a comparison set or a lightweight transaction where we are basically putting a lock on an individual key row so that basically imagine i have the key jordan and i'm going to return that as my tiny url key what we don't want to happen is that i go ahead and say give me a new key the database responds here take jordan it's unused and then before i can basically say oh now i'm using this what the database then goes and does is it responds to another request for a new key saying oh here jordan's the first one and it's unused so basically it's very important that we're using some sort of locking mechanism here on a row at a time such that we don't basically double allocate keys and so that's kind of the the crux of this part of the key generation service i've seen some adaptations where people will use two separate tables one of unused keys and one of used keys and so that's a possibility also but yeah that's kind of the most important thing there this can obviously also be partitioned where we go ahead and split up the you know unused keys over a bunch of tables and then use some sort of load balancer to just go ahead and map every single server request to a different chunk of the database so we can get a key from one of those but yeah that's kind of the main algorithm for actually going ahead and choosing one short url corresponding with an original url such that it's unique and then now we can go into things like replication partitioning load balancing in order to ensure that we're actually getting the performance and availability that we might expect out of our service at scale okay so firstly just to touch upon replication um i've kind of mentioned so far that one of the main objectives of this problem with tinyurl and pastebin is to basically not create conflicting short urls and so as a result it really doesn't make sense at least in my opinion to use some sort of database that supports some sort of multi-leader replication like cassandra or dynamo the reasoning for that being that eventually because of the fact that they basically all use last right wins to settle conflicts if we have two conflicting rights and neither of the writer knows about one another at the actual time of the right one of their tiny urls is basically just going to get clobbered by last right wins so it's funny to me because in grocking the system's design i see that they actually suggest these types of setups because in theory they are really fast for you know individual key reads and writes and that's because of their lsm tree based architecture and the fact that they actually use this leaderless replication strategy however i think there's actually a downside of this which is the fact that it's very possible to have these conflicting rights and unless we're okay with basically down the line saying to a user hey it actually turns out that someone else was already using that tiny url you're going to have to go back and pick a new one then it seems to be the case that using a leaderless replication schema doesn't really make sense here and i personally would prefer to use a single leader replication schema and you know even he's still here nosql is totally fine it doesn't have to be relational because like i said there's not many relations in the actual data but i think using a single leader replication setup where you have the ability to do some sort of lightweight transaction on a single partition is probably the best way to actually make sure that we're avoiding conflicts on a single url and this can be the case for either our key generation setup or our actual hashing schema so additionally as far as replication goes we mentioned i think that single leader is probably the best way to do things personally moving onwards talking about actual partitioning i think that the key should generally be partitioned very well because especially if we're using hashing we're already basically going to be partitioning based on a hash range as opposed to just a range of original urls which are probably going to be condensed in certain areas for example there are probably a lot more tiny urls to youtube videos and so if we were partitioning by the original url there would be a ton of things you know in the y partition but if we are just partitioning by the actual key itself those are already hashes so there should be a relatively even distribution of keys per machine the one thing to note is that even though they're going to be a relatively even distribution of keys per machine and as a result the amount of storage used per machine it is the case that some of those keys might experience very high loads just by virtue of so many people clicking on them however while this might normally be an issue and this is something that if there were a ton of rights to the keys we might need to do some like even more complicated partitioning the fact that all of those clicks are basically just reads from the database means that we can ensure that we're not going to be overloading any of these partitions by virtue of using caching so like i mentioned we can further partition and replicate our cache but the point of that is to just hold our most commonly used short keys in order to basically stop our database from experiencing super high amounts of traffic so that's really important to have additionally as far as cash goes a good eviction policy would probably just be least recently used where if a short url hasn't been clicked for a while and the cash is run out of space we can go ahead and just evict it from the cash and put in our new cash entry so that's kind of a good way of making sure that things aren't getting too crowded there and also keeping the relevant data in the cache so then going back to which database we should actually use i guess the question is kind of this i think it would be really useful to continue to use a nosql architecture because again we don't need all of these um you know abstractions that sql puts on top of our data which slows things down quite a bit in a distributed setting i'm thinking that any type of nosql architecture using an lsm tree based storage engine but still using single leader replication and you know allows for easy partitioning would be good here maybe something like mongodb which is a document database would be good or perhaps you could get away with ryak because ryak even though it is a leaderless replication type of database does have some automatic conflict resolution logic which might make it so that we don't just clobber conflicting short urls maybe we could make it so that one of them actually goes ahead and gets written to the database but with an auto-incrementing key or something like that my point is it's i guess you could get away with saying cassandra but you would just have to recognize the fact that there is this possibility that we might have right conflicts down the line and if those were to occur that it's possible that some users might just see their tiny urls disappearing so just keep that in mind another thing to mention is load balancing because we're having such relatively high throughput of both reads and writes i personally would probably decouple the actual creation of the tiny urls and the reading of the tiny urls into different microservices the reason for this being that since we have an 100 to 1 read to write ratio we want to be able to scale these two services at relatively different rates since reads are going to be so much more popular than rights as an operation as a result we could probably use load balancers for the both of those and even though round robin is probably a sufficient policy for load balancing or anything like that perhaps you could use consistent hashing where you're basically partitioning the load balancing request to the proper application servers based on the tiny url being clicked so that way each application server can go ahead and keep on its cache or its local cache the result of a tiny url redirect or something along those lines and if we ever do need to keep track of something like consistent hashing information especially as it pertains to partitioning especially this is probably something that i should have mentioned something like a coordination service which allows you to basically have this distributed store of truth that we know is going to be not strongly consistent but ordered and linearizable such as zookeeper would be really useful to kind of keep track of the partitioning information per server especially so that if a server is to be added to a cluster or go down zookeeper can kind of go ahead and keep track of that information so with all of these things in mind let me go ahead and pause the video and show off what i ended up basically writing about as my diagram and then i suppose i can talk about that for a little bit okay final design tied all together so i'm going to make sure to have the design up on the screen so that we can go ahead and look at it as i go through this but basically the point is more or less to have the following so the client after making either a create url or a fetch url request is going to hit our load balancer which in order to avoid having a single point of failure we can have it in active passive configuration more or less meaning that we have one load balancer which communicates with a second load balancer sitting in standby waiting to go ahead and become the main load balancer if the first one stops sending it heartbeats then i've split the service into two micro services one for creating the key which as you can see we have our horizontally scaled key creation servers which upon looking to create an actual key for a given original url we'll first go ahead and use our either key generation logic or our hashing logic to go ahead and hit our single leader replicated database for this one i've just chosen to use mongodb because mongodb supports single leader replication out of the box and it's pretty popular after going ahead and hitting our database what's going to go ahead and happen is we're going to replicate the result of that key creation to our replicas and then upon actually trying to have another user fetch the result of a tiny url we're going to first check if that link is in the key cache aka if it's one of the hot links and if not we're going to go ahead and access it from the database or one of the replicas if it is in the key cache we'll just go ahead and return that instantly and we'll save a lot of load from our database additionally in order to support the actual partitioning information of the replicas we will have a zookeeper instance which is going to periodically receive heartbeats from each of the replicas to make sure they're up and also all of the partition masters in the cluster in order to make sure they're up so the zookeeper is just going to be our coordination service in order to kind of deal with the status of all of our databases and then in addition to that even though i didn't draw it out here we should probably have a second set of load balancers between basically the fetching servers and the actual database themselves so that the basically that second load balancer is going to determine which replica to go ahead and read from in the event that we're going to actually make a read that is a cache miss and pulls from the database additionally on cache misses we can choose to go ahead and load that database into our cache using our lru eviction policy in order to go ahead and pull data from the database then finally if we're talking about pastebin because like i said pastebin is just a small modification to tinyurl in addition to actually having our database when we create one of our short urls corresponding to a paste what we must first do before actually updating our database full of short urls is putting the paste data in s3 and then for particularly popular pastes we can go ahead and load those into a content delivery network because that is basically a cache that is distributed all over the globe such that we can get content to our users really quickly cdns similarly to caches also have an expiration time for a bunch of the content in them and furthermore because they're distributed globally we can do this kind of smart partitioning strategy such that say only pastes in english will go to the american region and then we can kind of distribute our pace based on language in order to put them in one region or another okay so that's more or less the overall design and let's get into a conclusion for the video alright everyone that was our first actual systems design interview preparation video i know that this is a pretty common question but i feel like it still helps to post because a like i said i wanted to go a little bit more in depth on this problem than most people tend to probably do and i hope that i've actually done that by doing that little bit of a database comparison and looking at you know the trade-offs between using something like a multi-leader or leaderless replication model versus a single leader replication model as far as collisions go and kind of the fallout of having to deal with that if there were to be a collision in addition to that um please let me know if you guys enjoyed this or if you want me to go even deeper because obviously you always can i mean you can basically go anywhere short of building the thing out and so a i'd love to get some feedback from you guys and b and more importantly if there are other systems designed questions that you want to see answered please let me know i've probably got about 50 or not 50 maybe 25 written down in my notes that i plan on getting to but if you have more that are a little bit less common please let me know and i would love to address them i hope this video was useful i've put a decent amount of work into the editing and the filming and so i hope that's shows and i'm going to continue to try and get better have a good one guys