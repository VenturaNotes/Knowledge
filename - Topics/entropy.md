## Synthesis
- 
## Source [^1]
- (Shannon entropy) For a random variable $X$ which takes $k$ values with probabilities $p_1, p_2, ... p_k,$ the entropy of $X$ is given by $H(X) = \sum_1^k p_ilog_2p_i.$ This is the expected value of the information when sampling $X$. It is maximal when the probability distribution is uniform. See SHANNON'S THEOREM.
## Source[^2]
- A measure of the amount of information that is output by a source, or throughput by a channel, or received by an observer (per symbol or per second). Following Shannon (1948) and later writers, the entropy of a discrete memoryless source with alphabet $A=\{a i\}$ of size $n$, and output $X$ at time $t$ is

  

$$

H(X)=\sum_{i=0}^{n-1} p\left(x_{i}\right) \log _{2}\left(1 / p\left(x_{i}\right)\right)

$$

  

where

  

$$

p\left(x_{i}\right)=\operatorname{Prob}\left\{X_{t}=a_{i}\right\}

$$

  

The logarithmic base $b$ is chosen to give a convenient scale factor. Usually,

  

$$

\begin{aligned}

& b=2 \\

& b=e=2.71828 \ldots

\end{aligned}

$$

  

or

  

$$

b=10

$$

  

Entropy is then measured in bits, in natural units or nats, or in Hartleys, respectively. When the source has memory, account has to be taken of the dependence between successive symbols output by the source.

  

The term arises by analogy with entropy in thermodynamics, where the defining expression has the same form but with a physical scale factor $k$ (Boltzmann constant) and with the sign changed. The word [[negentropy]] is therefore sometimes used for the measure of information, as is uncertainty or simply 'information'.
## Source[^3]
- Symbol S. A measure of the unavailability of a system's energy to do work; an increase in entropy is accompanied by a decrease in energy availability. When a system undergoes a reversible change the entropy $(S)$ changes by an amount equal to the energy $(Q)$ absorbed by the system divided by the thermodynamic temperature $(T)$ at which the energy is absorbed, i.e. $\Delta S=\Delta Q / T$. However, all real processes are to a certain extent irreversible changes and in any closed system an irreversible change is always accompanied by an increase in entropy.
- In a wider sense entropy can be interpreted as a measure of a system's disorder; the higher the entropy the greater the disorder. As any real change to a closed system tends towards higher entropy, and therefore higher disorder, it follows that the entropy of the universe (if it can be considered a closed system) is increasing and its available energy is decreasing. This increase in the entropy of the universe is one way of stating the second law of thermodynamics.
## Source[^4]
- $n$. A measure of the degree of disorder of a closed system, originally applied to a contained gas, expressed by $S=\Sigma p_{i} \log _{2} p_{i}$ where $p_{i}$ is the probability of a particular state of the system, the value of $S$ in a closed system never decreasing over time according to the second law of thermodynamics. In work on information theory, the US mathematicians Norbert Wiener (1894-1964) and Claude E(lwood) Shannon (1916-2001) interpreted information as negative entropy, but this was deprecated by the British telecommunications engineer Colin Cherry (1914-79) and others. See also Nernst heat theorem. \[Coined in 1865 by the German physicist and mathematician Rudolf Julius Clausius (1822-88) from Greek en in or into + tropos a turn, from trepein to turn, alluding to transformation of energy]
## References

[^1]: [[(Home Page) The Concise Oxford Dictionary of Mathematics 6th Edition by Oxford Reference]]
[^2]: [[(Home Page) A Dictionary of Computer Science 7th Edition by Oxford Reference]]
[^3]: [[(Home Page) A Dictionary of Biology 8th Edition by Oxford Reference]]
[^4]: [[(Home Page) A Dictionary of Psychology 4th Edition by Oxford Reference]]