## Synthesis
- 
## Source [^1]
- (Shannon entropy) For a random variable $X$ which takes $k$ values with probabilities $p_1, p_2, ... p_k,$ the entropy of $X$ is given by $H(X) = \sum_1^k p_ilog_2p_i.$ This is the expected value of the information when sampling $X$. It is maximal when the probability distribution is uniform. See SHANNON'S THEOREM.
## References

[^1]: [[Home Page - The Concise Oxford Dictionary of Mathematics 6th Edition by Oxford Reference]]