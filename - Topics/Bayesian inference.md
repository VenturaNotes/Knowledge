## Synthesis
- It is a statistical method that uses probabilities to update our belief in a hypothesis based on new evidence or data.
- Grounded in [[Bayes' theorem]] which relates current evidence to prior beliefs
## Source[^1]
- Bayesian inference is a way of making [[statistical inferences]] in which the statistician assigns subjective probabilities to the distributions that could generate the data
## Source[^2]
- An approach to hypothesis testing that involves making an assessment of which of two hypotheses, the null $\left(H_{0}\right)$ or the alternative $\left(H_{1}\right)$, has a higher probability of being correct. First, prior probabilities of each of the hypotheses being correct, $P\left(H_{0}\right)$ and $P\left(H_{1}\right)$, are assumed, and the prior odds ratio, $P\left(H_{0}\right) / P\left(H_{1}\right)$, is formed. Then, based on the prior density functions and the likelihood functions of the data conditioned on each of the hypotheses, the prior odds ratio is modified to form a posterior odds ratio. In contrast to the classical approach, it is not necessary to accept or reject each hypothesis. If needed, such a decision can be made by minimizing the expected loss from making a wrong decision, using some specified loss function, where the expectations are calculated with respect to the posterior probabilities on each hypothesis.
## Source[^3]
- In Bayesian statistics, parameters have probability distributions, while in frequentist statistics parameters have fixed values. In Bayesian inference, a prior distribution is proposed for a parameter and after further data is collected Bayes' Theorem is used to calculate a posterior distribution in light of the new data. Frequentist inference, in contrast, would use a hypothesis test to assign a $p$-value to a null hypothesis that the data came from a population with a suggested parameter value.
## Source[^4]
- $n$. A form of statistical reasoning in which prior probabilities (2) are modified in the light of data or empirical evidence in accordance with Bayes' theorem to yield posterior probabilities, which may then be used as prior probabilities for further updating in the light of subsequent data. Bayesian inference is often based on subjective probabilities, and the following example shows how Bayesian updating is done. Suppose a physician begins with a prior probability $P(H)$ that a certain patient has HIV infection. This prior probability may be the base rate of HIV infection in the community or may be a subjective estimate derived from anywhere; let us assume that it is .001. The physician then observes a relevant symptom, dementia, and needs to determine the posterior probability $P(H \mid D)$ that the patient has HIV infection, given this new datum. Using Bayes' theorem, the physician multiplies the prior probability $P(H)$ by $P(D \mid H) / P(D)$, which is the likelihood ratio (1) of the subjective probability of the symptom occurring if the patient has the disorder (let us say .50) divided by the subjective probability of the symptom irrespective of the disorder$\textemdash$let us say .002. Thus the posterior probability $P(H \mid D)=(.001)(.50 / .002)=.25$, and the physician's subjective probability that the patient has HIV infection has risen from .001 to .25 . This posterior probability can then be used as a prior probability for updating in the light of further evidence, and so on. Also called Bayesian statistics or conditioning . See also insUFFICIENT REASON.
## References

[^1]: https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference
[^2]: [[(Home Page) A Dictionary of Economics 5th Edition by Oxford Reference]]
[^3]: [[(Home Page) The Concise Oxford Dictionary of Mathematics 6th Edition by Oxford Reference]]
[^4]: [[(Home Page) A Dictionary of Psychology 4th Edition by Oxford Reference]]