## Synthesis
- 
## Source [^1]
- For a sample from a population with an unknown parameter, the likelihood function is the probability that the sample could have occurred at random; it is a function of the unknown parameter. The method of the maximum likelihood estimator selects the value of the parameter that maximizes the likelihood function. The concept applies equally well when there are two or more unknown parameters.
## Source[^2]
- The probability or the probability density of the occurrence of a sample configuration $\left(x_{1}, \ldots, x_{n}\right)$, given the joint distribution $f\left(x_{1}, \ldots, x_{n} \mid \theta\right)$, expressed as a function of $\theta$ conditional on the sample$$L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=f\left(x_{1}, \ldots, x_{n} \mid \theta\right)$$where $\theta$ is a parameter or a vector of parameters of the distribution. In many applications the log-likelihood function, $\ell(\theta) \equiv \ln (L(\theta))$, is used.
## Source[^3]
- $n$. In statistics, the probability of obtaining a given set of observed scores in a random sample as a function of the parameter(s) of the population from which the scores are drawn. See also likelihood ratio (1).
## References

[^1]: [[(Home Page) The Concise Oxford Dictionary of Mathematics 6th Edition by Oxford Reference]]
[^2]: [[(Home Page) A Dictionary of Economics 5th Edition by Oxford Reference]]
[^3]: [[(Home Page) A Dictionary of Psychology 4th Edition by Oxford Reference]]