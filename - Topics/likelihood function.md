## Synthesis
- 
## Source [^1]
- For a sample from a population with an unknown parameter, the likelihood function is the probability that the sample could have occurred at random; it is a function of the unknown parameter. The method of the maximum likelihood estimator selects the value of the parameter that maximizes the likelihood function. The concept applies equally well when there are two or more unknown parameters.
## Source[^2]
- The probability or the probability density of the occurrence of a sample configuration $\left(x_{1}, \ldots, x_{n}\right)$, given the joint distribution $f\left(x_{1}, \ldots, x_{n} \mid \theta\right)$, expressed as a function of $\theta$ conditional on the sample$$L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=f\left(x_{1}, \ldots, x_{n} \mid \theta\right)$$where $\theta$ is a parameter or a vector of parameters of the distribution. In many applications the log-likelihood function, $\ell(\theta) \equiv \ln (L(\theta))$, is used.
## References

[^1]: [[(Home Page) The Concise Oxford Dictionary of Mathematics 6th Edition by Oxford Reference]]
[^2]: [[Home Page - A Dictionary of Economics 5th Edition by Oxford Reference]]