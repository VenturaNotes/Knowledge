## Synthesis
- 
## Source [^1]
- $n$. In statistics, a theorem, easily proved from Bayes' rule, expressing the conditional probability of an event $H$ given an event $D$, written $P(H \mid D)$, in terms of the conditional probability of $D$ given $H$, written $P(D \mid H)$, the probability of $D$, and the probability of $H$. In its simplest form, $P(H \mid D)=P(D \mid H) P(H) / P(D)$. The theorem enables the prior probability (2) of a hypothesis $(H)$ to be updated repeatedly to produce posterior probabilities in the light of data $(D)$ derived from observation or experience, and it underpins the whole edifice of Bayesian inference. In its most general form, the theorem states that if $H_{n}$ is one of a set $H_{i}$ of mutually exclusive and exhaustive events, then $P\left(H_{n} \mid D\right)=P(D \mid$ $\left.H_{n}\right) P\left(H_{n}\right) / \Sigma_{i}\left[P\left(D \mid H_{i}\right) P\left(H_{i}\right)\right]$. \[Named after the English mathematician and Presbyterian clergyman Thomas Bayes (1702-61) and published posthumously in 1763]
## References

[^1]: [[(Home Page) A Dictionary of Psychology 4th Edition by Oxford Reference]]