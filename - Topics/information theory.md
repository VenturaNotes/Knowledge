## Synthesis
- 
## Source [^1]
- The study of information by mathematical methods. Informally, information can be considered as the extent to which a message conveys what was previously unknown, and so is new or surprising. Mathematically, the rate at which information is conveyed from a source is identified with the entropy of the source (per second or per symbol). Although information theory is sometimes restricted to the entropy formulation of sources and channels, it may include coding theory, in which case the term is used synonymously with communication theory.

## Source[^2]
- The area of mathematics concerned with the transmission and processing of information, especially concerned with methods of coding, decoding, storage, and retrieval and evaluating likelihoods of degree of accuracy in the process. See entropy, Shannon's theorem. 
## Source[^3]
- $n$. A theory of information (2) in which the amount of information, symbolized by $H$, in a signal is defined as the smallest number of binary digits or bits that are required to encode it. If a signal specifies a choice of one of $N$ equiprobable alternatives, then the amount of information that it contains is $H=\log _{2} N$; hence, for example, if 16 people are equally likely to have committed a crime, a signal specifying the guilty person conveys $H=\log _{2} 16=4$ bits of information, which may be interpreted as four successive binary divisions of the set of suspects. As a second example, in a signal in which each letter of the English alphabet is equally probable, the information content of a single letter is $H=\log _{2} 26=$ 4.70 bits. A more general definition, applicable where the alternatives are not equiprobable, is given by the Wiener-Shannon formula $H=-\Sigma p_{\mathrm{i}} \log _{2} p_{i}$, where $p_{i}$ is the probability of the $i\text{th}$ alternative, and in the light of this formula information is sometimes defined as negative entropy. The theory was developed independently in the late 1940s and early 1950s by the English statistician and geneticist Ronald Aylmer Fisher (1890-1962) and the US mathematicians Norbert Wiener (1894-1964) and Claude E(lwood) Shannon (1916-2001), and it was Shannon's work that had the greatest influence in psychology. See also BYTE, INFORMATION TECHNOLOGY, NYBBLE.
## References

[^1]: [[(Home Page) A Dictionary of Computer Science 7th Edition by Oxford Reference]]
[^2]: [[(Home Page) The Concise Oxford Dictionary of Mathematics 6th Edition by Oxford Reference]]
[^3]: [[(Home Page) A Dictionary of Psychology 4th Edition by Oxford Reference]]