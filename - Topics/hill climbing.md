## Synthesis
- 
## Source [^1]
- A fast but sometimes unreliable optimization method. When searching for the minimum/maximum value of a function a random step is taken; if the value improves it replaces the current value, then another random step is taken. This method is fast and relatively easy to program but does not allow backtracking and therefore can become trapped on local minima/maxima in the search space.
- A heuristic variation uses an evaluation function to examine and select the best successor from the current position. This produces a faster ascent through the problem space.
## Source[^2]
- $n$. In connectionism (1), a numerical method for maximizing a function by invariably moving upwards along a slope of steepest ascent from the current position. The method does not guarantee to reach a global maximum because, like a mountaineer who tries to reach the highest point by always climbing up a slope of steepest ascent from any given point and who gets trapped on a local mound separated by valleys from higher peaks, the process may get trapped at a local maximum nowhere near the global maximum or summit. It is used in connectionism and parallel distributed processing to maximize the correlation between the output of a network model and the desired state for a given input. Also called steepest ascent. See also annealing (1), BACK-PROPAGATION ALGORITHM. Compare GRADIENT DESCENT.
## References

[^1]: [[(Home Page) A Dictionary of Computer Science 7th Edition by Oxford Reference]]
[^2]: [[(Home Page) A Dictionary of Psychology 4th Edition by Oxford Reference]]