## Synthesis
- 
## Source [^1]
- A sequence of random variables $x_{1}, \ldots, x_{n}, \ldots$ converges in mean squares to a random variable $x$ if $E\left[x^{2}\right]$ and $E\left[x_{n}{ }^{2}\right]$ exist and the expectation of the squared (Euclidean) distance between $x_{n}$ and $x$ converges to zero as $n$ tends to infinity: $\lim _{n \rightarrow \infty} E\left[\left(x_{n}-x\right)^{2}\right] = 0$. In particular, $x$ can be a constant, $x=\theta$. In this case convergence of $x_{n}$ to $\theta$ in mean squares is equivalent to the convergence of the bias and the variance of $x_{n}$ to zero as $n$ tends to infinity. Convergence in mean squares implies convergence in probability (the converse does not hold, in general). This is a particular case of convergence in the $p$th mean (or in $L^{p}$ norm) defined as $E\left[x^{p}\right], E\left[x_{n}{ }^{p}\right]$ exist and $\lim _{n \rightarrow \infty} E\left[\left(x_{n}-x\right)^{p}\right]=0$. Convergence in $p$th mean implies convergence in $r$th mean for every $r \in(1$, p).
## References

[^1]: [[(Home Page) A Dictionary of Economics 5th Edition by Oxford Reference]]